i am facing this strange behaviour , i got a HDFStore containing DataFrames . 
What happens if you do ` ax.legend() ` and ` plt.draw() ` ? 
E.g. , you can call ` pandas.isnull ( the_frame )` . 
do this : `` df.ix [ 0 , ' a '] += 1 `` . 
Using unstack() is one way . 
Clearly I'm missing something as to why df.loc [: , tuple ] is different than df [ tuple ] . 
datetime dtypes in pandas read_csv 
If you're using version ` 0.17.0 ` or higher then you can call this using ` .dt .strftime ` which is vectorised : #CODE 
Looks like you need histogram() of months . 
You want ` .dt .time ` see the docs for some more examples of things under the ` .dt ` accessor . 
pd.rolling_apply ( df.exma , 2 , ( df.alpha * df.exma.shift ( 1 )) + (( 1 - df.alpha ) * df.outperf )) 
I suggest not using ` file ` for your ` open() ` object . 
If you want every value from row 2 you can just do ` df.iloc [ 2 ]` or ` df.iloc [ 2 ] .values ` respectively . 
dtype : object 
Check out the glob module and pandas read_csv() and concat() 
` np.dtype ( ' datetime64 [ ns ]')` and ` np.dtype ( ' M 8[ ns ]')` : #CODE 
Doing ` pd.crosstab ( rows =[ df [ ' A '] , df [ ' B ']] , cols =[ df [ ' C ']] , margins=True , aggfu 
DataFrame.drop_duplicates and DataFrame.drop not removing rows 
` pd.rolling_mean ( df , num ) .dropna() .plot() ` 
This is a replacement for ` np.array_equal ` which is broken for nan positional detections ( and object dtypes ) . 
It looks like ` pd.unique ` does not respect the ` datetime64 ` dtype : #CODE 
df [ ' NumActivity '] = pd.factorize ( df [ ' Activity ']) [ 0 ] +1 works . 
calling ` as_matrix() ` on ` df ` returns a ` numpy.ndarray ` object #CODE 
And using pandas.Series.map : #CODE 
works , but I think ` df.select_dtypes ` should be preferred since it uses the 
Then you can access the relevant rows using groupby's ` get_group ` : #CODE 
It's not in the online docs , you have to check the ` help ( pd.bdate_range )` :) 
dtype : int64 ` 
dtype : object 
df.drop ([ ' one ' , ' two ' , ' three '] , axis=1 , inplace=True ) 
I tried make these ' filenames ' dataFrames using ` pd.DataFrame ` but wasn't able to do so . 
Try using .loc [ row_index , col_indexer ] = value instead ` . 
Good question , ` plt.hist ( hour_list , bins= np.arange ( 24 ) - 0.5 )` will work . 
what is the - 0.00343 ( ah I see , a consequence of not using right=False ) , pd.cut very neat 
2 ) Alternatively , don't create levels for ` aa ` and ` bb ` using ` as_index=False ` and ` pd.merge ` #CODE 
` df.groupby ( level =[ ' major ' , ' minor ']) .mean() .dropna() ` 
Argh , that still gives me ` dtype ( ' datetime64 [ us ]')` . 
or better yet , just don't specify a dtype : #CODE 
I was trying to do this with ` df [ df.shift() ! = df ] .dropna() .reset_index ( drop=True )` but ` shift() ` is not behaving in the way I meant . 
` median_search_query = np.median ( df.srch_query_affinity_score )` 
Example using df.asfreq ( ' Q ') : #CODE 
``` In [ 15 ]: pd.to_timedelta ( s.str.replace ( ' hrs ' , ' h ')) 
Python Dict , Lambda x , map() doesn't work 
I have tried to modify your answer to make it an example about modifying two columns like I asked ( for one the ` df.loc [: , ( ' A ' , ' a ')]` works like a charm ) . 
I thought np.ma.average is just what I need , but that also gives me NaN as a result . 
Although ``` pandas.cut() ``` is the better and more general answer , it looks like in this case you could do ``` df [ ' B '] = ( df.A / 500 ) .astype ( int ) + 1 ``` . 
outcome : displays good in ` df.head() ` , but reverts to scientific notation upon coercion to string concatenation using + operator 
The ` ewma ` case can be solved similarly : ` df.groupby ( level=0 ) .apply ( lambda x : pd.ewma ( x , com=2 ))` 
I didn't realise that ` .resample() ` wasn't inplace ! 
I can't think of a way to overload the ` .loc ` method properly ! 
Do I have to specific the dtypes to make this work ? 
It appears that the ` applymap ` is the culprit here :-) 
" " .join ( header.split() ) .split ( ' ') 
I want df.append ( df2 ) in this case to be : #CODE 
I completely forgot about .loc 
It sounds like maybe you want ` pandas.concat ` ? 
Is there anything in pandas that is the opposite to ` .dropna() ` ? 
Datetimes are handled if they can properly be converted ( e.g. they have a dtype of ' datetime64 [ ns ]' , notably datetimes.date are NOT handled ( NaN are a different story and depending on usage can cause the entire column type to be mishandled ) 
The ` args ` and ` kwargs ` parameters were added to ` rolling_apply ` in Pandas version 0.14.0 . 
try `` df [ ' LastName '] = df.apply ( updateDataframe )`` , but even better is : `` df.ix [ df [ ' LastName '] == ' Joe ' , ' LastName '] = ' Black '`` 
There should be no problem ; ` plt.plot ` and ` plt.fill_between ` accept arrays of dtype ` datetime64 [ ns ]` just fine . 
You could try pandas pivot() . 
What happens if you do ` ax.legend() ` and ` plt.draw() ` ? 
Pandas Python read_csv error_bad_lines producing shell feedback 
Though in some respects , the np.clip or np.max solutions are more easily read , I think this is the most precise answer to my original question . 
Did you try setting the delimiter to semicolon in your ` read_csv ` call ? 
In [ 8] : %timeit df [ ' r '] = df [[ ' minor ' , ' major ']] .abs() .max ( axis=1 ) 
I guess my confusion stems from the fact that the Series.value_counts doesn't seem to fit into the arguments required by the df.apply method . 
The problem is that ` a ` is dtype ` object ` . 
try ` video_base = pd.merge ( df_one , df_two [[ ' count_watched_yeterday ']] , how= ' left ' , left_index=True , right_index=True )` 
ax.xaxis.set_major_locator ( MultipleLocator ( 10 )) 
Actually I think you want : ` pd.concat ([ df_may , df_jun ] , axis=0 , ignore_index=True )` 
Maybe there is a better approach thats takes advantage of features of the Pandas.DataFrame class ? 
Try ` df.loc [ ' a ']` instead . 
Then this should work : ` df [ ' Gene.Symbol '] = df [ ' Gene.Symbol '] .str .strip() .str .upper() ` 
then [ ` df.groupy ( ' key ') .agg ( ... )`] ( #URL ) might be what you are looking for . 
DataFrame.apply in python pandas alters both original and duplicate DataFrames 
you might want to try ` df.iloc [ 0 ]` rather than ` df.iloc ( 0 )` . 
and , ` ts.asfreq ( ' H ' , method= ' ffill ')` to have hourly frequency . 
I tried something like ` set_index ` , ` pd.factorize() ` and ` index_col ` but they do not work . 
So df = df.reindex() results in the same indexing ... 
I think this solution will execute faster than using iterrows() , but I'm not sure . 
One option using ` df.reindex ` : #CODE 
I am using dtype as suggested in the answer there . 
You might be interested in ` pd.cut ` : #CODE 
df.groupby ( ' A ') .size() .apply ( lambda x : float ( x ) / df.groupby ( ' A ') .size() .sum() *100 ) 
` df.apply() ` works off a single row , ` shift() ` doesnt seem to work . 
Could you try this : ` df [ ' GC content '] = df [[ ' oligo_sequence ']] .apply ( lambda row : GC ( row ) , axis=1 )` 
And , ` pd.eval() ` works well with expressions containing large arrays #CODE 
The eventual goal being to arrange hierarchically to weekday hour-range , something like : #CODE 
Reading about ` applymap ` I wondered if there is a similar way of defining and applying operators that work on pairs of dataframes . 
Executing your code with pandas 0.16.2 yielded the two columns with dtype datetime64 [ ns ] . 
` df [ ' price_trend '] .apply ( lambda x :[ i.split ( ' : ') for i in x ])` #CODE 
Here the new value AND the existing dtype of the column matters . 
how about using the ` pd.DataFrame.drop_duplicates() ` method ? 
dtype : timedelta64 [ ns ]``` 
` gg [ ' cumt '] = gg.apply ( lambda x : x [ ' tavg '] + x [ ' tavg '] .shift ( 1 ) [ 1 :] )` 
" " .join ( header.split() ) .split ( ' ') 
print ' \nAfter replace\n ' , df.replace ( { ' c1 ' : c1_fromto , ' c2 ' : c2_fromto } ) 
Profiling shows the culprit is obviously ` B.ix [ row [ 0 ]] .irow ( np.searchsorted ( B.ts [ row [ 0 ]] , row [ 2 ])))` . 
At the first step I used ` df.T ` to transpose the dataframe , and tried something like ` df.value_counts() ` , however I'd 
Replace NaN in a dataframe with random values 
I want to replace all the NaN with some random values like . 
#CODE 
Resample function throwing error with Twitter Data 
I then try to resample for analysis #CODE 
I'm writing several pivot tables using pandas . 
For many of them , I need to return unique values . 
In a two-dimensional pivot table , the below code works as it should . 
When I add a third dimension , the code returns the count rather than the unique count . 
I suspect this has something to do with the aggfunc , but can't determine to what it should be changed . 
Use a groupby to get at each combination of ` col_1 ` and ` col_3 ` , then unstack to get the ` col_3 ` values as columns : #CODE 
Python pandas merge or concat dataframes 
The data is for 2 products ( BBG.XAMS.UL.S_pnl_pos_cost and BBG.XAMS.UNA.S_pnl_pos_cost ) by date , in the future there will be more products . 
I want to concat or merge ( not sure which ) the list of dataframes into one data frame ( called result ) so they look like : #CODE 
where axis is the date . 
It looks like the data is merged by date , but I am missing the data for the week beginning 2015-03-23 . 
My current concat result dataframe looks like : #CODE 
Try using axis=0 . 
This should concat column-wise , assuming each dataframe has the same column names . 
possible duplicate of [ Pandas join / merge / concat two dataframes ] ( #URL ) 
Also , how do you join this back to original dataframe ? 
We can resample this to days ; it'll be a much longer timeseries , of course , but memory is cheap and I'm lazy : #CODE 
How do I merge the birth rate back to the original table ? 
Indexes aren't compatible ... 
Turns out size isn't such an issue . 
Python & Pandas : Unable to drop columns 
I try to drop the data , but it reports some column does not exist . 
#CODE 
@USER , that's possible , but I don't know how to deal with it . 
In my previous experience with pandas , it will automatically turn the second ` Q ` into ` Q.1 ` when reading the data . 
However , in my case , it failed to do it , and I don't know why . 
However , This it cannot ` drop ` ` NCDC ` either . 
You can then drop your columns : #CODE 
I would like to take a given row from a DataFrame and prepend or append to the same DataFrame . 
Rather than concat I would just assign directly to the df after ` shift ` ing , then use ` iloc ` to reference the position you want to assign the row , you have to call ` squeeze ` so that you assign just the values and lose the original index value otherwise it'll raise a ` ValueError ` : #CODE 
To insert at the end : #CODE 
I'm not sure exactly what you're expecting , but you could replace your lists with numpy arrays ( I don't think it'll improve your specific code ): #CODE 
What is the best way for me to get this data into Pandas ? 
Is there a standard way I could use ` read_table ` or some similar function to read this file directly ? 
Should I write a script to insert commas where all the column breaks are and then read it in as CSV ? 
( I'd just do the latter , but I'm also interested in becoming better with Pandas so if there's an out-of-the-box way I'd like to know it . ) 
Any ideas on how to get this file to load ? 
Unfortunately I can't just strip out accents , as I have to interface with software that requires the proper name , and I have a ton of files to format ( not just the one ) . 
Thanks ! 
pls show your input and what is the expected output , in a copy-pastable form . 
What you are doing is very inefficient . 
A groupby should try to use vectorized functions when possible . 
Then join them up at the end . 
Hi Tom , it doesn't look like this works . 
It outputs just one array and is equivalent to df2 [ ' array '] .sum() . 
But you have given me an idea with apply . 
Let me see if I can figure something out . 
You need ` apply ( your_func , axis=1 )` to work on a row-by-row basis . 
#CODE 
Another way would be to call ` unique ` on the transpose of your df : #CODE 
Drop values satisfying condition plus arbitrary number of next values in a pandas DataFrame 
So my final goal is to drop values in one column of a ` pandas ` ` DataFrame ` according to some condition on another column of the same ` DataFrame ` , plus several next values e.g. : #CODE 
So this will drop the records where the condition is satisfied , but how do I drop the next 3 records after the condition was satisfied too ? 
My desired output would look something like this : #CODE 
We can use the boolean condition index to slice the df using ` loc ` and set the following values : #CODE 
Panda's boxplot but not showing the box 
Note , ` showbox ` and ` whiskerprops ` are the ` kwds ` of boxplot , which are in turn passed to ` matplotlib.boxplot ` . 
Applying aggregate function on columns of Pandas pivot table 
I generated the following pivot table via taking maximum of values in ` Z ` column : #CODE 
Here's a fairly general solution you can apply to multiple columns . 
The ' To ' column doesn't need to be rounded , I just included it for the generality of two columns rather than one : #CODE 
428 base , mult = _gfc ( freq ) 
--> 429 return tslib.dt64arr_to_periodarr ( data.view ( ' i8 ') , base , tz ) 
I could do a left merge , but I would end up with a huge file . 
Is there any way to add specific rows from df2 to df1 using merge ? 
Unclear why you think a left merge would produce a huge file , by performing a left merge on the product id you are stating that you are only interested in matches in the product_id column only 
Just perform a left ` merge ` on ' product_id ' column : #CODE 
What would be the Python equivalent ? 
I cannot think of a way to translate this where statement into pandas syntax . 
The only way I can think of is to add an arbitrary field to people_usa ( e.g. ` people_usa [ ' dummy '] =1 `) , do a left join , then take only the records where ' dummy ' is nan , then delete the dummy field - which seems a bit convoluted . 
Does this work only on the index of the dataframe ? 
I'd like the option to specify the field ( s ) to apply this to 
Is there any easy way to do this if you have multiple columns to check / join ? 
You could do a ` merge ` and then eliminate the rows that exist in the merged df otherwise you'd have to build a boolean condition for all the columns you want to compare but presumably when checking the multiple columns you're stating that it's unique for those columns , correct ? 
For instance it's not a match if say col1 and col2 match but col3 does not 
Yes merge is what I have been doing but it feels like a hassle . 
I've come up with this , using itertools , to find mid-day timestamps and group them by date , and now I'm coming up short trying to apply imap to find the means . 
#CODE 
Since not sure what your end output should look like , just create a time-based grouper manually ( this is essentially a resample ) , but doesn't do anything with the final results ( its just a list of the aggregated values ) #CODE 
You can get reasonable fancy here and say return a pandas object ( and potentially ` concat ` them ) . 
and I want to pivot it like this : #CODE 
I am calling a function from within a ' for each loop ' which attempts to insert values into a Pandas DataFrame based on a specified column start and end location . 
The function is this : #CODE 
My issue is that despite the same starting conditions when I call this function it seems to generate a list of inconsistent length . 
e.g. with values of srowb = 1 and erowb = 18 it will generate a list ( tmp_brollb ) which has either len ( tmp_brollb ) = 17 or len ( tmp_brollb ) = 18 
Use ` max ` and check for equality using ` eq ` and cast the boolean df to int using ` astype ` , this will convert ` True ` and ` False ` to ` 1 ` and ` 0 ` : #CODE 
Thanks @USER . 
Did you try my original post ? 
I would be interested to know how much time this one is taking compared to yours ? 
` for i in range ( len ( df )): ... 
df.loc [ i ] [ df.loc [ i ] .idxmax ( axis=1 )] = 1 ... 
df.loc [ i ] [ df.loc [ i ] ! = 1 ] = 0 ` 
I am trying to normalize the missing values in matrix . 
Here is the code . 
#CODE 
Last line should replace the values in dataset1 by mean values from ` ds2_mean [ 1 ]` . 
But it does not do . 
Anything wrong here ? 
And after that can I replace NaN with the average value of it's neighbours in dataset1 ? 
it does wrong . 
For any x in dataset2 it has mapped value in col2 . 
It should replace all values of x in ds1 by mapped value . 
But this also does not do it 
Sorry can you explain clearer , what are you mapping from what to what exactly ? 
By default fillna will use the index so how do you want the mapping from ` ds2 ` to map to the missing values in ` ds1 ` ? 
Are you wanting to map using the values in ` ds2 [ 0 ]` as the index lookup ? 
So use the index from ` ds1 ` find value in ` ds2 [ 0 ]` and return ` ds2 [ 1 ]` ? 
yes , I want to use the index from ds1 find value in ds2 [ 0 ] and replace it with ds2 [ 1 ]" sorry for inconvenience 
I want to add a new column which contains values based on df [ ' diff '] 
When using ` DataFrame.apply ` if you use ` axis=0 ` it applies the condition through columns , to use ` apply ` to go through each row , you need ` axis=1 ` . 
But given that , you can use ` Series.apply ` instead of ` DataFrame.apply ` on the `' diff '` series . 
Example - #CODE 
You can just set all the values that meet your criteria rather than looping over the df by calling ` apply ` so the following should work and as it's vectorised will scale better for larger datasets : #CODE 
this will set all rows that meet the criteria , the problem using ` apply ` is that it's just syntactic sugar for a ` for ` loop and where possible this should be avoided where a vectorised solution exists . 
Then you can ` stack ` ( first by `' Marker '` then by `' mrk '`) : #CODE 
Python DataFrame - apply different calculations due to a column's value 
You could do this using 2 ` loc ` calls : #CODE 
There are two reasons whiskers length vary from one boxplot to any other boxplot 
Are you asking why the top whisker isn't the same length as the bottom ? 
I think the whiskers are actually the lowest or highest data point within 1.5 IQR . 
So if there are no data points between Q3 and Q3 + 1.5 IQR , then the top whisker won't show up . 
For the one boxplot where the are outliers beyond the whiskers on both the top and the bottom , the whiskers do look about the same size . 
`` hist `` -> `` histogram `` ( `` hist `` is pyplot or something ) . 
There is a pandas equivalent to this ` cut ` there is a section describing this here . 
` cut ` returns the open closed intervals for each value : #CODE 
Pandas Dataframe , Apply Function , Return Index 
Then I can apply the function to my dataframe , grouped by I D: #CODE 
If I resample this DataField by any frequency , the timezone is kept : #CODE 
their are a couple of outstanding bugs w.r.t to resample and extra binning : #URL if you would like to investigate and try to pinpoint ( or better yet fix ) would be appreciated ! 
you can comment on that issue directly 
@USER ; You mean to the stack exchange answer ? 
I think that I understand what's going on : create a frequency table of ALL words . 
After each operation , drop all relevant columns , then finally count all remaining columns . 
Also , I quickly tried this in Python 3.4.3 and I got the error that freqDf isn't defined . 
Should I first create a new table named freqDf ? 
` df.precedingWord.isin ( neuter )` is just a Series of True or False ( results of the previous test ` isin `) , and pandas will just access True indexes with ` loc ` 
I have tried a some join / merge ideas but can't seem to get it to work . 
Just ` concat ` them and pass param ` axis=1 ` : #CODE 
Or ` merge ` on ' Symbol ' column : #CODE 
Pandas : join with outer product 
How to join / multiply the DataFrames ` areas ` and ` demand ` together in a decent way ? 
Now ` apply ` needs to return a ` Series ` , not a ` DataFrame ` . 
One way to turn a ` DataFrame ` into a ` Series ` is to use ` stack ` . 
` stack ` this DataFrame . 
This can be done with ` unstack ` : #CODE 
` del ` + ` pivot ` turns out to be faster than ` pivot_table ` in this case . 
Maybe the reason ` pivot ` exists is because it is faster than ` pivot_table ` for those cases where it is applicable ( such as when you don't need aggregation ) . 
` apply ` is now among my top 5 functions to always remember . 
Concerning the ` pivot_table ` solution : At which point am I supposed to enter the line ? 
No matter when in my attempt above , I always get ` no item named Edge ` . 
Or pass ` axis=0 ` to ` loc ` : #CODE 
I've got 2 pandas dataframes , each of them has an index with dtype ` object ` , and in both of them I can see the value ` 533 ` . 
However , when I join them the result is empty , as one of them is the number ` 533 ` and the other is a string `" 533 "` . 
Ideally I would like something like ` apply_chunk() ` which is the same as apply but only works on a piece of the dataframe . 
This has to be a common problem though , is there a design pattern I should be using for adding columns to large pandas dataframes ? 
whats about using the apply method ? 
Anytime you find yourself using ` apply ` or ` iloc ` in a loop it's likely that Pandas is operating much slower than is optimal . 
Convert freq string to DateOffset in pandas 
In pandas documentation one can read " Under the hood , these frequency strings are being translated into an instance of pandas DateOffset " when speaking of freq string such as " W " or " W-SUN " . 
stack / unstack / pivot dataframe on python / pandas 
yes , ` isnull ` will create a boolean series , ` all ` returns ` True ` if all are ` True ` 
Then merge the sub-tables back together in a way that replaces NaN values when there is data in one of the tables . 
I regularly work with very large data sets that are too big to manipulate in memory . 
I would like to read in a csv file iteratively , append each chunk into HDFStore object , and then work with subsets of the data . 
If you replace that line with : 
I wanted to merge these files so that i have something like this #CODE 
If it's six , then you can use join method by @USER Hayden . 
Then you can simply ` join ` them : #CODE 
@USER when you do a join with 2x2 duplicates you get 4 in the joined DataFrame . 
It's unclear how pandas should join in this case , so you need to be more explicit to it ( and tell it what do you want ) . 
On the similar note , is there a way to merge values based on index . 
For example , instead of listing Bact5 in two rows , can we merge its value corresponding to file2 in one row separated by a delimeter ? 
Pandas dataframe insert rows 
I want to insert rows in DF and modify its related values : 
The code can only append rows but how to modify its values in a faster way ? 
I want to use a function from an add-in in excel and apply it to some data i have simulated in python . 
I need to be able to call the add-in and apply my data indexes there ... something along these lines : = add-in_name ( data_range1 , data_range2 , " GGCV ") 
After reading one line I append the dictionary to a list ( so , the number of dictionaries in the list is equal to the number of lines in the file ) . 
I can easily do this iteratively with loops , but I've read that you're supposed to slice / merge / join data frames holistically , so I'm trying to see if I can find a better way of doing this . 
A join will give me all the stuff that matches , but that's not exactly what I'm looking for , since I need a resulting dataframe for each key ( i.e. for every row ) in A . 
You then want to apply some function to each group of rows in ` b ` where the ` b [ " key "]` is one of the values in ` keys ` . 
Under the covers , these are really similar uses of ` apply ` . 
` loop_iter = len ( A ) / max ( A [ ' SEQ_NUM ']) 
Easy way to apply transformation from ` pandas.get_dummies ` to new data ? 
As an aside that may help you in the meantime , with datetime-indexed data , [ resample ] ( #URL ) is usually a better choice than reindex . 
Call ` transform ` on the ' measurement ' column and pass the method ` diff ` , transform returns a series with an index aligned to the original df : #CODE 
If you are intending to apply some sorting on the result of ` transform ` then sort the df first : #CODE 
Or you can slice the columns and pass this to ` drop ` : #CODE 
These values are median values I calculated from elsewhere , and I have also their variance and standard deviation ( and standard error , too ) . 
= Hash [ 0 ] was my point , but even without arithmetic , there will be a huge range values for the keys that will give potentially unfortunate results . 
if precision is to decimal place , I'd multiply it by 10 and truncate maybe . 
the documentation to concat is impenetrable and its hard to find examples of this relatively simple task in the docs 
If you had not called ` apply ` on the ` groupby ` object then you could access the ` groups ` : #CODE 
pandas groupby X , Y and select last week of X1 and X2 ( which have diff frequency ) 
Then you can select the rows you want in an apply call on the grouped object : #CODE 
If you can't upgrade or don't solve the issue you have with 0.14 , you can try to use ` ix ` instead of ` iloc ` 
How do I export multiple pivot tables from python using pandas to a single csv document ? 
Say I have a function pivots() which aggregates pivot tables #CODE 
I know how to export a single pivot table #CODE 
You can use ` to_csv ( path , mode= ' a ')` to append files . 
Use ` shift ` and ` np.log ` : #CODE 
I'd look at seeing if you can export it in it's raw form , otherwise this must be a common problem and someone somewhere has probably coded a method to strip the emojis out of the text 
Python pandas map dict keys to values 
I have a csv for input , whose row values I'd like to join into a new field . 
This new field is a constructed url , which will then be processed by the requests.post() method . 
I tried to map values to keys with a dict comprehension , but the assignment of a key like ' FIRST_NAME ' could end up mapping to values from an arbitrary field like test_df [ ' CITY '] . 
which will give you output as follows : ` [ { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } , { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } ]` ( which will give you a list that has equal length as ` test_df `) . 
This might be one possibility to easily map it to a correct row . 
Do you know if append returns a copy / view / reference of the original dataframe ? 
Right now , I am trying to replace a stored procedure with a Python service , and the temp tables with Pandas dataframes . 
You could pass an argument to ` apply ` : #CODE 
Originally , I used append api to create a single table ' impression ' , however that was taking 80sec per dataframe and given that I have almost 200 of files to be processed , the ' append ' appeared to be too slow . 
Also , why is append so much slower than put ? 
pandas merge with MultiIndex , when only one level of index is to be used as key 
I want to recover the values in the column ' _Cat ' from df2 and merge them into df1 for the appropriate values of ' _ItemId ' . 
This is almost ( I think ? ) a standard many-to-one merge , except that the appropriate key for the left df is one of MultiIndex levels . 
Or is there a better approach to this merge ? 
loc will not attempt to use a number ( eg 1 ) as a positional argument at all ( and will raise instead ); see main pandas docs / selecting data 
I have the following boxplot : #CODE 
My question is : how can I change the whiskers / quantiles being plotted in the boxplot ? 
it'll be difficult to translate those ` ddply ` calls to pandas . 
I guess ` groupby ` should be used but I find this format very cryptic so it's hard to translate to python 
If you drop the " % " sign , you can make the plot without ticks . 
Append Two Dataframes Together ( Pandas , Python3 ) 
I am trying to append / join ( ? ) two different dataframes together that don't share any overlapping data . 
I am trying to append these together using #CODE 
EDIT : in regards to Edchum's answers , I have tried merge and join but each create somewhat strange tables . 
OK , what you have to do is reindex or reset the index so they align 
Use ` concat ` and pass param ` axis=1 ` : #CODE 
` join ` also works : #CODE 
As does ` merge ` : #CODE 
In the case where the indices do not align where for example your first df has index ` [ 0 , 1 , 2 , 3 ]` and your second df has index ` [ 0 , 2 ]` this will mean that the above operations will naturally align against the first df's index resulting in a ` NaN ` row for index row ` 1 ` . 
To fix this you can reindex the second df either by calling ` reset_index() ` or assign directly like so : ` df2.index =[ 0 , 1 ]` . 
And you could always drop back to numpy operations on the numpy array ` pan.values ` if need be , though , hopefully , that would be unnecessary . 
This argument is new in 1.9 ... but there is a workaround , try ` np.linspace ( 0 , len ( pep_list ) , n+1 , endpoint=True ) .astype ( int )` 
Take the time difference ( using ` shift ` ) til the next value , and multiply ( value * seconds ): #CODE 
Then do the resample to seconds ( sum the value*seconds ): #CODE 
you can isnull ( df [ ' difference ']) will give True on NaT , so you could subtract then use mask I think 
After they are done , merge the two frames together : #CODE 
Another solution ( slightly harder ): Merge the columns ` transcript_id ` , ` gene_id ` and ` gene_name ` in another column , say ` merged_id ` and ` groupby ` on ` merged_id ` . 
Geo Pandas Data Frame / Matrix - filter / drop NaN / False values 
Then I stack the dataframe , give the index levels the desired names , and select only the rows where we have ' True ' values : #CODE 
Can you enable the debugger to get a stack trace ? 
reshape data frame in pandas with pivot table 
With pivot table you can get a matrix showing which ` baz ` corresponds to which ` qux ` : #CODE 
Rolling apply question 
For each group in the groupby object , we will want to apply a function : #CODE 
We want to take the Times column , and for each time , apply a function . 
That's done with ` applymap ` : #CODE 
Given a time ` t ` , we can select the ` Value ` s from ` subf ` whose times are in the half-open interval ` ( t-60 , t ]` using the ` ix ` method : #CODE 
pandas join data frames on similar but not identical string using lower case only 
I need to join data frames on columns that are similar but not identical . 
So I am trying to isolate the lowercase letters from each column , create new columns to join on . 
Note that this assumes collecting all ASCII characters from ` a ` to ` z ` suffices to produce values on which to join . 
You can of course extend this with several joins , the join solution detects common indices automatically . 
My data is in a DataFrame of about 10378 rows and ` len ( df [ ' Full name '])` is 10378 , as expected . 
But ` len ( choices )` is only 1695 . 
I'm fairly certain that the issue is in the first line , with the ` to_dict() ` function , as ` len ( df [ ' Full name '] .astype ( str )` results in 10378 and ` len ( df [ ' Full name '] .to_dict() )` results in 1695 . 
what is ` len ( df.index.unique() )` ? 
@USER using ` choices = dict ( zip ( df [ ' n '] , df [ ' Full name '] .astype ( str )))` , where df [ ' n '] is np.arange ( len ( df )) , worked fine and got what I needed . 
Had some indexing issues because I was importing the data from different Excel spreadsheets . 
This is what is happening in your case , and noted from the comments , since the amount of ` unique ` values for the index are only ` 1695 ` , we can confirm this by testing the value of ` len ( df.index.unique() )` . 
what do you mean by normalize ? 
The other way is much easier and involves using ` resample ` to convert to daily observations and backfill daily consumption . 
( Note that the first and last months are based on partial data , you may want to either drop them or pro-rate the daily consumption . ) #CODE 
Basically , after calculating the daily consumption , do a partial resample by adding the first and last day of each month . 
I will implement it and see how it goes , but can you also explain what ' 1d ' means in the resample method ? 
@USER ' 1d ' just means 1 day for the frequency of the resample . 
So I want something that will drop the ` lob ` group , but keep every record of both the ` mol ` and ` thg ` group . 
Pandas Merge 2 data frames by 2 columns each 
In each data frame i have column with the same name and values ( Key_Merge1 ) and in each data frame i have 2 different column names with same values ( Key_Merge2 ) . 
How can i merge 2 data frames by 2 columns : 
Can you post an example data and df , your text description is not clear enough but generally you want to merge and pass the list of cols to merge the ; hs and rhs on : ` pd.merge ( df1 , df2 , left_on =[ ' Key_Merge1 ' , ' Key_Merge21 '] , right_on =[ ' Key_Merge1 ' , ' Key_merge22 '])` 
OK , you have to rename ' PRODUCT_GROUP ' in DF2 in order for the ` merge ` to work : #CODE 
the merge will naturally find the 2 columns that match and perform an inner merge as desired 
I can strip out the rightmost ' .csv ' part like this : #CODE 
How to merge two DataFrame columns and apply pandas.to_datetime to it ? 
What would be a more pythonic way to merge two columns , and apply a function into the result ? 
once sorted I replace the df.index with a numerical index #CODE 
This can be accomplished with a one line solution using Pandas ' boolean indexing . 
The one-liner also employs some other tricks : Pandas ' ` map ` and ` diff ` methods and a ` lambda ` function . 
` map ` is used to apply the ` lambda ` function to all rows . 
The ` lambda ` function is needed to create a custom less-then comparison that will evaluate NaN values to True . 
There is a built in method for this ` diff ` : #CODE 
as pointed out calling ` diff ` here will lose the first row so I'm using a ugly hack where I concatenate the first row with the result of the ` diff ` so I don't lose the first row 
Using ` diff ` like this drops the first row . 
( I can also use the chunksize option and concat myself , but that seems to be a bit of a hack . ) 
Jeff , I updated sec_id and dt in the dataframe . 
Sorry , I had to update " sec_id " and " dt " to " id " and " date " . 
0.12 is fine ; FYI the format keyword doesn't do anything with append ( and it's for 0.13 anyhow ); append always is a table 
I would like to get every , let's say , 6 hours of data and independently fit a curve to that data . 
Since pandas ' ` resample ` function has a ` how ` keyword that is supposed to be any numpy array function , I thought that I could maybe try to use resample to do that with ` polyfit ` , but apparently there is no way ( right ? ) . 
Why does the second block of code not work ? 
Doesn't DataFrame.apply() default to inplace ? 
There is no inplace parameter to the apply function . 
Even if it doesn't default to inplace , shouldn't it provide an inplace parameter the way replace() does ? 
No , apply does not work inplace* . 
In general apply is slow ( since you are basically iterating through each row in python ) , and the " game " is to rewrite that function in terms of pandas / numpy native functions and indexing . 
If you want to delve into more details about the internals , check out the BlockManager in core / internals.py , this is the object which holds the underlying numpy arrays . 
* apply is not usually going to make sense inplace ( and IMO this behaviour would rarely be desired ) . 
I use this function with pandas to apply it to each month of a historical record : #CODE 
I am trying to merge tsv files using pandas but cannot get pandas to return the file contents correctly . 
You can use the vectorised ` str ` methods to replace the unwanted characters and then cast the type to int : #CODE 
perhaps ` reindex ` creates a new dataframe , ` ix ` returns a view 
@USER you are , of course , absolutely right . what do ` loc ` and ` iloc ` do ? 
The reason for the seeming redundancy is that , while using ` ix ` is syntacticly limiting ( you can only pass a single argument to ` __getitem__ `) , ` reindex ` is a method , which supports taking various optional parameters . 
I am getting different results when using ` reindex ` with ` inplace=True ` vs using ` ix ` ( I updated the OP ) 
What if you have many conditions , e.g. you want to split up the scatters into 4 types of points or even more , plotting each in different shape / color . 
How can you elegantly apply condition a , b , c , etc . and make sure you then plot " the rest " ( things not in any of these conditions ) as the last step ? 
To find points skipped due to NA , try the ` isnull ` method : ` df [ df.col3.isnull() ]` 
How do I create a pivot table in Pandas where one column is the mean of some values , and the other column is the sum of others ? 
Basically , how would I create a pivot table that consolidates data , where one of the columns of data it represents is calculated , say , by ` likelihood percentage ` ( 0.0 - 1.0 ) by taking the mean , and another is calculated by ` number ordered ` which sums all of them ? 
I think that I understand what's going on : create a frequency table of ALL words . 
After each operation , drop all relevant columns , then finally count all remaining columns . 
Also , I quickly tried this in Python 3.4.3 and I got the error that freqDf isn't defined . 
Should I first create a new table named freqDf ? 
` df.precedingWord.isin ( neuter )` is just a Series of True or False ( results of the previous test ` isin `) , and pandas will just access True indexes with ` loc ` 
I have tried a some join / merge ideas but can't seem to get it to work . 
Just ` concat ` them and pass param ` axis=1 ` : #CODE 
Or ` merge ` on ' Symbol ' column : #CODE 
Pandas : join with outer product 
How to join / multiply the DataFrames ` areas ` and ` demand ` together in a decent way ? 
Now ` apply ` needs to return a ` Series ` , not a ` DataFrame ` . 
One way to turn a ` DataFrame ` into a ` Series ` is to use ` stack ` . 
` stack ` this DataFrame . 
This can be done with ` unstack ` : #CODE 
` del ` + ` pivot ` turns out to be faster than ` pivot_table ` in this case . 
Maybe the reason ` pivot ` exists is because it is faster than ` pivot_table ` for those cases where it is applicable ( such as when you don't need aggregation ) . 
` apply ` is now among my top 5 functions to always remember . 
Concerning the ` pivot_table ` solution : At which point am I supposed to enter the line ? 
No matter when in my attempt above , I always get ` no item named Edge ` . 
Or pass ` axis=0 ` to ` loc ` : #CODE 
I've got 2 pandas dataframes , each of them has an index with dtype ` object ` , and in both of them I can see the value ` 533 ` . 
However , when I join them the result is empty , as one of them is the number ` 533 ` and the other is a string `" 533 "` . 
Ideally I would like something like ` apply_chunk() ` which is the same as apply but only works on a piece of the dataframe . 
This has to be a common problem though , is there a design pattern I should be using for adding columns to large pandas dataframes ? 
whats about using the apply method ? 
Anytime you find yourself using ` apply ` or ` iloc ` in a loop it's likely that Pandas is operating much slower than is optimal . 
Convert freq string to DateOffset in pandas 
In pandas documentation one can read " Under the hood , these frequency strings are being translated into an instance of pandas DateOffset " when speaking of freq string such as " W " or " W-SUN " . 
stack / unstack / pivot dataframe on python / pandas 
yes , ` isnull ` will create a boolean series , ` all ` returns ` True ` if all are ` True ` 
Then merge the sub-tables back together in a way that replaces NaN values when there is data in one of the tables . 
I regularly work with very large data sets that are too big to manipulate in memory . 
I would like to read in a csv file iteratively , append each chunk into HDFStore object , and then work with subsets of the data . 
If you replace that line with : 
I wanted to merge these files so that i have something like this #CODE 
If it's six , then you can use join method by @USER Hayden . 
Then you can simply ` join ` them : #CODE 
@USER when you do a join with 2x2 duplicates you get 4 in the joined DataFrame . 
It's unclear how pandas should join in this case , so you need to be more explicit to it ( and tell it what do you want ) . 
On the similar note , is there a way to merge values based on index . 
For example , instead of listing Bact5 in two rows , can we merge its value corresponding to file2 in one row separated by a delimeter ? 
Pandas dataframe insert rows 
I want to insert rows in DF and modify its related values : 
The code can only append rows but how to modify its values in a faster way ? 
I want to use a function from an add-in in excel and apply it to some data i have simulated in python . 
I need to be able to call the add-in and apply my data indexes there ... something along these lines : = add-in_name ( data_range1 , data_range2 , " GGCV ") 
After reading one line I append the dictionary to a list ( so , the number of dictionaries in the list is equal to the number of lines in the file ) . 
I can easily do this iteratively with loops , but I've read that you're supposed to slice / merge / join data frames holistically , so I'm trying to see if I can find a better way of doing this . 
A join will give me all the stuff that matches , but that's not exactly what I'm looking for , since I need a resulting dataframe for each key ( i.e. for every row ) in A . 
You then want to apply some function to each group of rows in ` b ` where the ` b [ " key "]` is one of the values in ` keys ` . 
Under the covers , these are really similar uses of ` apply ` . 
` loop_iter = len ( A ) / max ( A [ ' SEQ_NUM ']) 
Easy way to apply transformation from ` pandas.get_dummies ` to new data ? 
As an aside that may help you in the meantime , with datetime-indexed data , [ resample ] ( #URL ) is usually a better choice than reindex . 
Call ` transform ` on the ' measurement ' column and pass the method ` diff ` , transform returns a series with an index aligned to the original df : #CODE 
If you are intending to apply some sorting on the result of ` transform ` then sort the df first : #CODE 
Or you can slice the columns and pass this to ` drop ` : #CODE 
These values are median values I calculated from elsewhere , and I have also their variance and standard deviation ( and standard error , too ) . 
= Hash [ 0 ] was my point , but even without arithmetic , there will be a huge range values for the keys that will give potentially unfortunate results . 
if precision is to decimal place , I'd multiply it by 10 and truncate maybe . 
the documentation to concat is impenetrable and its hard to find examples of this relatively simple task in the docs 
If you had not called ` apply ` on the ` groupby ` object then you could access the ` groups ` : #CODE 
pandas groupby X , Y and select last week of X1 and X2 ( which have diff frequency ) 
Then you can select the rows you want in an apply call on the grouped object : #CODE 
If you can't upgrade or don't solve the issue you have with 0.14 , you can try to use ` ix ` instead of ` iloc ` 
How do I export multiple pivot tables from python using pandas to a single csv document ? 
Say I have a function pivots() which aggregates pivot tables #CODE 
I know how to export a single pivot table #CODE 
You can use ` to_csv ( path , mode= ' a ')` to append files . 
Use ` shift ` and ` np.log ` : #CODE 
I'd look at seeing if you can export it in it's raw form , otherwise this must be a common problem and someone somewhere has probably coded a method to strip the emojis out of the text 
Python pandas map dict keys to values 
I have a csv for input , whose row values I'd like to join into a new field . 
This new field is a constructed url , which will then be processed by the requests.post() method . 
I tried to map values to keys with a dict comprehension , but the assignment of a key like ' FIRST_NAME ' could end up mapping to values from an arbitrary field like test_df [ ' CITY '] . 
which will give you output as follows : ` [ { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } , { ' FIRST_NAME ' : ..., ' LAST_NAME ' : ... } ]` ( which will give you a list that has equal length as ` test_df `) . 
This might be one possibility to easily map it to a correct row . 
Do you know if append returns a copy / view / reference of the original dataframe ? 
Right now , I am trying to replace a stored procedure with a Python service , and the temp tables with Pandas dataframes . 
You could pass an argument to ` apply ` : #CODE 
Originally , I used append api to create a single table ' impression ' , however that was taking 80sec per dataframe and given that I have almost 200 of files to be processed , the ' append ' appeared to be too slow . 
Also , why is append so much slower than put ? 
pandas merge with MultiIndex , when only one level of index is to be used as key 
I want to recover the values in the column ' _Cat ' from df2 and merge them into df1 for the appropriate values of ' _ItemId ' . 
This is almost ( I think ? ) a standard many-to-one merge , except that the appropriate key for the left df is one of MultiIndex levels . 
Or is there a better approach to this merge ? 
loc will not attempt to use a number ( eg 1 ) as a positional argument at all ( and will raise instead ); see main pandas docs / selecting data 
I have the following boxplot : #CODE 
My question is : how can I change the whiskers / quantiles being plotted in the boxplot ? 
it'll be difficult to translate those ` ddply ` calls to pandas . 
I guess ` groupby ` should be used but I find this format very cryptic so it's hard to translate to python 
If you drop the " % " sign , you can make the plot without ticks . 
Append Two Dataframes Together ( Pandas , Python3 ) 
I am trying to append / join ( ? ) two different dataframes together that don't share any overlapping data . 
I am trying to append these together using #CODE 
EDIT : in regards to Edchum's answers , I have tried merge and join but each create somewhat strange tables . 
OK , what you have to do is reindex or reset the index so they align 
Use ` concat ` and pass param ` axis=1 ` : #CODE 
` join ` also works : #CODE 
As does ` merge ` : #CODE 
In the case where the indices do not align where for example your first df has index ` [ 0 , 1 , 2 , 3 ]` and your second df has index ` [ 0 , 2 ]` this will mean that the above operations will naturally align against the first df's index resulting in a ` NaN ` row for index row ` 1 ` . 
To fix this you can reindex the second df either by calling ` reset_index() ` or assign directly like so : ` df2.index =[ 0 , 1 ]` . 
And you could always drop back to numpy operations on the numpy array ` pan.values ` if need be , though , hopefully , that would be unnecessary . 
This argument is new in 1.9 ... but there is a workaround , try ` np.linspace ( 0 , len ( pep_list ) , n+1 , endpoint=True ) .astype ( int )` 
Take the time difference ( using ` shift ` ) til the next value , and multiply ( value * seconds ): #CODE 
Then do the resample to seconds ( sum the value*seconds ): #CODE 
you can isnull ( df [ ' difference ']) will give True on NaT , so you could subtract then use mask I think 
After they are done , merge the two frames together : #CODE 
Another solution ( slightly harder ): Merge the columns ` transcript_id ` , ` gene_id ` and ` gene_name ` in another column , say ` merged_id ` and ` groupby ` on ` merged_id ` . 
Geo Pandas Data Frame / Matrix - filter / drop NaN / False values 
Then I stack the dataframe , give the index levels the desired names , and select only the rows where we have ' True ' values : #CODE 
Can you enable the debugger to get a stack trace ? 
reshape data frame in pandas with pivot table 
With pivot table you can get a matrix showing which ` baz ` corresponds to which ` qux ` : #CODE 
Rolling apply question 
For each group in the groupby object , we will want to apply a function : #CODE 
We want to take the Times column , and for each time , apply a function . 
That's done with ` applymap ` : #CODE 
Given a time ` t ` , we can select the ` Value ` s from ` subf ` whose times are in the half-open interval ` ( t-60 , t ]` using the ` ix ` method : #CODE 
pandas join data frames on similar but not identical string using lower case only 
I need to join data frames on columns that are similar but not identical . 
So I am trying to isolate the lowercase letters from each column , create new columns to join on . 
Note that this assumes collecting all ASCII characters from ` a ` to ` z ` suffices to produce values on which to join . 
You can of course extend this with several joins , the join solution detects common indices automatically . 
My data is in a DataFrame of about 10378 rows and ` len ( df [ ' Full name '])` is 10378 , as expected . 
But ` len ( choices )` is only 1695 . 
I'm fairly certain that the issue is in the first line , with the ` to_dict() ` function , as ` len ( df [ ' Full name '] .astype ( str )` results in 10378 and ` len ( df [ ' Full name '] .to_dict() )` results in 1695 . 
what is ` len ( df.index.unique() )` ? 
@USER using ` choices = dict ( zip ( df [ ' n '] , df [ ' Full name '] .astype ( str )))` , where df [ ' n '] is np.arange ( len ( df )) , worked fine and got what I needed . 
Had some indexing issues because I was importing the data from different Excel spreadsheets . 
This is what is happening in your case , and noted from the comments , since the amount of ` unique ` values for the index are only ` 1695 ` , we can confirm this by testing the value of ` len ( df.index.unique() )` . 
what do you mean by normalize ? 
The other way is much easier and involves using ` resample ` to convert to daily observations and backfill daily consumption . 
( Note that the first and last months are based on partial data , you may want to either drop them or pro-rate the daily consumption . ) #CODE 
Basically , after calculating the daily consumption , do a partial resample by adding the first and last day of each month . 
I will implement it and see how it goes , but can you also explain what ' 1d ' means in the resample method ? 
@USER ' 1d ' just means 1 day for the frequency of the resample . 
So I want something that will drop the ` lob ` group , but keep every record of both the ` mol ` and ` thg ` group . 
Pandas Merge 2 data frames by 2 columns each 
In each data frame i have column with the same name and values ( Key_Merge1 ) and in each data frame i have 2 different column names with same values ( Key_Merge2 ) . 
How can i merge 2 data frames by 2 columns : 
Can you post an example data and df , your text description is not clear enough but generally you want to merge and pass the list of cols to merge the ; hs and rhs on : ` pd.merge ( df1 , df2 , left_on =[ ' Key_Merge1 ' , ' Key_Merge21 '] , right_on =[ ' Key_Merge1 ' , ' Key_merge22 '])` 
OK , you have to rename ' PRODUCT_GROUP ' in DF2 in order for the ` merge ` to work : #CODE 
the merge will naturally find the 2 columns that match and perform an inner merge as desired 
I can strip out the rightmost ' .csv ' part like this : #CODE 
How to merge two DataFrame columns and apply pandas.to_datetime to it ? 
What would be a more pythonic way to merge two columns , and apply a function into the result ? 
once sorted I replace the df.index with a numerical index #CODE 
This can be accomplished with a one line solution using Pandas ' boolean indexing . 
The one-liner also employs some other tricks : Pandas ' ` map ` and ` diff ` methods and a ` lambda ` function . 
` map ` is used to apply the ` lambda ` function to all rows . 
The ` lambda ` function is needed to create a custom less-then comparison that will evaluate NaN values to True . 
There is a built in method for this ` diff ` : #CODE 
as pointed out calling ` diff ` here will lose the first row so I'm using a ugly hack where I concatenate the first row with the result of the ` diff ` so I don't lose the first row 
Using ` diff ` like this drops the first row . 
( I can also use the chunksize option and concat myself , but that seems to be a bit of a hack . ) 
Jeff , I updated sec_id and dt in the dataframe . 
Sorry , I had to update " sec_id " and " dt " to " id " and " date " . 
0.12 is fine ; FYI the format keyword doesn't do anything with append ( and it's for 0.13 anyhow ); append always is a table 
I would like to get every , let's say , 6 hours of data and independently fit a curve to that data . 
Since pandas ' ` resample ` function has a ` how ` keyword that is supposed to be any numpy array function , I thought that I could maybe try to use resample to do that with ` polyfit ` , but apparently there is no way ( right ? ) . 
Why does the second block of code not work ? 
Doesn't DataFrame.apply() default to inplace ? 
There is no inplace parameter to the apply function . 
Even if it doesn't default to inplace , shouldn't it provide an inplace parameter the way replace() does ? 
No , apply does not work inplace* . 
In general apply is slow ( since you are basically iterating through each row in python ) , and the " game " is to rewrite that function in terms of pandas / numpy native functions and indexing . 
If you want to delve into more details about the internals , check out the BlockManager in core / internals.py , this is the object which holds the underlying numpy arrays . 
* apply is not usually going to make sense inplace ( and IMO this behaviour would rarely be desired ) . 
I use this function with pandas to apply it to each month of a historical record : #CODE 
I am trying to merge tsv files using pandas but cannot get pandas to return the file contents correctly . 
You can use the vectorised ` str ` methods to replace the unwanted characters and then cast the type to int : #CODE 
perhaps ` reindex ` creates a new dataframe , ` ix ` returns a view 
@USER you are , of course , absolutely right . what do ` loc ` and ` iloc ` do ? 
The reason for the seeming redundancy is that , while using ` ix ` is syntacticly limiting ( you can only pass a single argument to ` __getitem__ `) , ` reindex ` is a method , which supports taking various optional parameters . 
I am getting different results when using ` reindex ` with ` inplace=True ` vs using ` ix ` ( I updated the OP ) 
What if you have many conditions , e.g. you want to split up the scatters into 4 types of points or even more , plotting each in different shape / color . 
How can you elegantly apply condition a , b , c , etc . and make sure you then plot " the rest " ( things not in any of these conditions ) as the last step ? 
To find points skipped due to NA , try the ` isnull ` method : ` df [ df.col3.isnull() ]` 
How do I create a pivot table in Pandas where one column is the mean of some values , and the other column is the sum of others ? 
Basically , how would I create a pivot table that consolidates data , where one of the columns of data it represents is calculated , say , by ` likelihood percentage ` ( 0.0 - 1.0 ) by taking the mean , and another is calculated by ` number ordered ` which sums all of them ? 
Why do you say that reusing plt.figure will lead to memory issues ? 
I am using matplotlib.pyplot to create histograms . 
I corrected this using the ` set_position ` command . 
` np.asarray ( Image.fromarray ( image.astype ( ' uint8 ') *255 ) .convert ( ' L ')) .astype ( float ) / 255 ` 
python : ImportError : No module named patheffects 
import matplotlib.patheffects 
` plt.bar ( left , height , width= 0.8 , bottom=None , hold=None , ** kwargs )` 
I typically have them in the ax.plot line . 
my attempt to define the aspect ratio for both ` y ` -axis while using ` twinx ` ( which does not work ) 
For each of this point , I can calculate the value of my unordered categorical variable X . 
You can achieve it using the ` ScalarFormatter ` : #CODE 
@USER how is hexbin useful here ? 
Hmm , doesn't look like there is anything obvious is rcParams or the matplotlibrc file . 
What you're doing is aliasing ( " renaming ") matplotlib.font_manager to FontProperties so calling fontP = FontProperties() is actually calling matplotlib.font_manager which is not callable . 
I think changing the arrowstyle will help here . 
plt.plot ( x , z ) 
` plt.tight_layout ( pad= 2.0 , h_pad= 6.5 , w_pad= 4.5 )` gives the desired result ( Fig.3 ) . 
Should work without the line ` X , Y = np.meshgrid ( X , Y )` . 
O1 = np.array ( new_OI ) 
There's no need for a vector representation of each pixel , and ` imshow ` will be much faster . 
So definitely not in an equidistant meshgrid . 
Inserted after pyplot.savefig() and the problem seems to be solved . 
` numpy.polyfit ( x , y , deg , rcond=None , full=False , w=None , cov=False )` 
Simply putting in : ` bbox =d ict ( facecolor= ' blue ' , alpha= 0.5 )` in the ax.text statement changes the color . 
ax.set_xticks ([ " 1975-01-01 " , " 1980-01-01 " , " 1985-01-01 " , " 1990-01-01 " , " 1995-01-01 " , 
Also , don't mix ` pyplot ` with guis ( unless you are explicitly using a non-interactive backend ) . 
FuncAnimation 
` x2 ' = -mu / np.sqrt ( x ** 2 + y ** 2 + z ** 2 ) * x ` , 
matplotlib.pyplot.stem ( *args , ** kwargs ) 
I've confirmed that this works for ` plot() ` as well as ` imshow() ` . 
PCA ( X ) is not the same thing as PCA ( X.T ) .T 
I should have used OffsetImage and AnnotationBbox . 
Using GridSpec 
Try removing that argument from your call to ` streamplot ` . 
Try `` ax.text ( 0 , 0 , ' whatever ')`` . 
matplotlib tripcolor - removing edges 
` ax.xaxis.get_children() [ 1 ] .set_size ( 15 )` 
I have tried lots of different combinations of the ax.plot() part , but if I change it to : #CODE 
but i want to keep the ytick marks only on the left hand side . thank you . 
fig is defined as : ` fig = plt.figure() ` 
We used the same method , but I somehow missed ` set_markeredgewidth ` . 
---> 29 import matplotlib.colorbar 
ax.plot ( r.date , r.close ) 
plt.plot() 
The ` .format ` form is the preferred way to do string formatting now as opposed to the ` % ` operator . 
` GridSpec ` works for me ( ` matplotlib ` v1.5.0 in case it matters ): #CODE 
A search of the ` matplotlib.pyplot ` API documentation does not reveal anything , so I'm afraid you'll have to live with using ` matplotlib.patches.Ellipse ` 
for why ` LinearSegmentedColormap ` shows strange color , I think this link would be helpful . 
I did so both in the Windows shell and emacs shell and the same PYTHONPATH is used . doing the rcParams both return TkAgg . 
However , the savefig() output is too zoomed out , too general . 
plt.imshow ( Xt [ 0 , :] .reshape ( 105,105 ) .T , interpolation= ' nearest ' , cmap= cm.Greys_r ) and it gives me the correct flipped output . 
See [ ` matplotlib.pyplot.contourf `] ( #URL ) for example . 
Why pyplot ( matplotlib ) is joining points randomly ? 
x = np.array ([ 1 , 3 , 4 ]) , ([ 1 , 2 ]) 
` np.arange ` doc 
plt.colorbar ( cs ) 
` tick_params ` should do it . 
I have experiemented with turning interactivity on and off via ` plt.ioff() ` , but to no avail . 
don't do ` plt.xticks ... 
thx ! but in the end this worked : ` ax [ 1 ] .bar ( bins [: -1 ] , 1 . *hist / N , np.diff ( bins ) , log=True )` 
For example ` griddata ` , ` Rbf ` , ... 
It sounds like you would rather have an ` axhspan ` or ` axvspan ` . 
matplotlib : plt.isinteractive() returns 0 instead of True or False after plt.ion() is called , when the script is being run in a terminal ? 
matplotlib FuncAnimation input 
@USER Do I need to pass anything additional to ` ColorbarBase() ` ? 
In some other code , I used the OpenCV ` SaveImage ` on a single frame to provide a reference for what I would expect from ` imshow ` . 
AxesSubplot has no attribute hist2d - Matplotlib 
fig = plt.figure() 
customize ticks for AxesImage ? 
In addition , " changing plt.figure ( i )" is not correct . 
I tried it out . linecollection does not appear to work in 3D . 
You can verify it by calling ` plt.xlim() ` both before and after the ` pandas.plot() ` call . 
Did you try calling ` plt.show() ` ? 
Edit : My original answer used ` ax.scatter ` . 
The helper functions date2num() , num2date() and drange() are used to facilitate easy conversion to and from datetime and numeric ranges . 
Here's an example of how you can display multiple plots side-by-side below a larger one using Gridspec : #CODE 
I only just discovered the awesome that is ` matplotlib.mlab.psd() ` , but I am having one issue , that is : how can I change the frequency range used by the method ? 
Python Matlplotlib errorbar : some errors disappear 
@USER It says : font_manager attribute not found o_O . 
Documentation of legacy ` subplot() ` is [ here ] ( #URL ) and ` subplots() ` is [ here ] ( #URL ) . 
I am not completely sure if this is the kind of thing you're looking for , but ` legend() ` inside ` matplotlib.pyplot ` may be able to help you out . 
Is the PCA stuff really relevant ? 
Seaborns clustermap doesn't work with plt.tight_layout() . 
fig = plt.figure ( **figprops ) . 
From your remark that ` datetime ( 2009 , 10 , 7 , 0 ) .strftime ( ' %Hz%d%b ')` gave the right string I would've thought it worked . 
yerr = np.array ([ ]) 
ax = fig.add_subplot ( 111 ); 
plt.subplots : 
( It's not the edgecolor option in pcolormesh ) #CODE 
` plt.xlabel ( u'Wavelength ( \u03bc m )')` 
You may see that how ` .ix ` was used for selecting an interval . 
I am running the tutorial for pylearn2 and I'm getting some errors when it imports ` matplotlib.pyplot ` . 
then a call to ` fig.get_size_inches() ` gives ` [ 6 , 7.475 ]` . 
I am experiencing the same problem as described in import matplotlib.pyplot hangs after updating my matplotlib version ` import matplotlib.pyplot as plt ` hangs while executing ` fc-list ` . 
Using ` pcolor ` from matplotlib I am unable to do it because my pc goes easily out of memory ( more than 8G ) .. 
+ " plt.plot ([ " + d.x1 + " , " + d.x2 + "] , [ " + d.y1 + " , " + d.y2 + "] , ' k- ') \n " 
I am trying to import Pylab using the ` import matplotlib.pyplot as plt ` . 
The plt.subplots_adjust method : #CODE 
for i , j in np.where ( significant ): print i , j 
ax = fig.add_subplot ( 111 , projection= ' 3d ') 
matplotlib.pyplot does savefig() but not show() in python3 but works in 2.7 
In this case , pcolor and pcolormesh will produce the same result . 
From the documentation of ` pyplot.errorbar() ` : 
Tkinter and pyplot running out of memory 
You've already created ` ax ` with ` plt.subplots ` so don't you just need to pass ` ax=ax ` to ` merged2.fcast.plot ` instead of setting ` ax= ... 
You also should not import pylab , you should do ` import matplotlib.pyplot as plt ` 
Alright I got it to work by using plt.minorticks_on() and changing the range of x to x = np.arange ( 0 , 361 , 50 , dtype = int ) .... 
You can control the ratio of the height / width with ` ax.set_aspect ( number )` . 
What about plt.title ? 
Is there a way to darken or make smoother the HSV colours so they look more like this 
plt.plot ( arr [ ' lapse_time '] , arr [ ' contact_angle ']) 
We are looking for some ` .line ` objects and there are two . 
My ` rcParams ` are not read . 
Now , it says ` get_data ` is not defined ... after I move ` my_plot ` before the loop 
Xaxis interval y1 ( y2 ) ax2 : ( - 100000.0 , 800000.0 ) ( wrong ) 
Then define a method ` show() ` that calls ` plt.show() ` . 
` plt.setp ( plt.gca() , ' yticklabels ' , ylabels )` 
matplotlib.get_backend() MacOSX 
# matplotlib.use ( ' tkagg ') ; 
From the ` savefig() ` docs , #CODE 
You might also want to look into [ ` numpy.vectorize `] ( #URL ) . 
' and ' plt.figure() ; for result in results : plt.clf() ... 
Python , matplotlib pyplot show() not blocking 
According to the docs ` numpy.loadtxt ` is 
What is ravel() ? 
` plot() ` in ` pandas ` are build on ` matplotlib ` . 
` plt.colorbar() ` ? 
from matplotlib.ticker import LinearLocator , FormatStrFormatter 
Is matplotlib.pyplot.hist() what you are looking for ? 
File " / usr / lib / python2.7 / dist-packages / pandas / core / generic.py " , line 2018 , in astype 
Perhaps you should be calling ` axvline ` on ` ax2 ` instead of calling the ` pyplot ` method ? 
( ` np.all ( np.diff ( lats , axis=0 ) > 0 ))` , similar fro ` lons `) 
The problem is ' cause my np.mgrid should vary from -1 to 1 and have the self.width and self.height . 
awesome . and thanks for the np.transpose tip , too . 
And using ` set_bad ` , ` set_under ` and ` set_over ` is not a very good option as I would like to have a possibility to mark different pixels with different colours , and this limits their number to 3 . 
@USER numpy's ` memmap ` will probably not be useful for you . 
How about using ` interpolate() ` : #CODE 
cb.ax.set_major_formatter ( ticker.FuncFormatter ( myfmt )) 
special method ` __call__() ` is a good approach for such 
` plt.tight_layout() ` might do it . 
Unfortunately , I don't think you can simply do np.minimum ( array1 , array2 , array3 ) like to have above , so I think you need to nest the np.minimum calls . 
Using ` set_offsets ` doesn't seem to behave as I expect it to . 
Does the import of Axes3D do something behind the scenes to alter the import of pyplot ? 
` get_color() ` just returns the color attribute from a line . 
plt.tight_layout() ` 
( Now I see : ` linspace ` is even mentioned in ` arange `' s docstring ... ) 
By default ` animation.MovieWriter ` uses a ` subprocess.PIPE ` to feed the frames to the writer . 
With your ` matplotlibrc ` file , the variables ` mp.rcParams [ ' lines.linestyle ']` and ` mp.rcParams [ ' axes.grid ']` are correctly specified and this works as expected ... 
pyplot interface 
( and ` set_yticklabels ` for the y-axis ) 
but ` from matplotlib.path import Path ` throws #CODE 
The Path object does not store the points along a Bezier curve , just the minimum parameters it needs . 
Users of Path objects should not access the vertices and codes arrays 
The trick is to use Path and PathPatch . 
This allows you to get the Matplotlib Path vertices and codes in the projection coordinates which you can then convert into a new Path . 
I was playing with PathCollection ( my dyslexia is killing me switching between Path and Patch ) , and if I can get a unit circle drawn and I can apply an affine transformation to it , then I think it should work , as it doesn't seem to inherently fill the space . 
Thus the ` Spine ` objects are in question 
You can use the ` set_position() ` method of the ` Spine ` class : #CODE 
You mentioned the use of ` Locator ` and ` Formatter ` objects in your comment . 
You need to call remove on the instance of the ` Annotation ` object . 
Using Annotation Instead of Ticklabels 
What is the difference between a Text instance and string in python ? 
I believe the anti-aliasing of Text objects is up to the font engine being used . 
What you should do is save a reference to the first ` Text ` object and update its contents by calling its ` set_text() ` method . 
There is a references to the Text object returned by the original setting of suptitle in figure.texts . 
I'm having a problem with the Text object that matplotlib use to represent the ticklabels . 
I think you need to displace the ` Text ` object , using the ` set_position (( x , y ))` method . 
This includes ` Text ` objects , ` Line2D ` objects , ` collection ` objects , ` Patch ` objects ... 
Text object in matplotlib doesnt respond to zooming properly 
After drawing a matplotlib Text instance and then interactively panning , the resulting drawn text is clipped to the data window but not the surrounding bounding box . 
Something like ` Text ( 2 , 0 , u'Text ( 2 , 0 , u " Text ( 0.4 , u\ ' 0.4 \ ')")')` . 
` plt.legend ` returns a ` Legend ` object with methods that allow you to modify the appearance of the legend . 
So first we'll save the ` Legend ` object : #CODE 
In practice I seem to end up mixing them both myself in SW ; it's largely a matter of taste whether you go through the pyplot API or access the objects . pyplot is certainly very convenient although as you want to do more complex / exotic things you'll find what you can do with pyplot alone limited and you'll need to get to know at least the full API's Axes , Figure , Legend and Path objects better . 
There was a refactor of the ` Legend ` class awhile back . 
call ` Legend.get_texts() ` will get a list of Text object in the legend object : #CODE 
Some parameters could easily be read from the ` Legend ` object , others ( like ` title ` , ` fancybox `) required some ' artistics ' . 
42 from matplotlib.legend import Legend 
How do I choose the optimum width for a matplotlib Button ? 
I removed ` color=axcolor ` from the ` Button ` call ; and I added a ` plt.show() ` before the event connect , otherwise a figure window didn't appear for me ( neither through ` ipython ` , nor with ` python `) . 
button = Button ( ax=reset_axis , label= ' Reset ' , color= ' lightblue ' , hovercolor= ' 0.975 ') 
` tbar.add_button ( a Button object ); ` 
For your buttons to work , you need to keep a reference to the ` Button ` object around . 
I know how to add and work with single cursor ` self.cursor = Cursor ( self.static_canvas.Dataplot , useblit=True , color= ' red ' , linewidth=2 )` , but what I should do to create to cursors ? 
I think tillsten is right -- study how the [ ` Cursor ` class ] ( #URL ) does it . 
I've checked briefly , but I don't know how to modify the properties of an ` Arc ` instance , although I'm sure it's possible . 
You could also try passing a ` Arrow ` instance as marker , but I'm not sure whether that works . 
Currently , I don't think it possible as the ` Arrow ` class only supports `' solid ' | ' dashed ' | ' dashdot ' | ' dotted '` four different linestyles . 
The Node class has the coordinates stored as members ( x and y ) , as well as index . 
You can scale your z values to fit this range with Normalize . 
You should mention the ` Normalize ` methods here as well . 
Matplotlib provides the class ` Normalize ` for that : #CODE 
Create a new instance of ` Normalize ` for each image you want to process . 
And then you can also replicate the functionality of ` Normalize ` ( since you seem to not like it ): #CODE 
Use these to create a ` Normalize ` instance ( other normalisation classes are available , e.g. log scale ) . 
The documentation of Normalize might be a bit deceiving here : ` process_value ` is a function which is only used for preprocessing ( and static ) . 
I have several questions regarding the Normalize class in Matplotlib . 
with your own ` Normalize ` function . 
This might be why they've avoided implementing it as part of Normalize . 
matplotlib : can I create AxesSubplot objects , then add them to a Figure instance ? 
Also do you use Figure from the local matplotlib installation or not ? 
Additionally , there are functions from the pyplot interface and there are methods on the ` Figure ` class . 
I'll assume in the following that ` fig ` is an instance of a ` Figure ` : 
Both the Figure and the Axes class have a ` clear() ` method . 
I had this same problem , and it was caused by calling ` show() ` on the Figure object instead of the pyplot object . 
It got wrapped up in a subclass of Figure , which also got level-of-detail functionality and context menus . 
I have an external function which returns a Figure object , and in this situation , each Figure object is composed of exactly one Axes object . 
While a search did lead me to see that pyplot.figure() is the development team's recommended instantiation technique , it doesn't change the question : is there any way to do Axes / Figure copy construction and Figure construction via composition of copied Axes ? 
A reference to it isn't stored in the ` Figure ` object , but is stored in the ` Canvas ` object so you can destroy a window via #CODE 
Removing the ` frameon=False ` option from ` f = Figure ( figsize =( 12 , 5 ) , dpi=100 , frameon=False )` solved the issue . 
I used the Bar class to plot length of videos vs views which are the x and y values of the tuples . 
Animation will not work on inplace ( or inline ? ) on notebook . 
By artifacts , I now understand to mean you are creating a second ` Animation ` object and what you get is both of them running in parallel ( which I am not sure what I expect to happen there ) . 
by ` ax ` I mean the current ` Axis ` object , which you were getting at via ` gca ` . 
Your legend command is using the markers , not the lines as inputs by using ` plot [ 0 ]` . 
` Axes ` objects know about things like tick location and labels and the display range ( which it does by knowing about ` Axis ` object , but that is getting even more into the weeds ) . 
Use Axis method ` set_xscale ` or ` set_yscale ` . 
` ax.set_xticks() ` is a method of the ` Axes ` object , whilst ` matplotlib.ticker.FixedLocator ` is an object which is used with the ` Axis ` object ( ` Axes ` is basically the whole graph , whilst ` Axis ` is just one of the ( two ) axis ( x or y ) . 
This is not particularly well documented , but ` Polygon ` objects have a pair of methods ` get_xy ` and ` set_xy ` . 
In particular , if / when there are so many circles for the figure to become connected , the ` for polygon in polygons : ` fails with a ` TypeError : ' Polygon ' object is not iterable ` . 
Just use the ` Polygon ` or ` Rectangle ` classes : #CODE 
As you can see , the edge is center-positioned along the border of the domain of the Rectangle object , and so bleeds into this domain . 
You can do this by overplotting a Rectangle patch on the cell that you would want to highlight . 
Currently I use the ` add_patch ( Rectangle ( ... ))` , but it does not fit really well . 
Unfortunately it seems that the ' Rectangle ' object has no attribute ' set_bottom ' . 
You can acces ALL the properties of a ` Tick ` object using this approach : #CODE 
Fortunately you can update figures you've moved to where you want them pretty easily , by using the object interface specifically , and updating the Axes object without creating a new figure . 
( You can also always find this data inside the ` Axes ` object if you know where to look . ) 
By deferring the expense of drawing you can greatly improve the performance of functions that make many calls to ` Figure ` and ` Axes ` methods . 
` Figure.add_subplot() ` returns an ` Axes ` instance , and ` pyplot.subplots() ` returns an ` Axis ` object as second output parameter . 
As a side note , it is better to pass ` Axes ` objects into your function than create them ( implicitly ) internally . 
Here , what you have done is capture the ` Axes ` instance that is returned from ` add_subplot() ` . 
If I create an ` Axes ` object in ` matplotlib ` and mutate it ( i.e. by plotting some data ) and then I call a function without passing my ` Axes ` object to that function then that function can still mutate my ` Axes ` . 
Pass multiple matplotlib Axes objects from generator function and display them 
The ` transform ` in this case is a ` BboxTransformTo ` object , which : 
You can use the ` transform ` keyword : #CODE 
But when I checked the source code of draw_networkx_nodes draw_networkx , I realized that it is not a straight forward task as the draw function stores the positions ( nodes and edges ) in a numpy array , send it to the ax.scatter function of matplotlib ( sourcecode ) which is a bit hard to manipulate without messing something up . 
This simply calls the draw method of the figure periodically . 
Then I just added a signal to the custom version and overrode the draw method . 
EDIT : I'm super blind , sorry for that , you're calling the draw method , however it is a good idea to add * args and ** kwargs to any overriden methods .. try that , and perhaps call FancyBboxPatch.draw at the end of overriden method 
I'm also panning and zooming into this figure , and have been using the draw method to show the new perspectives from zooming in ( using set_xlim and set_ylim ) and from panning ( drag_pan and start_pan ) . 
I would use ` ax.autoscale ( enable=False )` before your call to ` scatter ` . 
You can perhaps loop into your ` AxesSubplot ` objects and call ` autoscale ` passing the ` axis ` parameter : #CODE 
@USER - If you'd like to rescale the axes , call ` ax.autoscale() ` every time you remove a point . 
Python : Add x-y margins automatically with autoscale ( pyplot ) 
Ahh , it's the call to ` autoscale ` that I'm missing . 
The trick is at the end , in the custom refresh method ! 
For what you're wanting to do , you'd be far better off using annotate . 
It's quite simple to do manually with ` annotate ` . 
If you're wanting to adjust vertical alignment , horizontal alignment , etc , just add those as arguments to ` annotate ` ( e.g. ` horizontalalignment= ' right '` or equivalently ` ha= ' right '`) 
It's quite simple to do manually with ` annotate ` . 
You can use the annotate command to place text annotations at any x and y values you want . 
+1 Just as a side note , annotate has " offseting the annotations a little " built-in . 
While there's nothing wrong with Ofri's answer , ` annotate ` is intended especially for this purpose : #CODE 
Just use ` annotate ` and specify axis coordinates . 
It is also possible to make an arrow with text , using the annotate method . 
The closest thing I have been able to find while searching here is the annotate command , but that appears to create a fixed label on the plot . 
` plt.annotate ( ' Something ' , ( 0 , 0 ) , ( 0 , -20 ) , xycoords= ' axes fraction ' , textcoords= ' offset points ' , va= ' top ')` 
@USER I'm not sure why we have to explicitly call ` draw ` . 
The ` boxplot ` function only plots quartiles ( 0 , 25 , 50 , 75 , 100 ) . 
Is there any way I can extract these values for use in my downstream code from the boxplot object ? 
I am plotting a non-normal distribution using boxplot and interested in finding out about outliers using boxplot function of matplotlib . 
You should use the ` hist ` function . 
An similar example of such plot is " hist ( x , orientation= ' horizontal ')" . 
I need to use the ` hist ` function , I have looked elsewhere on this site but I haven't found anything . 
I tried doing hist ( x / x.sum() ) but that reduce the values of the numbers in the x axis and does not change their frequency , so the y-axis is unchanged 
How can I prevent the labels of ` xticks ` from overlapping with the labels of ` yticks ` when using ` hist ` ( or other plotting commands ) in matplotlib ? 
An easy way to do this is to add ` 0j ` to the argument of the ` sqrt ` , like this , ` sqrt ( 1+0j- ( 1 / x ) **2 )` . 
You need to set the new coordinates of the datapoints in the internal ` _ofsets3d ` variable of the ` Line3DCollection ` object returned by the ` scatter ` function . 
You can use ` scatter ` for this , but that requires having numerical values for your ` key1 ` , and you won't have a legend , as you noticed . 
The ` scatter ` and ` hist ` commands really expect ` x ` and ` y ` to be 1D arrays . 
I'm wondering if it is possible to have individual alpha values for each point to be plotted using the scatter function of Matplotlib . 
The problem happens when I call the scatter function . 
I tried adding ` / Library / TeX / Root / bin / universal-darwin ` to the Global Python Path of the Project Properties , but I still get the same errors . 
My Python Path is correct C :\ Python27_32bit\ and I also have \lib\ site-packages \ and \DLL included correctly . 
Adding Python Path on Windows 7 
Append ` ; C :\ python27\Scripts ` to the end of ` Path ` variable 
I use a palette file that I'm able to create using GIMP + Sample a Gradient Along a Path . 
Spine position is specified by a 2 tuple of ( position type , amount ) . 
Annotation along a curve in matplotlib 
Heat Map Annotation with text 
Annotation on top of the bars show the actual percentage of that category . 
Text File . 
I'm using Ubuntu and Sublime Text . 
Check this page : Text Rendering with LaTeX . 
When using Matplotlib ( Pylab ) for rendering Text with the same metrics as AriaL ( e.g. , Arial itself or Liberation Sans ) output looks ok - e.g. , the width of the legend box is right . 
Text box with line wrapping in matplotlib ? 
Python / Matplotlib - Quickly Updating Text on Axes 
Text display problem with matplotlib ? 
Network from Table ( Text / MS Excel ) .. 
Text alignment in a Matplotlib legend 
Underlining Text in Python / Matplotlib 
( Text colour comes out as black ) #CODE 
However , when I paste this code into my Wordpress page ( using the Text editor , not the visual editor ) nothing happens . 
This generates the plot but the Legend B is placed at the upper right hand side corner and Legend A is by default placed at the left hand upper corner . 
Multiple Pie Charts with a Shared Legend 
You can add a legend to only the first axes after plotting all of your pie charts : #CODE 
Matplotlib : Legend not displayed properly 
Legend colors in Matplotlib table function ? 
Legend transparency , when using secondary axis 
Legend is outside the frame . 
Button 1 : Shows the pointscores of John and Daniel in level 1 . 
How to get multiple Button Click Events in Python 
Button click -> ser.write , ser.read , draw 
Another solution is print a ' Cursor ' or marker line on the plot , and change its coordinates with the mouse events . 
An Arrow would appear with data about the invisible line one , but he wanted info about line two . 
Arrow on a line plot with matplotlib 
Drawing Arrow in ( x , y ) coordinate in Python 
To be more specific , if you look at the answer to that question it shows how to make a 3D Arrow and use that to annotate points in a 3D plot -- I have used this recipe extensively and it makes me wonder how it isn't built into matplotlib yet . 
Node positions are generated , stored and assigned like this #CODE 
Node and edge attribute files are simply formatted : a node attribute file begins with the name of the attribute on the first line ( note that it cannot contain spaces ) . 
Normalize the input data and scale it 
Normalize your data . 
Normalize again 
Normalize your data , apply the colormap , save the image . matplotlib provides all the necessary functionality : #CODE 
Normalize histogram2d by bin area 
Python / Matplotlib - Figure Borders in wxPython 
gcf() means Get Current Figure 
Figure title with several colors in matplotlib 
However , the sample consists of distinct solutions which form lines in the parameter space such that putting everything into a matrix and using ` imshow ` is not desirable because of the pixelation artefacts ( Figure 1 ) . 
See Figure 2 . 
Labeling Figure from String List 
I was able to maximize Figure windows for TkAgg , QT4Agg , and wxAgg using the following lines : #CODE 
I want to draw a quadratic Figure . 
So I guess I have to built each button separately , and don't know how to " order " them in the buttons_frame 2 ) I am working within a for loop , so I can't use a " main " specifically for this . and 3 ) the Figure to plot is already created before entering the GUI . 
But I wonder if there is a way to do the same link ( when zooming on figure 1 , I get the same zoom on figure 2 on one particular axis ) when defining 2 different Figures ( I want those graph to appear far from each other so I guess that I can't put them in the same Figure ... ) 
I removed the reference to Figure 0 and had it generate a new figure each time . 
Right now , that it the best I can do , see Figure . 
Sounds like you just want an image in the Figure , right ? 
You should perhaps try some converter that can produce DXF from EPS , PDF , SVG ... 
For example see this converter in java : 
You already have one such converter in your ` np.loadtxt ` method call . 
You can achieve this by defining a converter that converts sting representations of dates to ` datenums ` . 
Matplotlib Plot Lines Above Each Bar 
Matplotlib.pyplot Bar Plot Grouping Subplots 
How to properly give Annotations to Pandas Bar Charts ? 
Pandas , Bar Chart Settings Customization 
Two Bar Plots- Non side by side 
I have a couple of problems with the Bar Chart that I'm trying to create in python . 
I have a simple graph with a Navigation Tool Bar . 
Matplotlib Subplot Animation with Basemap 
Basic Animation with matplotlib's pyplot 
How Can I Save Animation.Artist animation ? 
Animation with contours matplotlib 
I found the solution ( set 2 ) that uses the mpl Toolkit and AA to allow sharing of the X Axis and present more than 2 scales . by changing the code to allow for set 2 is where I noticed problems . 
The side scales almost look good ( exception is repeats on the right ) and I CANNOT CHANGE FONT size of the X-Axis labels and Y Axis labels . 
Reverse Z Axis on matplotlib 3D Plot 
You have an example of the broken axis in the matplotlib examples : Broken Axis 
Axis scale with bins 
Converting Integer ( Day Count ) X Axis to Months 
3d Polygon Plot in matplotlib baselines slanted 
I am trying to create my own version of the 3D Polygon plot as shown on the Matplotlib web site : 
I'm trying to make a polar chart with matplotlib and python 2.7 , but I'm struggling on how to increase the space between the X-Axis and the Tick Labels for that same axis . 
Tick labels on x-axis aren't symmetric ( Matplotlib ) 
Tick label displayed when clicking on graph ? 
Axes missing when plotting in matplotlib using vispy as backend 
In theory , this code doesn't change any coordinates ; it just gets the coordinates of each label , maps it to Axes coordinates using the Text object's internally-stored transform , and then sets the position . 
@USER I think he wants to get the tick positions in Axes coordinates . 
Axes fonts with text.usetex ' true ' does not use set font 
Also , when specifying where you want the subplot to be placed ( the third input to ` Figure.add_subplot() `) , you do not want to use ` y+1 ` because that would start at ` 1 ` and end at ` 6 ` which would go out of the available range of 0-5 . 
While it might be possible to hack ` JointGrid ` to get this to work , I suspect it will very likely be easier to just use ` kdeplot ` on one Axes , using ` twinx ` and ` twiny to place and scale the marginal plots properly . 
I want to transform those intensity values to pixel intensity values as in the following image : 
The longer the time period becomes , the sharper the peaks will become ( the Fourier transform of the cosines ): #CODE 
How to transform your data into this format is a simple question , maybe a bit googling and trial and error . 
Perhaps it is best to transform to linear coordinates and calculate how to produce the curved grid for the declination and the radial lines for the right ascension . 
Since my data is roughly spherical I triangulate the azimuth and zenith angles from the spherical coordinate transform of my data points . 
A workaround might be to log10 transform the data before plotting , but the approaches I have tried , #CODE 
Without having to transform everything to string or another kind of object ? 
However , the dates mysteriously transform themselves to an ugly and unreadable format when plotting the same data as a bar plot . 
I want to find out how to transform magnitude value of accelerometer to frequency domain . 
Then , you need to set the transform for the new lines on ` a_all ` to move them to the new axis . 
What about applying a rectangular ( ok , in three dimensions , cubic ) window to your field before fourier transform ? 
The canvas seems to get locked and after the call of the pick event I can not use the other functionalities as well . 
When it is triggered it seems that the canvas gets locked and I can not use any other functionality . 
I am trying to draw an arrow on the scatterplot . 
Are these parametric orbits , so that you could draw vertical lines for each time ( or whatever ) _t_ ? 
I've follow this subject : How to draw planes from a set of linear equations in Python ? 
I am able to draw with all variables at x-axis if convert it to bar graph . 
But in my particular case I have to draw Line2D instances using Points coordinates on top of the regular plots that are all using Data coordinates . 
I'm using imshow() to draw a 2D numpy array , so for example : #CODE 
I need to draw all function in the same window 
I want to draw a small red box around one of the ticklabels , as so : 
I draw 4D plot . 
I'd like to draw / plot an horizontal line on top of the heatmap like in this figure 
How do I draw edge labels for MultiGraph in NetworkX ? 
How to draw a contour plot using Python ? 
I tried to draw a contour plot using Python . 
The second option is a touch more verbose , but has the advantage that the y-axis limits on the second plot will autoscale as you'd expect . 
I included the code to autoscale the viewport , but that's not strictly necessary . 
possible duplicate of [ How to autoscale y axis in matplotlib ? ] ( #URL ) 
if the image is a NxM array of any type , it is interpreted through the colormap ( autoscale , if not indicated otherwise ) . 
Now I changed my mind and decide to autoscale the data ( and the view ) , expecting these limits : 
I would like to be able to autoscale a matplotlib figure to make arbitrarily placed text annotations visible . 
I have to refresh the page to get the tooltips back . 
will refresh your system's reference to the bash_profile and you should be good to go in importing and using matplotlib 
Id like the user to be able to update an existing and open axis i.e. to refresh the axis . 
Placing Custom Images in a Plot Window -- as custom data markers or to annotate those markers 
How to annotate / highlight a 3d plot in MatPlotLib 
Well , it takes a loop to annotate all data points , I thought that there should be a function that does just that . 
You can annotate a specific point in the image using ` plt.text ( x , y , str )` . 
To state it in a general form , I'm looking for a way to join several points with a gradient color line using matplotlib , and I'm not finding it anywhere . 
Now instead of straight arrows , I want to join points by curve arrows . 
If you like , join circos's google group to discuss : #URL 
Then join me in upvoting that answer and that comment :-) 
Because there have been closest points but when we join them they lead to an intersections or points which are not so close but should be joined together 
@USER Because Python automatically join two adjacent strings into one string . 
This produces a figure like the first figure at How to join overlapping circles ? 
Matplotlib has lots of pre-defined colormaps for you to use . 
Here are all of the predefined colormaps . 
For me the simplest way is plotting directly the masks with imshow , passing different colormaps . 
There is a list of colormaps to choose from here . 
There is also a way to define custom colormaps . 
The list of available colormaps by default is here . 
You can add your own colormaps to ` _cm.py ` in your mpl directory and then change your rc file . 
How can I tell append_axes that I want to append the y subplot to the right of the " main axes " containing the scatter plot ? 
I got a boxplot graph like this : 
add boxplot to other graph in python 
but when I do it on the ' link ' series I can draw the boxplot correctly . 
and I am able to draw the boxplot . 
Or , more generally , modify / transform to you heart's content , and then boxplot . 
If the ticklabels are already set to a string as in e.g. a boxplot , this is still working . 
How do I add inset axes and zoom in on the first boxplot of the two ? 
I'm not setting the position of each boxplot , so I don't know where they will appear exactly . 
I wanted to add a box for each boxplot with a zoomed-in view on a specific y-axis range . 
UserWarning : 2D hist input should be nsamples x nvariables ; 
I'm doing a hist plot and I want some numbers shown in the plot , so I put in a text box using mathtext for the text , but I doesn't work and I can't see why . 
This lead to my colorbar having sqrt ticks and I want to translate them back to the original values . 
avoiding the local cache when fetching yahoo finance data from matplotlib.finance in python 
I am fetching yahoo finance data in Python through this interface : 
It would also be worth looking at the scatter plot documentation at #URL #CODE 
I am using the following code to stitch a .png into a scatter plot from MATPLOTLIB . 
Now I want to create a scatter plot with the data above .. 
matplotlib : Understanding and changing axis labels for a scatter plot updated incrementally 
I have a script that generates scatter plots for data being generated by a hardware device . 
I build a scatter plot using matplotlib and python2.7 
Build a scatter plot for baz based on the x-axis ( foo ) and y-axis ( bar ) 
It is a normal scatter plot . 
Value Error with color array when slicing values for scatter plot 
I want to specify the frequency of markers that are printed in my scatter plot . 
Regression line and fitted curve for scatter plots in matplotlib 
If you are trying to create an animation , look in to the ` animation ` module of matplotlib , it takes care of a lot of the details for you . 
Apparently , the " Animation " class of Matplotlib runs the animation in a separate thread . 
It simply don't use the Animation class and builds its own animation from crash . 
I think ` axis ([ xo , x1 , y0 , y1 ])` is in terms of proportion of the Figure , not the data transform . 
I used the scatter function and plotted the points , but the surface function is not working ( the window is empty ) . 
The solution I found for this involves using Normalize to make a normalised colour list based on the relevant data , mapping it to a ScalarMappable , and using that to set the face colour and c limits on each frame of the animation . 
But because all of the functionality relies on the hook , when the canvas is finally shown I presume Python's garbage collection has removed the Animation instance --- since it was never assigned to a variable --- and therefore the animation can never be started . 
Notice ` xytext = ( 0 , 0 )` means no offset , and omitting ` arrowprops ` causes ` plt.annotate ` to not draw an arrow . 
OLS solution using pinv / svd #CODE 
The bad days are eliminated , and the good ones are kept . 
@USER Well in this case , ` searchsorted ` is basically looking for places or indices where elements from ` message ` exists in the keys of ` codes ` . 
@USER -- I had a hard time remembering how ` translate ` and ` maketrans ` work for quite a while too , but I've gotten used to it . 
Edit : if you're using a version of numpy > = 1.8.0 , then ` np.linalg.eigvals ` operates over the last two dimensions of whatever array you hand it , so if you reshape your input to an ` ( n_subarrays , nrows , ncols )` array you'll only have to call ` eigvals ` once : #CODE 
` reshape ` returns a view of the original array , not a copy , so the conversion to 3D only requires altering the ` shape ` and ` strides ` attributes of the array , without having to copy any of the actual data . 
sum this new array along particular axes ; and then maybe 
Calling ` reshape ` returns a view , so it doesn't incur any big copying costs or anything like that . 
so at some point in the execution it will max my memory . 
Note that extension to even higher combinatorics should be trivial , along the lines presented ; but keep an eye on the n used in that case . 
These functions return a list , which I convert to a numpy array and then sum over . 
Can it be because of the many zeros in the initial table ? 
I also understand that sum ( A , axis=1 ) will sum each row . 
But what I really want to do , is to bin ` array [: , 1 ]` by day ( as derived by the unix timestamps in array [: , 0 ]) , and plot these as a stacked histogram , with each ( colored ) stack representing a day . 
It's interesting to see that when I go back to ` nloop=1000 ` , ` nreps=3 ` I actually see a slightly * greater * rate of cache misses for the row sum ( 17% vs 13% ) , even though it's faster than the column sum . 
You can concatenate arrays in ` numpy ` . 
If you are 100% sure that l2 would only be one column then you can reshape that array to make it one dimensional before doing the subtraction . 
You won't be able to create a 2D array that way , and @USER method of returning a 1D array that you reshape afterwards is a sure go . 
I have a square matrix A ( could be any size ) and I want to take the upper triangular part and place those values in an array without the values below the center diagonal ( k=0 ) . 
You can mimic this behavior with a simple function to flatten a list : #CODE 
So ` popt ` , according to the documentation , returns * " Optimal values for the parameters so that the sum of the squared error of f ( xdata , popt ) - ydata is minimized " . 
And I'd like indices ` i ` such that , #CODE 
But , ` resize ` looks like it just might be the thing I'm looking for ... 
` rfft ` , apart from repeated terms excluded , and an almost 2x speed-up , returns the exact same you would get from ` fft ` . 
Plus , if I have 4 dimensions , I thought I should have 4 eigenvalues and not 150 like the eig gives me . 
If I run your code to generate ` d ` and ` dx ` with ` eig ` I get the following : #CODE 
In other words- it is not just taking a min or max . 
D [ I+1 , J+1 ] = map ( norm , x [ I ] -y [ J ]) + np.minimum ( np.minimum ( D [ I , J ] , D [ I , J+1 ]) , D [ I+1 , J ]) ? 
` dot ` just has tighter code for a specific combination of dimensions . 
numpy sum does not agree 
Since you are only adding many ` 1 ` s you can convert ` diff ` to ` bool ` : #CODE 
It isn't mathematically possible to represent 0 on a log scale , so the first value will have to either be masked or clipped to a very small positive number . 
possible duplicate of [ Efficiently count the number of occurrences of unique subarrays in NumPy ? ] ( #URL ) 
Or you could mask the x value as well , so the indices were consistent between x and y #CODE 
Here the ` outer ` method of the ` multiply ` ufunc is used to create the new 20x20 array . 
I have a 3D numpy array consisting of 1's and zeros defining open versus filled space in a porous solid ( it's currently a numpy Int64 array ) . 
You are attempting to broadcast a 4-D array together with a 3-D array . 
Scipy NDimage correlate : unbearably slow 
I know that I can reshape the array to a 100 x 2 array of grid points : #CODE 
You probably could get ` append ` to work , but it just does a step by step concatenate , which is slower . 
This produces a random permutation of each column's indices . 
As it happens , the histogram is enough for the former . 
I see how the symmetry of the trace lets you replace the final ` dot ` . 
In that question , I sought to sum values in a numpy structured array based on multiple criteria , including matches in a list . 
to delete the lines that had zeros in them ! 
Fill scipy / numpy matrix based on indices and values 
It looks like a vector product followed by a sum along the resulting array . 
The trick is that this convolve function can be used in-place so the double for loop : #CODE 
But this reshape should produce a ` ( n , 1 , 1 )` array , not your ` ( 1 , 1 , 1 ,... )` array . 
For an extreme example , consider a sequence that consists of 9 zeros followed by the result of a coin toss , 9 zeros and another coin toss , etc . 
If so then ` np.array ( a )` is a 2d array , and you can sum over ` axis=1 ` . 
I am trying to create a lat / lon grid that contains an array of found indices where two conditions are met for a lat / lon combination . 
This NAMBE is the absolute difference between a base vector and another vector , divided by the base vector and multiplied by a hundred , in pseudo-code notation : #CODE 
this my code to and i want to use histogram data to plot scatter where y axis is counts center from the histogram , is there any direct command or way to do this ? 
Please compile with ` cython -a ` , then show us the C code that the ` a [ 0 ] += sum ` line turns into . 
The revised question is still a duplicate , see [ this question ] ( #URL ) , and [ this question ] ( #URL ) for finding the indices . 
ValueError : operands could not be broadcast together with different shapes in numpy ? 
There are thousands of numbers below the ones shown here . 
Assuming you want to align all the arrays to the left , and pad to the right with zeros , then you could first find the maximum length with #CODE 
How to do the same If I want to apply norm column-wise to a matrix ? 
The easiest approach is to reshape to data to a long format using ` .stack ` , which can be be passed straight into rolling mean . 
It's pretty low-level , and mostly focused on how to address the more difficult problem of how to pass C++ data to and from NumPy without copying , but here's how you'd do a copied std :: vector return with that : #CODE 
` std = RMS ( data - mean )` . 
This generalized diagonal would be defined as those elements of the array whose 0th and 2nd index coincide , and would have shape ( 3 , 3 , 7 ) . 
I have a given array ` [ 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 2 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 2 , 1 , 0 , 2 , 3 `] ( arbitrary elements from 0-5 ) and I want to have a counter for the occurence of zeros in a row . 
To see the benefits of this , you need to use ` z , p , k = butter ( output= ' zpk ')` and then work with poles and zeros instead of numerator and denominator . 
In that case you would " extrapolate " zeros to the left and the right . 
can numpy interpret column of indices like matlab does 
To get the diagonal elements you can get their indices with ` np.triu_indices ` ( or , for the lower triangle , ` np.tril_indices `) and then index by them . 
The question states that the input array is of shape ` ( 128 , 36 , 8) ` and we are interested in finding unique subarrays of length ` 8 ` in the last dimension . 
What does work , however is nesting append and concatenate #CODE 
( ` b ` will be broadcast along ( ? ) the first axis ) #CODE 
As he points out , the ` [ 0 ] [ 1 ]` element is what you'd want for ` cov ( a , b )` . 
returns ` 1 ` , making the sum not commutative ! 
But as I have a log of values ( 10000+ ) , this will be quite slow . 
@USER - good point . anyway , ` diff ` works on python lists too . 
It will also work if they are both arrays that can be broadcast . 
It's column stack that requires equal length strings . 
In the end it is usually not too complicated , especially if you use [ ` mgrid `] ( #URL ) or similar to get the indices . 
The absolute error will be at most 1 / 2 ULP , 2 -150 . 
AttributeError : ' Add ' object has no attribute ' log ' Python 
Or , you could initialize an array of all zeros if you know the size of the array ahead of time . 
Are you checking shape or number of nonzero values ? 
Something like ` eigvals , eigvecs = la.eigh ( mat )` ` principal = eigvecs [: , eigvals.argmax() ]` ` if ( principal > = 0 ) .all() or ( pricipal <= 0 ) .all() : print ' all the same '` ? 
I also want bins to have a width of .5 so that I can have a bin from 10.5 to 11 or 24 to 24.5 etc ... because otherwise , python outputs the histogram with the bins random and undetermined . 
Maximum is always bigger than the minimum ( more to the right on a 1d axis , not by absolute value ) . 
should give the sum of the columns . 
Suppose , You wanna check how many times you will get six if you roll dice 10 times . 
With this option , the result will broadcast correctly 
Do you mean ` indices = np.where ( a == a.max() )` in line 3 ? 
The problem I have much later on in the code is that if one of these parameters isn't in the ASCII file it throws errors up so I have to keep adding in ones I don't need . 
` append ` adds them to the end of the list , which is exactly what you want . 
I have two 3dim numpy matrices and I want to do a dot product according to one axis without using a loop in theano . 
you have at most 4 in that dimension ( see your reshape line ) , so the index it will count are 0 and 2 ( 1 and 3 are skipped , and 3 is the last element ) . 
Once we have the indices to sort ` data ` , to get a sorted copy of the array it is faster to use the indices than to re-sort the array : #CODE 
I hope this will help you perform your transpose and column-wise operations 
It is better to specify that I'm looking for something that performs the log-sum-exp trick , doing a simply succession of exp elem-wise , summing the rows and doing a log elem-wise is trivial in ` scipy.sparse ` . 
Scipy uses ` int32 ` to store ` indptr ` and ` indices ` for the sparse formats . 
But not able to plot it as a graph ( something like a histogram ) ... that is the problem . 
It gave error testing doesnot have attribute append as its of None Type . 
In both cases , you can access individual elements by indices , like ` R [ 0 ]` ( which would give you a specific object , a ` np.void ` , that still gives you the possibility to access the fields separately ) , or by slices ` R [ 1 : -1 ]` ... 
I think you can have a sum over a sliding window ( or a rolling window ) or a mean over a sliding window . 
I got your point and I find it more logical , but when trying the code you've suggested to get rid of the second error I got another error : ` AttributeError : flatten ` 
` dot ` does many things under the hood , it is apparent that ` np.dot ( A , x )` is not calling BLAS and is somehow defaulting over to numpy's internal GEMM routine . 
Below is some code which uses a callback to print out the current azimuthal and elevation angles , as well as append them to a list for further use later . 
It's super alex , here to answer NumPy questions in the blink of an eye :) 
Your solution of searching the eigenvalues for the ones you want seems plausible enough . 
If d is larger than 8 or 9 , then bases will be sufficiently long that you probably would be better off going with the other version using the dot product . 
I'm not sure which indices i need to change to achieve the minimum and not the maximum values . 
The dimension of ` result ` has been set earlier to the correct dimension , so can check it , but it would be nice to only use the length of ` indices ` to determine it . 
Alternatively , what about applying the same function without indices along the depth axes ? 
Here's an O ( n log n ) algorithm for your problem . 
You need to add axes to ` coeffs ` so it will broadcast in the dimension ( s ) you want . 
If you want to search for a certain rank on B randomly , you need to start off with a valid B with max rank , and rotate a random column j of a random B i by a random amount . 
I want to save some histogram data in a csv file . 
I want to read a mat file back in python but I have trouble going back to a graph , because the mat file gives a numpy.ndarray type file and I need a sparse matrix to reconstruct my graph . 
numpy makes it easy to translate python objects into numpy ndarrays , and will even pick an appropriate resulting data type if one is not specified : #CODE 
This ` T ` and ` X ` broadcast together just fine , for example ` T*X ` works . 
I have a numpy matrix A and I need a function that will count ( A [ i , j ] / sum of all elements in i-th column ) - A [ i , j ] / sum of all elements in j-th row 
This also works if , instead of a single index , you provide an array of indices : #CODE 
How to solve nonlinear equation without sympy ( max and min ) ? 
Bivariate Legendre Polynomial Fitting to find orthogonal coefficents 
I have a big n-square diagonal matrix , in the scipy's sparse DIA format 
To find the most frequent value of a flat array , use ` unique ` , ` bincount ` and ` argmax ` : #CODE 
The funny thing is in the above function If i pass an extra argument and just divide sum by it , then the times are the same again . 
are the same as the ones posted in the examples of this web page . 
How to remove rings from convolve healpix map ? 
With the information of the full stack trace report the bug to the ubuntu team . 
fastest way to get lookup table indices with numpy 
Well , a few more , anyway : ` cos ` , ` pi ` , ` diag ` 
I implemented a LOWESS smoother ( which is the curve you see ) with a tight fit to eliminate noise , since the real waveforms have a non-trivial noise component , and then tried doing a rolling max with a window over the data , but I can't get anything solid . 
But sum function from numpy doesn't suport " 1:3 " 
( the ` np.nonzero ` should return a tuple with one element , an array of indices ) . 
Can the " small values of derivative " be small with respect to the sin curve ? 
6 columns , 92370574 rows , 2496502 locations , 37 months each , unique amounts for each value . 
Note that where possible , ` reshape ` will give you a view of the array . 
Here you append only a REFERENCE to your only one existing ` energy ` array . 
And you can combine the summation and multiplication into a dot product : #CODE 
For example , ` a ` is generated from ` a = z [ z ! =0 ]` ; ` a ` then changes through some processing , and now I need to insert ` nan ` s where there were originally zeros . 
I frequently use the numpy.where function to gather a tuple of indices of a matrix having some property . 
I suspect the original formula was right but you didn't encode it right in Python . 
This gets me the sum of all red combined in original - all red combined in mutated . 
` p2 = einsum ( ' nk , nk -> n ' , p1 , delta )` is the pairwise dot product of the rows of ` p1 ` and ` delta ` . 
I did the reshape , just so that both arrays are same shape , but I do not think you really need the reshaping , with the list comprehension the shape of array you get is ` ( length of string , )` 
Also , I expect the positions of the zeros to be relatively sparse ( ~1% of all bit positions ) . 
Slicing arrays with meshgrid / array indices in Numpy 
( An nonzero exit status usually indicates an error on Unix style systems . A couple programs are different , e.g. , ` diff ` . ) Try examining the ` stderr ` produced by the subprocess to see what error messages are printed there . 
To achieve exactly what you are asking for I would apply a ` [ 3x3 ]` box-filter on the image and than I would resize the matrix using nearest neighbor interpolation . 
Is there a quick way to reshape my ` csr_matrix ` without copying everything in it ? 
The catch is that I need to keep the colors exactly the way they are ( background : I'm resizing a map where provinces are color-coded ) , and so I cannot just perform a resize with bicubic interpolation , because that will also interpolate the pixel colors while smoothing . 
You should " flatten " the array of arrays first . unfortunately , there's no builtin method , see #URL 
then concatenate the saved objects whit this code : #CODE 
For something like a dot product , pandas ` DataFrames ` are generally going to be slower than a numpy array since pandas is doing ** a lot more stuff ** aligning labels , potentially dealing with heterogenous types , and so on . 
I want to pass an array of indices and column names and get a list of objects that are found in the corresponding index and column name . 
From this you would expect the total sum to be ` 100,679,697 = 200* ( 1,000,000 - 499,097 ) + 499,097 ` 
The histogram way is not the fastest , and can't tell the difference between an arbitrarily small separation of points and ` 2 * sqrt ( 2 ) * b ` ( where ` b ` is bin width ) . 
} for n=1 , 2 , 3 , 4 , 5 , 6 ( using Sum ( c_n exp ( i 2 pi n x ) ) as Fourier series ) . 
I think I can t just simple sum the " seq * " array , because instead of a chord I will get noise . 
I presume you want to transpose first : #CODE 
Oh , that's interesting you can do it with stack . 
In this case , using numpy outer operations allow you to compute the multiplications and sums at the ` C ` loop speed . 
The most efficient way is likely to use ' np.empty() ' to allocate the space / memory for your end dataset and then load data & broadcast within that using slice indexing . 
Ok , with your histogram I get at least the total number of each pair . 
This is because python's sum is basically summing a for loop over the object . 
Then the entire shape changes from ( x , y ) to merely ( x , ) and I get ' too many indices ' errors when I try to use masks . 
If reps has length d , the result will have dimension of max ( d , A.ndim ) . 
I want to do this by dividing each histogram by its maximum value so all the distributions have the same scale . 
An obvious path would be to transpose the array so that the indices that I am selecting would come up first . 
Now , for mean calculations , those numeric IDs could be used as `" weights "` for binning with ` np.bincount ` , giving us the sum of data elements corresponding to each ` ID ` . 
However , what I need is a string containing all the elements in the list linked by ' ; ' , not the list itself , so it seems like I have to sum all the elements in asString with another iteration ? 
the output I need : ` S = [ 2 , 5 , 8 , 11 , 14 ]` I thought something like : ` S1 = np.array ( L [: ] [ 1 , 0 ])` should work but whatever I try I have the error like : ` TypeError : list indices must be integers , not tuple ` . 
I need it because in the next part I will sum up this large np.array with some delta_array that has the same shape . 
Used reshape to make rows into columns . 
I understand that you could create an array of zeros and iteratively change the values in each column , but I also understand this is not an efficient method . 
I'm trying to implement the univariate gradient descent algorithm in python . 
numpy glossary says the sum along axis argument ` axis=1 ` sums over rows : " we can sum each row of an array , in which case we operate along columns , or axis 1 " . 
It also prints out the new indices signature . 
At first , your ` result ` does not look like a complex FFT output 
debug performance diff of Same code on nearly same cpu / ram 
The HTML file generated by Cython indicates that the bottleneck is the dot products ( which is expected of course ) . 
` numpy.unique ` with ` return_index=True ` will give you a list of indices to take from . 
I forgot exactly why , but there is a good reason why you calculate it as the ratio between these two averages , instead of directly averaging ` fft ( y ) / fft ( x )` . 
Do you really want this ' roll ' ? 
By adding a nonzero number at the end of the array , you can still use np.nonzero to get your desired outcome . 
which simply sorts the terms and then takes the ones which aren't equal to the previous one . 
4 : I am not sure about the indices , by writing couple of code lines I just able to get cluster indices based on fclusterdata . 
Matlab gives me a norm = 2 for your matrix . 
I first generated a labelled array of unique IDs for each discrete region , calculated sizes for each ID , masked the size array to focus only on size == 1 blobs , then index the original array and set IDs with a size == 1 to 0 : #CODE 
absolute ( a - b ) = ( atol + rtol * absolute ( b )) 
Then I reshape this to form a 2D numpy array . 
n=5 ( min length of sequence ) 
I have written a function which contains nested loops and a conditional statement ; the purpose of the loop is to return a list of indices for the nearest elements in array x when compared to array y . 
I also want to color the 1D histogram bars according to the same normalization . 
If you are calling it with an empty matrix for [ low , high ] it will just use whatever the max and min values in the array are . 
Creating a class deriving from ` ndarray ` and overriding indexing such that the absolute indices are used . 
One solution is to sort both arrays ( adding an index column so that the sorted arrays still contains the original indices ) . 
Use ` reshape ` : #CODE 
What's wrong with the normal div / mod operations ? 
You can use ` argmin ` to find the False values , and this will be faster and take less memory than using nonzero , but this is linear in the length of ` a ` . 
I'd like it to be like 8x10^8 or .8x10 ^9 to save space instead of putting all those zeros . 
The one I pointed out in a comment to other answer as to encode the binary representation of the array as a Base64 text block . 
due to broadcasting , you don't need to repeat duplicate indices , thus : #CODE 
Maybe ` flatten() ` the original array , then use your 1D solution , finally calculate the real nD indices using the original shape ? 
Note that the diagonal is always zero since ` mahalanobis ( x , x )` equals zero for 
possible duplicate of [ NumPy min / max in-place assignment ] ( #URL ) 
Once the tree structure has been built , go back and collect all the branches and leaves into the array structure and by definition , they will be unique . 
I wrote the following code but the output only contains the ids ( single column ) . 
Maximum is always bigger than the minimum ( more to the right on a 1d axis , not by absolute value ) . 
Note that ` unq_count ` doesn't count the occurrences of the last unique item , because that is not needed to split the index array . 
If yes , you can use the Linux terminal to strip quotes from the ends of the rows quickly . 
The append method for a numpy array returns a copy of the array with new items added to the end . 
I want to get the norm of this array using numpy . 
The only problem here is that I think it will append directly to the column , when I would prefer it to append to a new column . 
You don't need to import string , and you don't need to loop through all the lines and append text or count the characters . 
The transpose of the transpose of a matrix == that matrix , or , [ A^T ] ^T == A . 
Currently I am looping through the arrays and using numpy.dstack to stack the 1000 arrays into a rather large 3d array ... and then will calculate the mean across the 3rd ( ? ) dimension . 
If you strip all these out and just call lapack in your for loop ( since you already know the dimensions of your matrix and maybe know that it's real , not complex ) , things run MUCH faster ( Note that I've made my array larger ) : #CODE 
First , you have a binomial response : having or not having a particular behavior . 
The call to ` np.sqrt ` , which is a Python function call , is killing your performance You are computing the square root of scalar floating point value , so you should use the ` sqrt ` function from the C math library . 
This would call the function ` np.loadtxt ` which would load the file ` GPBUSD1d.txt '` and transpose ( " unpack ") it . 
You can't change the typing of the array in-place ( unless I'm grossly mistaken ) , but you can floor . 
Finally I just transpose the dataframe to get ids as rows and categories as columns . 
The following way of obtaining the unique elements in all sub-arrays is very fast : #CODE 
You can't use the numpy reshape for a simple reason : you have data duplicity in your original array ( time and positions ) and not in the result you want . 
So it does not make much sense to me to reshape it to a " 1d-matrix " . 
Now create 5-bit bitstrings from each integer and join them together : #CODE 
It would probably be just as much work to translate the top Matlab routine from Maurits . 
In the particular case of your example , where your unique values are sequential integers , you can use ` find_objects ` directly . 
axis=1 refers to working on rows in this 2d case ( axis=0 , in contrast , would be getting you the max in each column ) 
There are many other ` ufunc ` , and other iteration modes - ` accumulate ` , ` reduceat ` . 
All diagonal elements will be of the form ` s_i ** 2 / s_i ** 2 == 1 ` . 
@USER In the example above , I get the following error : Non-broadcastable operand with shape ( 100 ) doesn't match the broadcast shape ( 100,100 ) 
is calculated such that all but the diagonal #CODE 
To compute the number of unique elements in a numpy array , you can use ` unique ( x ) .size ` or ` len ( unique ( x ))` ( see ` numpy.unique ` ) . 
Or would that basically require implementing the outer loop in Cython ? 
For a tensor it is not clear how to define an inverse or a transpose . 
Second , you are doing transpose the hard way . 
Where does log ( b , 2 ) come from ? 
( The values in the corners correspond to the diagonal elements . ) 
I tried using the scipy.stat module by creating my numbers with ` np.random.normal ` , since it only takes data and not stat values like mean and std dev ( is there any way to use these values directly ) . 
The asymptotic complexity of both of the ` matrix_rank ` and ` det ` calls are therefore O ( n^3 ) , the complexity of LU decomposition . 
I think the np.std() is just universal std . 
Golub and Van Loan also provide a way of storing a matrix in diagonal dominant form . 
I see no reason why ` numpy ` would need to make a copy for an operation like this , as long as it does the necessary checks for overlaps ( though of course as others have noted , ` resize ` may itself have to allocate a new block of memory ) . 
I found another stack question about this here , but I am not entirely sure how it was resolved , I'm still a little confused . 
Maybe ` floor ( arange ( 0 , 10 , 0.1 ))` ? 
In python , I would like to convolve the two matrices along the second axis only . 
` view ` is basically taking your two coordinates as a single variable that can be used to find the unique coordinates . 
Keep in mind that machine precision for a 32-bit double is ~ 10^-16 , which will be an absolute limiting factor . 
Also , if there is then I could just append to the b and c arrays each time instead of overwriting and starting from scratch each loop . 
Use ` multiprocessing.Process ( target = somefunc , args = ( sa , )` ( and ` start ` , maybe ` join `) to call ` somefunc ` in a separate process , passing the shared array . 
Take a look a the concatenate function . 
Unlike Joe Kington's answer , the benefit of this is that you don't need to know the original shape of the data in the ` .mat ` file , i.e. no need to reshape upon reading in . 
but I think , finding the local max can be simplified to : #CODE 
@USER ` swapaxes ` seemed to be indistinguishable from ` transpose ( 0 , 2 , 1 )` . 
Do gradient actually compute really a gradient ? 
I would suggest to first program it with ` np.nditer ` and then translate it into C . 
As you can see , using the join function on the list ( ` binary_list `) works properly , but on the equivalent numpy array ( ` binary_split_array `) it doesn't : we can see the string returned is only 72 characters long instead of 80 . 
@USER .B . the above question is significantly different from mine ; it asks for both min and max , and it is for 2D matrix 
This will join the rows and write them to a new csv : #CODE 
The reason I have ` -det ( mat )` in the energy function is because the simulated annealing algorithm does minimization . 
Also is ` x ` unique ? 
Pandas append filtered row to another DataFrame 
Again , the code notes that set of combinations is not unique ; but it does have a unique subset , namely [[ 2 3 ] , [ 0 1 ]] , which as you just revealed , you do consider a valid combination . 
That concatenate action should be pretty fast . 
If you want to pass in the transpose , you'll need to set ` rowvar ` to zero . 
You can override this behavior by using the arguments ` vmin ` and ` vmax ` ( or ` norm `) of ` imshow ` . 
@USER , ` cs ` is sorted and ` searchsorted() ` exploits that to do a binary search - only ` O ( log ( len ( weights )))` comparisons are needed . 
Think ` flatten ` without the copy . 
In your case it looks like the weight arrays will have the same dimension as ' A ' , so you reshape them accordingly and multiply dx and dy by their individual weight vectors . 
Does this mean the standard error of the gradient or intercept ? 
Also , the algo has a lot of matrices manipulation ( fft , filters , etc . ) , so using numpy / scipy should result in faster run time . 
You can broadcast that into an array using expressions , for example #CODE 
If I use the above test on the absolute values of the angles to be tested , everything 
The returned gradient hence has 
" In the first case the gradient is 1 mV / ms , in the second case it is 50 mV / ms . 
If True , uses the old behavior from Numeric , ( correlate ( a , v ) == correlate ( v , a ) , and the conjugate is not taken for complex arrays ) . 
Why don't you just compress the files with the built-in ` gzip ` module ? 
So you need to write some function that convert a poly parameters array to a latex string , here is an example : #CODE 
In your example , the square root is calculated by evaluating the the module and the argument of your complex number ( essentially via the log function , which returns log ( module ) + i phase ) . 
I am trying to run hstack to join a column of integer values to a list of columns created by a TF-IDF ( so I can eventually use all of these columns / features in a classifier ) . 
How to pass these ` norm ` and ` cmap ` parameters in matplotlib to ` plt.show ` or ` imshow() ` ? 
Forget about the C stack , numpy objects can't use it . 
You can use the append function as he has defined . 
This can be particularly tricky when trying to append to a numpy array quickly . 
I have a question regarding to the ` fft ` and ` ifft ` functions . 
So for now , I just changed the max ( z ) to a number that I know is the max ( 1567 ) . 
The ` add ` operation does not do the same thing as ` join ` . 
You don't specify ` x ` or ` y ` , and your ` mat [: , i+1 ]` indexing will not work with a structured array . 
This is because in some cases it's not just NaNs and 1s , but other integers , which gives a std > 0 . 
` fromiter ` wants a 1d input , e.g. ` [ 1 , 2 , 3 ]` ( or the generator equivalent ) . 
read more : take() 
For this I'm using an instance of the ` numpy ` class ` RandomState ` . 
You can write a thinly wrapped subclass to ` np.ndarray ` . 
Using ` ndarray.reshape ` #CODE 
E.g. this works in the interpreter : ` >>> a = np.arange ( 10 , dtype=float ) .resize ( 1 , 5 )` , because the interpreter doesn't " see " the intermediate value . 
I attempted your suggestion but got stuck trying to iterate through the existing dtype . 
` numpy.setdiff1d ( a , a [ sel_id ])` should do the trick . 
Instead of disabling the behavior you could try using np.select : 
+1 I liked you approach , but how to make ` np.copyto() ` work with a memoryvew ? 
Just import Decimal and for the printing just write print Decimal ( ndarray [ i ]) . 
Or , for that matter , numpy.genfromtxt . 
glad to hear it - I only recently found out about ` np.einsum ` myself , and it has rocked my world ever since 
The ` dtype ` could be deduced from one ( or more ) of the dictionary items : #CODE 
I didn't realize ` array_split ` existed ! 
However , in that case , you could just do : ( ` searchsorted ` uses bisection ) #CODE 
Btw . you can also implicitly force the ` dtype ` to be ` float ` when using dots : #CODE 
dtypes . 
I would prefer using the xor ufunc I think , which is ` bitwise_xor ` ( or ` logical_xor `) : #CODE 
This is the root of why your ` fromarrays ` works , but not the ` append_fields ` . 
The dtype should be big endian . 
parameterArray += line.split() \nline = self.inputBuffer.next() \ nnp.parameterArray = np.array ( parameterArray ) 
As JoshAdel points out , ` vectorize ` wraps ` frompyfunc ` . 
Sorry , the line was output [ i , j ] = np.sum ( ssd_difference [ #URL ( ) ) 
( or ` np.array ([[ 1 ] , [ 2 ] , [ 3 ] , [ 4 ]]) .shape `) 
Thank you for the great tipp with ` plt.hist ( img.ravel() )` ! 
The ` recarray ` class accepts an aligned parameter , but looks to lose it in ` format_parser ` . 
In case someone comes past this , numpy ( as of 1.8 I think ) support higher that 2D generation of position grids with meshgrid . 
` numpy.random.choice ` is not implemented in Python but in a ` .pyx ` file which needs to be compiled to C using Cython . 
A plain ` .copy ` did work for me . 
` A [ np.ix_ ( x , y )]` 
einsum : 5.2 s 
10**423 exceeds the largest int representable as an integer ( or float ) NumPy dtype , so there is no point in using NumPy here : ` np.iinfo ( ' int64 ') .max < 10**423 ` . 
Probably , better performance is by using ` numpy.fromiter ` : #CODE 
Why are the polyfit constants from the third case listed as NAN ? 
Try ` numpy.array_split ` . 
Using np.repeat on sub-arrays 
shows that ' region ' has an ` object ` dtype : #CODE 
What I am looking for is something along the original functionality of ` np.unique ` #CODE 
In my opinion , np.matrix should override for addition and subtraction as well . 
or ` np.vstack ` , ` np.dstack ` ` np.r_ ` , ` np.c_ ` , ` np.concatenate ` depending on the desired shapes . 
TypeError when using SymPy matrices for numpy.linalg.eig 
Doing ` a.astype ( float )` actually creates a * new * ndarray which is of type ` float ` . 
Trying to vectorize the code also resulted in very poor performance , 
Also look into the genfromtxt and loadtxt family of Numpy functions . 
` coll [ 1 ] .set_color ( " r ") # this does not work , coll not indexable this way ` 
The ` testing.assert_equal ` approach is almost good , except that it presumably fails if ` __debug__ ` is False ! 
I've just checked and found out that my implementation is about 2.x times * faster * than using ` numpy.convolve ` . 
Not as concise as I wanted ( I was experimenting with ` mask_indices ` ) , but this will also do the work : #CODE 
The documentation of ` numpy.nonzero() ` describes how its result must be interpreted . 
scikits-learn pca dimension reduction issue 
` np.mean ` can also preserve dimensions if needed . 
Are there alternatives to do the sorts of things ` einsum ` can do with sparse matrices ? 
Your immediate problem is ` numpy.putmask ` . 
why not ` np.array ([ o.value1 for o in objects ])` ? 
In a comment to ` @USER ` s answer I suggested ` np.delete ` . 
I have a ` numpy.ndarray ` . 
I believe it comes down to the fact that Python calls a ` __getitem__ ` on your objects and treats the entire block of code of ` for ` loop as an inline statement . 
In the Notes section to column_stack , it points out this : 
` logical_or ( a , logical_or ( b , c ))` 
How about reading them in correctly as numpy.datetime64 objects using numpy.loadtxt ( they are coming from a csv file ) ? 
Also - I see that np.getfromtxt() has a ' dtype ' option which allows the user to specify the datatype of each column . 
No worries , the dtype is inferred as ` int64 ` unless you pass it explicitly 
whats the result of ` print a ` after ` a = np.loadtxt ` 
Keep in mind that ` np.cov ` is basically doing ` data.dot ( data.T )` . 
If you want to vectorize operations , you need to think in terms of these higher dimensional arrays . 
Does ` s2 = pd.Series ( s , dtype =o bject )` work ? 
` PyArray_DATA ` is defined in 
` a [: , : , 5 ] .shape = ( 10 , 10 , 1 )` 
Can you print ` datas [ 0 ] .shape ` ? 
actually used is this line within the definition for ` np.array_repr ` 
That's why ` dstack ` behaves the way it does . 
>>> x = np.asanyarray ( [ ] , dtype= ' float64 ') 
This doesn't work for floating point types ( it will not consider + 0.0 and - 0.0 the same value ) , and ` np.intersect1d ` uses sorting , so it is has linearithmic , not linear , performance . 
But off course , isreal would be more readable :-) 
mshgrd = ax.pcolormesh ( X , Y , Z ) 
Otherwise , the performance advantages of using numpy are quickly nullified , regardless of how you implement your ringbuffer . 
The answer is numpy.clip #CODE 
Can you please go into more depth about nesting a recarray in another by using the np.object method ? 
Why do you need ` vectorize ` for that ? 
I did try gc.colletc() without success but adding a clf() inside the loop does the trick ! 
not a bad solution ; though I am somewhat wary of the performance of random.shuffle . 
date2num , ValueError : ordinal must be > = 1 
And you could override ` __mul__ ` , ` __add__ ` , ` __sub__ ` accordingly , but I don't know exactly how numpy-like you actually * need * this to be , so I can't say for sure . 
` np.array = partial ( np.array , dtype= np.float32 )` with ` partial ` from the ` functools ` module . 
A solution that worked uses griddata . 
Numpy 1.7.0 assert_array_almost_equal documentation 
You can read matlab ( .mat ) files in Python , try this : #CODE 
Thanks for the idea of genfromtxt() . 
If you are using numpy , for multidimensional lists ` numpy.repeat ` is your best bet . 
If the following equation is element-wise True , then allclose returns ` True ` : #CODE 
` np.vstack ` just vertically stacks the arrays you pass to it , and so something else in your code may be cutting off the rest of the results inadvertently . 
If you move the line ` np_verticies= np.array ( verticies )` outside of ` Fnumpy ` and the timed section your results will be very different : #CODE 
` fromiter `' s example is essentially this : ` np.fromiter (( x*x for x in range ( 5 )) , int )` . 
In Python , I have a numpy.array of integers ` [ 2 , 4 , 7 , 8 , 9 , 10 , 15 , 10 8] ` . 
I will go with newaxis then . 
pcolormesh returns a QuadMesh . 
And when I call each of the instructions inside f() individually it gives me an other result ( which is correct ): #CODE 
Here's one vectorized approach based on ` np.einsum ` - #CODE 
What's the ` dtype ` of these arrays ? 
try adding a ` show() ` in the end 
` pandas.DataFrame ` 
You can define your own types by creating a class and writing a ` __add__ ` or ` __sub__ ` method . 
On the other hand , if I did with ` genfromtxt ` , the third column is problem because it includes comma inside double-quota . 
Apparently , if there is no ' missing_value ' attribute Netcdf4 defaults to a missing value appropriate for the dtype . 
do be aware that if you have NaNs , there is an equivalent np.nanstd with the similar ddof options 
[ True , True ]] , dtype =b ool )` 
@USER true , although ` np.array ([ x for bb in b for x in bb ])` will do the job . 
return matrix_power ( self , other ) 
vector = numpy.array ( vector ); 
If so then you should have no problem fitting the ` numpy.fft.rfftfreq ` method into your own code . 
This fails : ` einsum ( ' i ..., i ... 
Python & Numpy - create dynamic , arbitrary subsets of ndarray 
For example , I have a ` ndarray ` that is : #CODE 
I went with the np.memmap because the performance is similar to hdf5 and I already have numpy in production . 
its np.log not m.log 
what happens if you [ ` Py_INCREF ( self )`] ( #URL ) after ` .base ` assignment ? 
File " / usr / lib64 / python2.6 / site-packages / numpy / core / fromnumeric.py " , line 806 , in searchsorted 
why isn't the ` ndarray ` constructor mentioned here ? 
>>> z = numpy.array ([ 1 , 2 ] 
Is there an equivelent to ` fseek ` when using ` fromfile ` to skip the beginning of the file ? 
The linear algebra functions are generally grouped in ` numpy.linalg ` . 
np.mean : #CODE 
As others have said , 32-bit versions of numpy still support 64-bit dtypes . 
` vstack ` is coercing the type of ` d ` to the type of ` e ` . 
` df.plot ` returns an AxesSubplot , which has a ` axvspan ` method . 
With the variables defined above , ` np.searchsorted ( lat , x )` is 16x faster than the equivalent call ` np.nanargmin (( lat-x ) **2 )` on my computer . 
Pypy with iterators is still solving this about 3x faster than CPython + Numpy , even when using ` np.searchsorted ` ( see my solution ) . 
and ` hstack (( a , z ))` ? 
Have you tried passing ` interpolation= ' nearest '` to ` imshow ` ? 
` cumsum ` might not be the best example . 
I think you're after ` plt.axis ([ xmin , xmax , ymin , ymax ])` : #CODE 
Is ` ( dry , unrch ) = (( G == 3 ) .sum() , ( G == 1 ) .sum() )` more vectorized ? 
Then , ` np.array ( np.matrix ( s.strip ( ' [ ]')))` will do the same magic . 
I'm trying to vectorize Z , but I'm finding it rather difficult for a triple for loop . 
How would that be done using np.dot ? 
I was surprised how descending sorting of np.array seem so un-pythonic . 
` numpy.genfromtxt ` accepts generators , so you can chain ` genfromtext ` and ` ifilter ` : #CODE 
I also tried ` df.query() ` , but no much improvement . 
According to the documentation ( e.g. , here ) , ` PyArray_SimpleNew ` has a return of type ` PyObject * ` and thus the above should be perfectly fine . 
Edit : ` np.where ` is optional , thanks @USER . 
or with ` numpy.concatenate ` ? 
@USER you can do it , it's easy with ` np.histogram ` . 
` numpy.base_repr ` uses this , but only operates on scalars . 
Python apply_along_axis of multiple arrays 
` numpy.average() ` has a weights option , but ` numpy.std() ` does not . 
I wanted to write ` M.det() ` instead of ` numpy.linalg.det ( M )` , 
I was working with something like ``` s = pd.DataFrame ([ ' 1 ' , ' na ' , ' 3 ' , ' 4 ']) . 
tested it a bit myself : sympy.sin is much slower than numpy.sin 
I was thinking of something like ` frombuffer ` . 
Tables have an append method that can easily add additional rows . 
@USER , I tested the append method of array by measure the time it cost , since resize the array will use more time . 
I think you need to use append function to append new array with previous array , asarray function converts input to array . 
I would store all your data in a python list and use the append function to add new measurement . 
The Series must also have a ` name ` to be used with ` join ` , which gets pulled in as a new field called ` name ` . 
NumPy by itself is a fairly low-level tool , and will be very much similar to using MATLAB . pandas on the other hand provides rich time series functionality , data alignment , NA-friendly statistics , groupby , merge and join methods , and lots of other conveniences . 
already have all header names listed then you can use " join " and 
Because these are irregular operations , I can't use merge / join . 
It should be a bit quicker just by not using so many intermediary bitstrings - it's all done in the join method . 
How can i use the unique ( a , rows ) from MATLab at python ? 
If you convert your 2D coordinates into ` target_map ` into flat indices into it using ` np.ravel_multi_index ` , you can use ` np.unique ` and ` np.bincount ` to speed things up quite a bit : #CODE 
To get the unobserved values filled , we'll use the ` unstack ` and ` stack ` methods . 
The actual RMS would be ` norm ( x ) / sqrt ( x.size )` , but for minimization the constant multiplier doesn't make any difference . 
You could check whether this is the case , and append to your sample in a loop if necessary . 
The above code works , but would be better to append the data_array retrieved from for loop directly into the numpy array rather than using python list . 
If you want an intersection between the two arrays you can loop ; for i data : and get i from first array , and i from second array.But I'm not sure if I follow it correctly , you have some data which has 0 occurences in some columns of your array , if you append the other values to a new array the memory of where in the data those values came from is already automatically stored . 
( 2 ) Collect the parts of the SQL command in a list and do a ` str.join ` in the end , to avoid allocating an increasingly long string each time ( you can't really append to a string in Python as they are immutable ) . 
append C :\ libav\usr\bin\ to the ' Path ' environment variable 
How to efficiently join a list of values to a list of intervals ? 
The alignment angles may be unique , a discrete set , or a continuum as below . 
Alternatively , you could assign the flip cards their own unique IDs and store them in the same record array as the regular cards , as the properties seem to have the same names , and then have a ` flip_id ` field that would be some set value such as ` 0 ` or ` None ` for cards without flip aspects and then the ID of the flip card for those cards that do have a flip . 
You can notice that there is gray strip on top and on the left ... 
Furthermore you then go and compute the norm of those three values . 
My question concerns iterating through the rows of a data frame and on each row setting a field based on information in a different data frame . 
EDIT : Adding logic to default empty strings to ` 0 ` , use a different value if you want to handle empty strings in ` years ` colomn differently #CODE 
I would suggest that you use 2-dimensional numpy array . 
I renamed them to aa , ab and ac but still get the same error . 
In this last case , RAM usage fits the equivalent ` chunk ` size #CODE 
` pandas ` , like ` numpy ` and many other modules , is not written in pure Python - it has components written in C and Cython that get compiled into version- and platform-specific libraries during the build process . 
It gave me the error : cqid = row [ ' ClearQuest ID '] TypeError : string indices must be integers , not str ........... 
The use case is that I have different time series coming from different data sources . 
How can I get pandas Timestamp offset by certain amount of months ? 
I managed to get the stats by placing everything in nested dictionary , but I feel that there may be a much easier way to the approach by using pandas dataframes and groubpy . 
Just to get a sense of what I'm trying to achieve . 
Which is suspect is due to my data range ,, but it may well be that I don't understand the other parameters . 
Your second one doesn't really make sense as an aggregation . 
How can I get the index of certain element of a Series in python pandas ? 
( Very , very late reply - apologies . ) That's true , you'd use the method EdChum suggested for longer lists of columns . 
If actual_sum and expected_to_date are equal , put a 0 
` ts [ ts [ ' values '] 0 ]` should produce the output you are looking for . 
And I get the counts : #CODE 
The standard deviation differs between pandas and numpy . 
I would like to get rid of the loops , if that is possible . 
If I change the names then there is nothing to reference . 
I even tried building from the git , but whatever I seem to do , I get the same error : #CODE 
I want to do the following operations on the data storage : 
How do I get it to actually show the graph ? 
#URL shows a way to get the number of days in a month , making the rest more or less trivial as they don't vary . 
to create average values with an equidistant time-vector . 
I get something where all " newlines " are escaped . 
Reproducing without a data file , using Jeff's suggestion : #CODE 
However , I also want to get it on the basis of the ` Group ` variable , which means I don't want to get ` Bob `' s ` Value ` based on the ` Jared `' s ` Value ` , since those two records's ` Group ` value is different - I only compute it within each specific ` Group ` variable . 
I try to use jsonlint to validate these json files but encounter some error messages . 
The logic to arrive at that database is an intricate mix of Python processing and SQL joins done in sqlite3 . 
I want to take advantage of the ` str ` accessor to split the data into two columns , such that the first column is , Name , contains the actual name ( first name last name ) , and the second column , Email , contains the email address ) . 
In fact the only really relevant data needed for the plot is the first and second column , namely : ` Compression Force ` and ` Compression Velocity ` . 
How to get special characters from Excel to screen using pandas ? 
And replace `' Month '` with `' Day '` below . 
But if you have a huge amount to data , it * might * be interesting to think of a more complex data model . 
What are you trying to do where this is the bottleneck ? 
How can I change that and use insted the first line of output code as a column ( In this case line 10 : Sub-Data Item ... ) 
My objective was to have a DTM like the one you get in R tm . 
So right now all the data comes from each iteration group , and all of its is transformed into one column vector . 
I am trying to get to the point where I can run #CODE 
While I don't get that warning with #CODE 
Thanks @USER - I mean that , if we do ` A-B ` we should only get the NaNs in ` A ` , and not the NaNs in ` B ` . 
I have three columns in my data set , namely " age " , " race " , " sex " , that I care about . 
Cannot get the average date using pandas 
Any suggestions ? 
All values ought to be integers , no floats . 
Note : this will get tripped up by some strings , so use with caution . 
The purpose of all these stuff is a geographical representation of data on a spatial grid . 
Since Name ` C ` does not have ` 3 ` or ` 5 ` in the column ` Activity ` , I do not want to get this data frame . 
Data has to be collected before local data frame is created . 
PANDAS : Extracting values from a column by applying a condition on other columnns 
If you try to produce the groups from my example you'll see what I mean . 
` pandas ` , like ` numpy ` and many other modules , is not written in pure Python - it has components written in C and Cython that get compiled into version- and platform-specific libraries during the build process . 
Not sure how to get around this ... pretty new to pandas . 
Here's the product : #CODE 
However , as the data became large , we played with SQLAlchemy / SQLite3 . 
But this time I get another error : #CODE 
Makes the change the idea of trying to use this approach all together . 
For a generalized scenario where there are many different combinations of values under ' COL1 ' and ' COL3 ' , this works but is probably not nearly as efficient as it can be : #CODE 
Similarly in your example where you plot ` col1 , col2 ` differently based on ` col3 ` , what if there are NA values that break the association between ` col1 , col2 , col3 ` ? 
For example , I want to take values from ` col_3 ` and ` col_4 ` and use them to generate a single values . 
The speed difference is astonishing . 
The summation in one group won't reduce the size of the result , the summation I want to do is across different groups . 
If you really prefer ` 1 `' s and ` 0 `' s replace the last line with : #CODE 
So traverse the data once and generate both arrays would be preferred . 
Im not fully adjusted to how Pandas is using matplotlib so i often switch to matplotlib myself if plots get more complicated , eg : #CODE 
The table that gives this message contains a few columns , none of them have data in them . 
so yes later i have open the file but thanks to pandas i can use the ` chunksize ` command to get the information i need . 
create column names by joining two labels of different levels with pandas 
@USER so how should i write it so that the program gives seq to ' Hsequence ' column when ' Hcolumn ' contains the title from fasta file ? 
Also , in my larger directory , this is taking forever - as in , about a gig of CSVs is timing out for me ( by my hand ) at around 20 minutes . 
The key was unstacking the data first : #CODE 
I want to get the latitude and longitude coordinates for any one of the columns in the data frame below . 
Option values are restored automatically when you exit the ` with ` block . 
I am finding difficulty to plot reason every csv file starts with different date , that's the reason I was trying to convert into no . of days , so that I can plot all in one go with starting day - 1 , for example : - csv file 2 fall short as compared to csv file 1 . 
Most of the time you can get away with using something else ... 
In that case the index is composed of integers from 0 to n : #CODE 
You have a difference between a mac and a pc , and * presumably * the same code . 
Suppose you want to find the row or rows where ` beef ` production was the highest . 
The number of columns may differ and so does the column names . 
How do I avoid that and rather generate it in a sparse matrix CSR format ? 
I download and scrape a webpage for some data in TSV format . 
You can set parameter ` labels=False ` to get the integer representation #CODE 
it's not too much of a stretch to insert NaN's into the data using reindexing so that i get this : #CODE 
Any suggestions ? 
Data-driven DOM manipulation ( maybe the hardest thing to wrap one's head around ): your data gets transformed into DOM elements . 
Your regex is matching on all ` - ` characters : #CODE 
1 ) create additional columns with clock time headings for 5 minute intervals between 9:30 and 4:00 pm , so the headings of the data frame look like : 
` Index ([ u'id opinion '] , dtype= ' object ')` Thanks for the response 
The end product would be ten timeseries plots with charted lines over time for each TID . 
And get the result : #CODE 
However , I still don't get why ` iconv ` messes it up . 
If you have huge CSV data , NYSOL's mcmd is the best . 
I get #CODE 
If I use a tweaked version of @USER ' s suggestion below , I get this error : #CODE 
ValueError : Unknown format code ' f ' for object of type ' str ' - why do I get this the second time but not the first time ? 
Any suggestion about the reason ? 
I have a data set which has multiple columns , strings and integers 
is the condition , returning a booleans array of True / False for all values meeting the condition or not , and then the corresponding A values are selected 
I fixed this bug in 0.11-dev in any event , see here : #URL thanks ! 
To split ` my_data2 ` into two arrays of roughly equal size : #CODE 
to get a ` Series ` of ` list ` s of strings . 
For example , you can't sum a mix of strings and floats in pandas but Excel would silently drop the string value and sum the floats . 
Notice how the values in the second column are no longer integers , as they were originally . 
I have a large but very sparse matrix ( 50,000 rows* 100,000 columns , only 10% of the values are known ) . 
In python normally you don't need and you shouldn't use a semicolon at the end of the line . 
That's all data python is reading in , apparently : the 16 first lines , or at least I am not able to get the rest of data in . 
The problem is to find average values of temp1 , temp2 and temp3 for a period of time ( say , 2 days ) over the same intervals ( for that example - 15 minutes ) . 
In generally I wonder if pandas should not at least throw a warning , afterall broadcasting the result to both columns should be almost never what is wanted . 
I get pandas error when I try to read HDF5 format files that I have created with h5py . 
Additionally you can use numpys matrix #CODE 
I updated pandas ' sudo pip install -- upgrade pandas ' , between both of these fixes , everything worked . 
Sorry can't reproduce nor understand your real problem , please post what you see in your question 
When I used ' ethnicity ' or ' veteran ' as a value my results came out really strange and didn't match my value counts numbers . 
` post_start ` is the date that the employee started in the post , and ` change_date ` is the date that the post title was changed . 
How do I replace the ints with the float values from another column ( by same row ) , but leave all the nulls ? 
There may be a more foolproof , cleaner way of computing date time differences in pandas . 
However , to get the row sum , one needs to specify axis=1 . 
Using the second method I get the following error : #CODE 
Filter data to get only first day of the month rows 
( FYI if i insert a print print ( vals ) in the middle of that loop , it prints #CODE 
For days in a month ( ' 2015-07 ' say ) You could change #CODE 
Doesnt the frame variable get overwritten during each iteration in the loop ? 
Any other advice I can leverage in the meantime ? 
If ` Change Closing Date ` is True , I would like to add ` Closing Date2 ` column into my new column with adding 1 year . 
If you REALLY want to get by a group individually #CODE 
but I get the error : #CODE 
I am new to pandas for data analysis and I just installed pandas with required dependencies ( NumPy , python-dateutil , pytz , numexpr , bottleneck and matplotlib ) . 
What do you get if you print that ? 
Can't you use sets and intersections ? 
is there a way to insert ` s ` into ` df ` without creating a reindexed copy of ` df ` first ? 
I'm using python 2.7.5 ( with all the packages in the python ( x , y ) bundle ) , and running files from the command prompt . 
Any suggestions ?? 
This will never get the similar graph as the kernel estimate base of the original data , result : 
The working version I have is this one , but I feel there is potential for improvement , as I find my solution unreadable and I am unsure about how it would generalize to multiindexes #CODE 
Also , once you get to 15 points , you'll be able to upvote as well . 
You can then get the last first value by forward filling ` first_values ` , reindexing like ` second_values ` , stacking again and indexing into the result using the original `' time ' , ' second '` pairs : #CODE 
how do i avoid creating so many variables as I add columns together ? 
Any suggestion on how to efficiently achieve this ? 
I get : #CODE 
For instance , I can compute the value for data record 3 by taking ` len ( set ([ 4 , 4 , 6 , 12 ]))` which gives 3 . 
@USER That's a great suggestion ( for some use-cases ) it should be its own answer ( so I can upvote it ) Though it does need tweak to multiply by 100 . 
python how to sum together all values within a time interval in datetime64 ? 
was trying to do a " for i in range ( len ( results ))" before the " for item in results [ i ]" that you did but not working for me ... 
But , on the other hand , if your columns aren't in the same order , then my suggestion won't work . 
When I execute the program for the data of the same day , processor time becomes long from the same point . 
I'm new to pandas , python , and scripting in general , so am still getting my head around the basics . 
You can , for example , use interpolation to get equally spaced datapoints out off your timeseries . 
What I was hoping for was to add up all of the frequencies across the websites and to create two columns : Column A with the word , and Column B with all of the frequencies added together . 
It does not work without dropping index . 
Now I was wondering how I could subtract my multi-year timeseries from this standard year , in order to get a timeseries that show which days were below or above it's standard . 
I may try installing an older version to find out what was actually getting calculated . 
Is there any disadvantage ? 
The length of the frame is over 2 million rows and looping to extract the elements I need is a poor choice . 
edit I believe ' endog ' as defined is incorrect-I should be passing the values for which I want to predict ; therefore I've created a date range of 12 periods past the last recorded value . 
@USER It should be a little quicker with a boolean index like that , but it does do a cast ( timedelta ) so I'm not 100% sure on that . 
I still get the same TypeError message using the line you suggest . 
Use regex with ` python ` engine #CODE 
( it's pretty clear that ` id ` maps to ` individual ` , but I would clean that up too ) . 
Being able to quickly determine the time difference between Order 1 and Order 2 ( per PersonID ) would be great too . 
Thus , if there is an update to some value on a memory page , that page is 
and make this a Series , mapping names to their respective numbers : #CODE 
That is , for each second there is a value and they should not be averaged , just grouped together to a new series .. 
Specifically , in this case , I'd only like to drop row with Indices ' 1991-12-31 ' and ' 1992-01-31 ' . 
Or read it in directly as a csv , by appending ' na ' to the list of values to be considered NaN : #CODE 
I fail to see the corelation between " John " and the dates in the target . 
I get : 
The question is , how can I remove or filter out all entries that have frequency 1 ? 
For all the other names that are not in the top ten frequencies I want to combine their number of occurences together under say the name " other " . 
You should get the following result : 
Which indeed is longer ( 50 ) than my number of columns / indices ( 25 ) . 
I am new to Python ( and programming in general ! ) , trying to conduct some data analysis using Pandas . 
I would like to combine these columns into start time ( index ) and length in actual seconds . 
I'm looking to find , for each Census Block centroid , the distance to it's closest restaurant . 
You will get the exception " appended items do not match existing items in table ! 
Honestly - we were going to originally do visualizations with it ( heatmaps ) - but for a lot of reasons we're now going to use D3 ... 
For example , if I say year , the entire column needs to be appended into a list like [ 1 year , 3 minutes , 2 hours ] . 
Anyone have any suggestions for how to accomplish this ? 
Yeah I know it gives NaN padding , but only on the indices the joining is done over . 
The paired measurements should have the same month , just different years . 
You can get started on debugging this by just adding a line to your code and running again : #CODE 
When I run the solution I get the error . 
Then let's add a helper column , called Safe , that will be a concatenation of all the Safex columns . 
product 1111 non-null object 
In R , using the car package , there is a useful function ` some ( x , n )` which is similar to head but selects , in this example , 10 rows at random from x . 
The separator ( between cells ) is defined by the operating system ( at least under Windows ) , and when the system wide list separator differs from comma , pandas ( or anything else I tried ) cannot determine what separator should be used . 
Setting up a histogram with a range and an appropriate bin size is an unknown . 
Thanks TravisJ , I guess I was just struggling to get the ( ... something involving group ... ) in when i was using the ax=fig1 .... method . 
I am optimising the span of an exponential moving average and the number of lagged variables that I use in the regression . 
The error message that I get is : #CODE 
It doesn't however take advantage of the psql package in Pandas . 
On a much larger data set , this runs in 790 ms compared to 1345 ms for ajcr's and Primer's solutions . 
I've put together one approach to that solution that should scale relatively well . 
I was hoping there was an easy way to get the set of B values per each A value like ` { ' one ' :[ ' A ' , ' B '] , ' two ' :[ ' A '] , ' three ' :[ ' B '] } ` but I don't see anything like that in the pandas documentation 
To avoid chained indexing , you need to get all your conditions into a single set of brackets . 
But trying to parse the column name and hierarchy and auto-generate the insertable thing with matching index is unpleasant . 
The seaborn package will allow you to plot long form data like you have without pivoting but pandas requires shared index and one column per plotted line by default so your solution is the correct one . 
Unable to filter out missing ( NaN ) location data while using Pandas and Geocoder modules in Python 
problem is the sum i now get is lined up in week intervals but not in the right sequence . this wouldn't be a problem but i need to get to the month of each date in order to do the next step i guess . 
How could I sum consecutive day values here , so I would get something like this ? 
I changed this to use \t as the separator . 
It's possible , but if your data is organized it's very quick with shifting it 
@USER fixed , was a typo ; this take a full uri 
Just get rid of it and reindent the loop body to the left one level . 
However , I still get the warning . 
For any x in dataset2 it has mapped value in col2 . 
but you then need to store these dfs somewhere which means either in a list or tuple or some other container or use a generator 
and so on for the remaining location categories . 
@USER do ** all ** the columns in the DF require that same replacement ? 
did you get any warnings while installing numpy or pandas ? 
This ` df ` consist of volume observations at every 10 second for 22 non-consecutive days . 
I've also included a section to immediately identify any redundant genes that don't have any SNPs that fall within their range . 
Using some string formatting to get the index , but works for any combination of months ( as long as the first month is explicitly included ) . 
Any suggestions please ? 
I get : #CODE 
I understand that we can line them all together side by side so their dates match and loop row by row , but then when i have 100k different securities , this is slow in memory . 
I would like to automate this table so If I change my parameters in my code , I get a new table with that new data . 
But I get , which I cannot understand , #CODE 
Here's the product : #CODE 
( My actual problem involves parsing strings into lists , then checking for presents of a 1 or 0 in one list and if so marking the cosponsoring element in the other list with a asterix , but I didn't want to put that in my example and it is long and harder to follow . 
What is the error you get ? 
So first chunk is stored as integer and in second chunk gets NaN values and store cannot convert NaN to integer 
Just play around with it to get it right . 
I get the following error : #CODE 
Edit : Here's an example of generating a close-enough data set , so you can get some idea of what I mean : #CODE 
@USER that means you have non string values mixed in ; you need to specify ` na=True ` or ` na=False ` depending on what those values are . see my edits . 
Preferably , use Pandas for the data structure and Python for the language . 
Then as in the first point , I would like to calculate the number of continuous up and down sequences from the previous point . 
0 can be changed to 1 or other values later in the code 
and in the instance when i am able to set the index of the df to the range , the data in the 4 columns change to NaN since they have no data that matches the new index . 
( Note that this produces an unrealistically high number of flooding events , but that's just because of how the sample data is set up and not reflective of a typical pond , though I'm not an expert on pond flooding ! ) #CODE 
What output do you get from this ? 
` Ideally , for the pages that have multiple groups of 34 , i'd like to add a suffix of _1 , _2 , _3 , etc . 
That means duplicating values from cols ` product_id ` and tem_name ` as long as there are items in list ` prices ` . 
cool , but I get a syntax error for 
problem is the sum i now get is lined up in week intervals but not in the right sequence . this wouldn't be a problem but i need to get to the month of each date in order to do the next step i guess . 
For empty date cells I am inserting a NaT , which I would have thought would be fine , but in Oracle that is becoming some weird invalid time that displays as " 0001-255-255 00:00 : 00 " ( Something like MAXINT or 0 being converted to a timestamp I'm guessing ? ) #CODE 
I would like to get the following result : #CODE 
so in all both suggestions below worked for me : 
I get the following error message : #CODE 
I would suggest using the duplicated method on the Pandas Index itself : #CODE 
I would like to group the x values into equal size bins , and for each bin take the average value of both x and y . 
For this data set the two numbers are always equal . 
Do you want to check if the value is in the provided bounds and return a boolean True / False array , or you want to represent your values in categories represented by those bounds ? 
The series I'd like to get would contain : #CODE 
The error I get : #CODE 
How do you deal with apparently overlapping date ranges ? 
There are more columns in the data that are not shown above , and using this code causes the non-numeric columns to drop off . 
Any suggestions ? 
python - trying to get a new pandas release 
In the process of creating an example with code , I managed to get it working . 
Can you post raw data and example code that demonstrates this ' cutting ' off 
Running your code on the sample data produces the same result . 
I want to take advantage of sortedness since with very large series merging when we know it's sorted should be linear in total length of the arrays , whereas any sort will be non-linear . 
What if you just changed the index from date / status to date / var1 / status ? 
which when imported into pandas data frames and each joined to a common timestamp , with a day of year field added , so looking something like : #CODE 
This works only if your object is table format ( rather than fixed format ) . 
Drop range of columns by labels 
You could put ``` [ ' col1 ']``` at the end to get an int . 
Hence , the width of each bin over the interval from [ -1 , 1 ] should be ` 2 / 10= 0.20 ` ; however , the graph does not have any bins with a width of 0.20 . 
For encoding training data you can use fit_transform which will discover the category labels and create appropriate dummy variables . 
I think you're confusing how to filter here , if you're looking for a specific value then ` stock [ stock [ ' Whs '] == ' VKO ']` will return only the rows where that condition is satisfied , for your last part the reason you get an empty row is that you're slicing the first 3 rows and then comparing the first value with your string but the first string value is ' VKO ' and not ' ZZZ ' , you should do this : ` stock [ stock [ ' Whs '] == ' ZZZ ']` to filter the resuls first 
I had changed the data on the local file . 
Note that this is slightly different as we are returning the first index here ( and not the normally returned last , youy could do either ) . 
I'm not sure how the archive block reading works and how much data it loads into memory , but it's clear that you will have to somehow control the size of the chunks . 
Regarding nesting of functions : if you believe a function has a general purpose or is reusable , then it should be defined at the top level of a module or some place where other functions can call it . 
I get an error saying : lambda ( ) takes exactly 1 argument ( 3 given ) 
You can get the list of columns with : #CODE 
One is I only wanted to get the mean of the next rows that relate to the same group . 
By default this will add a column of integers ( because R factors are encoded as integers ) . 
I am trying to calculate the percent change by month for each product . 
If i want only USA Equities vs all other equity and not the enitre 89 columns how do i do it ? 
Pandas : Efficient way to get first row with element that is smaller than a given value 
I first thought this was a spacing issue in the columns values , so I replaced them with underscores , but it also doesn't work in columns which only contain a single word and no spaces ? 
Are you getting the values from the GUI ok , but your calculations are returning nothin ? 
This is not precisely what I'm after , but I think I'll have to write a loop top get that . 
Hence , the width of each bin over the interval from [ -1 , 1 ] should be ` 2 / 10= 0.20 ` ; however , the graph does not have any bins with a width of 0.20 . 
I'm having a problem trying to get a character count column of the string values in another column , and haven't figured out how to do it efficiently . 
I think I get the idea . 
How can I get the position of index ` 18 ` ? 
When using ` groupy-apply ` , instead of dropping the group key index using : #CODE 
No real advantage if there are just two categories : #CODE 
@USER I suspect something else is going on then , because when I memory profile this way ( with ` drop `) using the snippet that Michael Laszlo posted , I do not see memory growth . 
Of course you may not like the index as tuples ; you could reset the index within the list comprehension to get the following if you prefer ( for example , this if for part 1 ): #CODE 
Here is how I am trying to get the output to look like : #CODE 
memory efficient Python ( pandas ) aggregates of categories from one csv file per period 
But this last line generate the error message : ` no item named timestamp ` . 
The problem here , well the biggest one , is that your ` data ` is string , not valid data structure , same thing with dictionary inside it , you creating strings , not data structures . 
I also want to create a new column that shows the difference in days between the end and begin dates . 
In the next column ( B ) , I want to create an indexed series that begins at 1000 based on the percent changes . 
I can't use fixed position to slice it . 
This gets you to where I am . 
I am using this to generate nodes in a graph , if x1 , x2 are not exactly equal , networkx recognizes them as different nodes , if x1=x2 , i get a recombinant tree which is what i want . 
My example was not good enough , as your script smartly took the size from the length of the df . 
iPython's Rmagic is already able to perform an automagic conversion between the two in a number of situations , and might be a good way to get familiar with Python . 
So if your dataset is really large , perhaps store them first in on-disk database / HDF rather than csv file and sort them there , and then query . 
One had no problem at all ( the xlsx file , example 2 ) and the other ( xls , example 1 ) had a difference between the columns . 
then with the ` sorted ` function and ` datetime ` module ( remember the ` sorted ` function change the ` data ` it self ) #CODE 
BUT , I get the " SettingWithCopyWarning " : 
This is machine generated data . 
You can't use ` or ` with arrays , if you try this you get an error ` ValueError : The truth value of a Series is ambiguous . 
Incidentally , this is the same result that you would get with the Spearman R coefficient as well . 
How could I do to have exactly one calendar year between dates in spite of leap years ? 
" However , we still have one large difference . 
All that remains is to merged the contents of the second-level dictionaries : #CODE 
I am trying to create a single image with heatmaps representing the correlation of features of data points for each label separately . 
I have two Series which have a format equal to this : #CODE 
However , when I do the following , the error does not show up and I get the expected result : #CODE 
As brackets are part of the regex syntax if you're trying to match literal brackets you need to escape them : #CODE 
Yeah , the best idea I've had this whole time was to actually sign up to SO so I can post my own questions instead of trying to funble my way through problems by patching together answers to other peoples questions - sometimes what I need just isn't covered in other people's questions . 
But when I try and import pandas I get : #CODE 
However , I can't get the column to fill up with the appropriate user inputted value . 
Also I'm not getting any traceback messages but I think I have an idea of where my problem may be . 
Basically I'm trying to get at this : #CODE 
So this is not a fully working answer , but maybe it can be extended to ultimatively get you there . 
I have a pandas Series holding one numpy array per entry ( same length for all entries ) and I would like to convert this to a 2D numpy array . 
For example , let's say I want to select 50% ( but this could change ) . 
For this purpose I would like to find the soonest date ( month ) and from there start counting months and their averages . 
You've changed your data , so the script as written doesn't work . 
Hopefully you'll get an answer soon . 
This should get you to the point in your code where you start dropping columns and start concatenating . 
So , if k were equal to 2 , and this were my data frame : #CODE 
It seems as though the second approach , using " where " is only returning data from the last few appended files , while the first approach is returning much more data . 
I have to improve this to get rid of redundancy , and I am not sure how to go about this . 
Are you able to post raw data and code to reproduce this issue ? 
The relative size between consecutive levels . 
This will potentially cater the corner cases if you happen to have conditions like : " value " 360 then +360 else -360 but the sequence of the update will cause the results reapply , ie . 
I'll put an example of what I'm suggesting in my answer . 
But you said you want only the time points from the longest ` csv ` . 
replacing this in code just drop those whole rows ... 
Thank you for response and for helping me get to next level of pyhton , great stuff ! 
For instance , you can insert new values into the index ( and even choose what value it should have ): #CODE 
What I need to do is to compute the average temperature for every run , averaging all the temperature measurements belonging to a run . 
@USER I get ` Type Error : ' bool ' object is not callable ` when I do that 
For your specific request of entries between 12:00 to 13:00 for every single day , you can fetch the rows with : #CODE 
Also , their order matters ( they are sorted by decreasing standard deviation across rows and should appear in this order in the heatmap . ) 
I'm not averse the reformatting the data in Pandas --> dumping to CSV --> importing to NetworkX , but it seems as if I should be able to generate the edges from the index and the nodes from the values . 
What is the simplest way to get a sum of only numbers across the entire frame ? 
I think you mean a Lorenz plot : #URL This would make sense then as it requires a specific preordering of the data . 
Any suggestions ? 
Despite the title , similar problems can occur with other operating systems if you mix 32-bit and 64-fit versions . 
Use ` select_as_coordinates ` to actually execute your query ; this returns an ` Int64Index ` of the row number ( the coordinates ) . 
Somehow create a mapping so that instead of the labels being 29 , 30 etc , they say " week 29 " , " Week 30 " etc . 
The fix you describe would work , of course , but then I could skip pandas all-together and directly plot the results of my individual simulations . 
These two timezones have different names but represent the same thing , however 
which would just change the last data point . 
What do you mean by reproducible example ? 
But thought i'd make it clear what my next objective was , in case someone could illuminate a better method to get there . 
What output do you get when you just enter ` pd ` in the console ? 
When I train on each label I get et better than 73% on each label . 
but what value does it grab when indexing ? in other words , if i'm just testing one side i'll get the value corresponding to that row if true . since both sides could be true and one of them is always true , which row's values will be selected ? 
However , this is a bug as you should get an error . 
guys at least tell me why i am getting downvoted ? 
In some cases the data might be out of sync which makes direct comparisons difficult . 
I would like to generate a matrix which contains the output of the function for every combination of X and Y . 
I'm trying to get the data to in the column " Structure " to repeat the row labels so it look like this : #CODE 
decision for single rows to get converted into a series - why not a 
I get #CODE 
More info as requested #CODE 
[ Their product page ] ( #URL ) holds the answer . 
Next , you wish to get the specific groups from this ` grouped ` object . 
To get datetime64 that uses seconds directly : #CODE 
The only option you may have is to setup your data structures to be light weight so each worker isn't boated by redundant copies of the same data or excessive amounts of data which might be better off split across different workers . 
Inplace dropping seems more like idiomatic pandas to me than making a copy only to instantly garbage collect the now-defunct original . 
And fortunately , these days , ` -pylab ` has been deprecated and using ` -- matplotlib ` and importing ` pylab ` manually is encouraged . 
Are you sure you don't mean ` range ( 1 , len ( DF )): ` ? 
Where " timeblock " #1 will include the first 4:59 minutes of observation period #1 , #2 will include 5:00 to 9:59 minutes ... through to 25:00 and over , for each observation period . 
It seems like I'm maybe getting confused between the underlying data and views on it . 
When I try specifying index_col=0 , as some examples in the documentation do , I get a " IndexError : list index out of range " error , which was a solution to several related questions but for some reason isn't working for me . 
That's what I thought about my original code but for some reason when I check len ( Sframe ) at the end in the main code , it still has the duplicate values even if the conditional statement applies option 2 to remove duplicates . 
The main thing I need to do is to group the days by week such that I can get sum of the data to be by week . 
To move the third row to the first , you can create an index moving the target row to the first element . 
So using your approach , how can I : 1 ) plot the scores as a histogram in a memory-conscious way , and 2 ) extract the scores belonging to the certain cell types to plot those as well ? 
I have a CSV file , I wanted to filter it where I keep just rows where I have values in row " d " bigger then 0 . 
Simulations can be repeated for different scenarios and each one of these scenarios will produce a different hourly set of data for each room and each variable . 
To get around this , I'm passing in a large number for the max_results parameter and specifying a chunksize . 
You get back a float because each row contains a mix of ` float ` and ` int ` types . 
Should I use something different . 
The best would be to convert that one file to a an actual comma ( semicolon or other ) separated file or make sure that compound values are quoted ( " Alabama County ") and then specify the quotechar : #CODE 
I would like to calculate the average of time per org per cluster . 
I found a way which seems very inefficient ( stacking and unstacking which will create many many columns in case of millions of categories ) . 
I was going to suggest cumcount and tail ( 1 ) , but you're after something else ( these would be much faster ) . 
This is because working with dictionaries is so easy and thinking of them like simple dicts often means you can find a solution to an issue without having to get too deep into pandas . 
If you're trying to slice each string to get the substring from 5 to 7 , you need a ` : ` , not a ` , ` : #CODE 
However , since each of your new ` DataFrames ` is a summary of a single customer , I would suggest writing one function that can return all of your desired results in a single ` Series ` . 
I want to get statistics of debt_ratio based on subgroups of market_capitalization . 
Any ideas why this error might be showing up so I can know what to go after to fix ? 
I believe it is getting at what I want . 
I haven't done any stress testing but I'd imagine this could get slow on very large DataFrames . 
Is there a quick way to sort my data by a given column that only takes chunks into account and doesn't require loading entire datasets in memory ? 
Take a look at the regex docs . 
Any logic requirements ( like comparing elem+1 to elem ) should be in your question so there is no confusion . 
I'd like to get a list as ` [ ' abcd ' , ' ddse ' , ' 123d ' , ' aaaaa* ']` . 
but get the following error : 
I get #CODE 
Ideally the question would provide a self-contained piece of code generating the data structure , or even just something like ` df = [[ 1 , 2 ] , [ 2 , 3 ] , [ 4 , 5 ]]` , enough to try to get an answer without diving into panda . 
I'm running daily simulations in a batch : I do 365 simluations to get results for a full year . 
I edited my answer to use capwords per your suggestion , that fixed the problem I missed where it capitalized the ' s ' in Guy's Name . 
Does not work exactly right since categories can be mixed like that so it will produce more duplications 
Essentially , I want to look at quintiles ( since there are 5 days in a business week ) rank 1 and 5 and see how they change from week to week . 
Doing this transformation for 500,000 file takes time . 
The requirement isn't easy to wrap once mind around , so sorry If I am not being clear . 
My question is , from ` result ` how can I get the column index of the first level as list : #CODE 
This function will then work out the maximum , minimum , and return rages of values based on the fact I want 5 categories : ( 1 , 2 ) , ( 3 , 4 ) , ( 5 , 6 ) , ( 7 , 8) , ( 9 , 10 ) . 
Any suggestions of a better way ? 
In addition this is unlikely to be only time I have to do this so being able to change the numbers and columns i'm interested in would be good . 
And at that time i think the state becomes bad , so subsequent calls will lead to exceptions like these : #CODE 
Let me fix that and return to this issue , as I have read something about format changes between 10 and 12 . 
All the data , columns 222 and 333 are offset as required , but it isn't even the same size as the output in the first order . 
But I can only get this : #CODE 
I have summary-level data of the count of people by age group , city , income and the industry in which they work , or in this case four dimensions . 
@USER : Yes , it can as I said , " They all suck in different ways " . 
Error : list indices must be integers , not Series 
Just so that it does not fail silently if the wrong kind of data is passed in . 
If you want to take advantage of NumPy / Pandas to perform fast ( er ) calculations you must keep the data in a NumPy array or Pandas NDFrame . 
Your benchmark is actually too small to show the real difference . 
I was able to resolve this by opening / closing a connection each time i need to execute a query . 
I used a chunksize of 4 to make the grouping noticeable on the small dataset ; you'll want to change it to 90000 for your real dataset . 
It's mostly trial and error for me , plus knowledge that you can do a lot just with ``` rank ``` and ``` count ``` , which are both pretty fast . 
And those columns which have differing values : #CODE 
( I will use logistic regression and random forest to do the prediction , which support sparse matrix . ) Is there anyway to efficiently slicing a sparseDataFrame or for the whole process I am doing , it should be improved in anyway ? 
I looked here , but when I ran that in iPython Notebook , I don't get anything . 
does not produce any difference in terms of file size than ... 
You will either have to split the table up or choose another storage format . 
To get the number of groups you can use the ngroups attribute : #CODE 
this does not seem to work and I am not sure if it just is not possible , I can always generate separate dictionaries from a master dictionary to get around having to update data in multiple locations 
I want to compare these two data frames by row based on column Value and keep the row from first or second depending where the Value is bigger . 
Then , you can run ` sudo port install py27-pandas ` to get Python and all of the dependencies installed . 
See the shape method of the input array , and you should get something like ` ( N , )` and not ` ( N , 1 )` . 
So , will the lower value always come first , or could that change ? 
and get the following error : 
In order to have the index the exact same as the first example you'd need to change to int from float . 
The graph bit is sorted but the part i'm finding hard is the fact the column headers can change so picking up their data without manually calling them is something I'm unable to do . 
If pytables used msgpack it would be easier for other languages to read the data but obviously their target is python . 
The company name may be variable length , it will however always be after the first ` \s ` 
To get the same form of broadcasting to occur like the diagram above shows we have to decompose to numpy arrays which then become anonymous data : #CODE 
I want to generate a plot showing these dates graphically . 
How is it possible to get the label of value ' 12 ' ? 
You shall note that ` and ` and ` or ` are not appropriate for a vector of booleans , use ` ` and ` | ` instead . 
No , this table is used by a lot of other code ( mostly C# ) , I am just doing some data analysis on it from Python , so I'm not in a position to change the semantics / data structure . 
# Valid positions in output array to be changed 
Is there a bug in my code or is there another reason for the huge computation speed difference between those two lines of code ? 
I also used a longer window because there were only 15 values per array but you seemed to be planning on using the last 50 days . 
I am trying to get the data into the following shape : #CODE 
I then get the following error : #CODE 
I want to : plot a heatmap of x , y and the colour is the z value .. 
I'm also running python 3.4 and I didn't get any warning when I ran your code exactly as is . 
This does it in a one liner but is not so readable , basically it tests where the value counts for each column is equal to 1 , filters the resultant list out and uses the index as a boolean indec : #CODE 
I still think pandas is not correctly handling your empty column and you end up with either with 5 columns , or with 6 columns , but shifted one to the left . 
However , I am receiving this warning ` / usr / local / lib / python2.7 / dist-packages / pandas / core / index . 
It looks like this changed at some point ; maybe he has an old version of pandas where S and Sec are no good . 
I have also heard of Orange library for imputation , but haven't had a chance to use it yet . 
I managed to find how this is almost done : #CODE 
For this purpose you need pandas - most popular python package for working with timeseries and another analytic data . 
well , what I am trying to do is to have all my results ready out of mysql , and then do different types of merging to get my plots . 
I agree with Chang that it would help to have a very clear example of how the exact alignment should be . 
You mention in update 2 above that you want to get the columns and the only way is opening the hdf . 
So where those indices don't match up ( 50 , and 51 ) , you get ` NaN ` as I would hope . 
However , if I save it as a csv file and reload it again , I got error message and the plot is not quite right either , #CODE 
My main goal is to match the index value from ` ds2 ` into ` ds1 ` and replace it with corresponding value , so the output would look like #CODE 
That would be a possibility but the problem is that each frame5 has a different index . 
First , you need some kind of mapping of what makes up each level . 
PyTables 3.1 was just released that changes the file caching mechanism at least on a lower HDF5 version , do to see your version : #CODE 
backfilling data from one column into another 
Also , I know I am missing patterns that may be useful because if a pattern exists between Variable_1 and Variable_2 and Variable_3 and Variable_4 are missing completely at random , then concatenating them as strings will not capture the pattern between Variable_1 and Variable_2 . 
Do you know what is the difference in this case between both ? 
The goal is to take the 2x2 piece of df with index ( 4 , 5 ) and columns ( ' date ' , ' val ') and replace it with a same-shaped , same-typed 2x2 block . 
Convert Matrix format to Column in Pandas 
` nan ` is commonly used for this purpose , but here I'm actually just using the time that was already there if there isn't a new one defined for it in the ` time_map ` ` dict ` . 
I want to drop all values after index ` 5 ` because it has no values , but not index ` 2 ` , ` 3 ` . 
You should be able to uninstall Anaconda ( it is only a directory ) to reverse any changes . 
Since it is a very large data frame , I think it might be inefficient to do a loop and row by row extraction . 
Since the NumPy array has no index , there should be no " Unalignable boolean Series " problem . 
My guess is that I am either not applying the functions correctly for a column or the values I am getting arent integers . 
which gives you your date as a list arranged in the order of importance ... 
I'm not sure what the difference was . 
The mongodb collection contains sensor values tagged with date and time . 
This should not make any difference . 
I would like to split that file into files of len ( index ) = 2 , using linux : #CODE 
You can coerce the response into a data frame after you get it . 
Oh wait , your matrix must already be in the form of differences from the mean ( by column ) ? 
Any suggestions ? 
I get the following error : #CODE 
I am curious why doing ` unique_df [ i ] = " AAA "` no longer modifies the data frame values . 
Unfortunately I get an error , and the shading doesn't work . 
The range of the values in ` megaball ` are from 1 to 25 , and this line of code : #CODE 
yes it works fine but I need to drop ' 2014-07-16 14:24 ' thnx 
If you have more pressing things to do , you could temporarily rename it out of the way to get through your installs , then rename it back . 
As you can see , the lines overlap perfectly for the days where there is data : no original data is ' changed ' . 
I have a massive file with per timestamp survey data from about thousands of different people and over 20 different locations . 
So if you have a million items that , on average , belong to three categories each , then you need storage for the million items plus three million references . 
Given how the sample was built , there was a need to weight adjust the respondent data so that not every one is deemed as " equal " when performing the analysis . 
What I'm hoping to achieve is knowing where the first / last row of trimmed data is located so I can set up a for-loop to go through the data and perform mathematical calculations with the values and then send those results back into new columns attached directly to the same date as the date in question . 
Ultimately I want to be able to loop through the json to display the dates and corresponding values , but I cant do that until this error is no longer happening . 
They are however extremely useful once you get to grips with them . 
I know the values within the CSV are not all " NaN " so why does the output looks like this and how can I get the correct output with the numbers in reach of the rows ? 
I've filtered my data as suggested here : With Pandas in Python , select the highest value row for each group #CODE 
I was able to get the more precise value in my previous environment by doing the incremental update to cumulative mean instead of taking a batch sum and divide . 
Then use the ` ~ ` to flip the bools . 
The above entire expression is therefore evaluating to an array of truth values , rather than a single ` True ` / ` False ` . 
For the multiprocessing : You can distribute the data sets across cores , do ` partial_fit ` , get the weight vectors , average them , distribute them to the estimators , do partial fit again . 
the size of Y is 100e6 x 1 
Put I think that following this route would lead to an inefficient solution . 
Your example come at a good time for me , so I now have something concrete to train with . 
One thing I find very confortable with Numpy is the vectorization of operations with arrays ( ie . the absence of any explicit looping ) , and the implicit element-by-element behavior of operations . 
I suggest to set it to some reasonable upper limit , though . 
Gonna try to find another solution . 
Is it possible to construct a ` numpy ` matrix from a function ? 
I have 2D numpy array , with example shape : #CODE 
` grid [ 0 ]` can be used as a proxy for the index ` i ` , and 
Thank you Martijn :) - your are BIG help , and just one thing confuses me , how do I tell python to read all CDR records if record is 907 bytes long . 
I want to get the elements of a ` numpy ` array using an index array like so #CODE 
I can weight them how I want to as long as sum of their weights adds to 1 . 
I wanted to try to duplicate those performance gains when solving the distance between two equal sized arrays . 
Even if it worked , I would not expect any speed-up from this compared to an ordinary loop , since it needs to call a Python function for every entry . 
@USER , you're right , if you have to convert everything to ndarrays it's often not worth it . 
An example implementation without recalculating the distance array would be this : #CODE 
I need to return all of the points within a distance of X units from every point . 
EDIT : Actually renaming my package does not fix it . 
2 ) look at the lengths distance ( point , centre , metric= ... ) of all the rays . 
Sorry , all are positive values greater than 0 . 
After that I convert the image to BGR model : #CODE 
How do I standardize a matrix ? 
Speed can probably be increased by ensuring that the record array you pass to Cython is contiguous . 
fid is the file currently being looked at 
I'm guessing it's opening TWO filehandles per iteration , just based on the 498 ( a bit less than half 1024 , and Python would have some files open itself ( maybe 25-odd ? ) . 
The idea is to count the number of occurrences of each transition , and use the counts in a vectorized update of the matrix . 
I kept them in to distinguish them from the ` math ` ones , which won't work for this approach . 
Powers of two are simple to compute , but mixed radix sizes can be faster and use less memory . 
The stars / dots are the ` X ` and ` Y ` plotted with two modifications , I removed the first position and added a false one to make this a full example of the sought algorithm . 
Please look at my EDIT 2 , where I described my problem with input data ... and why I can't get matrix .. 
pyqt : Convert numpy array to QImage 
To find the difference between your data and a point , you'd just do ` data - point ` . 
Unfortunately when numpy reads the 19-digit number as a floating point number , there is not enough precision to get all the significant digits , so there is a rounding error . 
The exceptions are very rare , if any . 
I can't reproduce your problem on Linux using the same versions of numpy and python and a quickly made test file ( with dos line endings , even ) ... 
I imagine I would have to use the uncompiled source provided from each of these three projects . 
However , I am checking optimization routine result , and sometimes power is negative , sometimes it is positive . 
What about array of arrays that contains some structures ? 
The y data takes the shape of the triangle wave below . 
There are some algorithm to calculate faster the results for low valued matrix , but just google for this . 
Those are not random replacements by any means . 
I would suggest to make the library use an ( NumPy- ) array you allocate in Python and pass on to the library . 
For the simple case of " remove column 3 " , ` delete ` makes more sense ; for a more complicated case , ` take ` probably makes more sense . 
I have an array of points in numpy : #CODE 
I have done 7 of the problems on Project Euler ( nothing to brag about , but it might give you a better idea of where I stand in skills ) . 
How do I find the length ( or dimensions , size ) of a numpy matrix in python ? 
It's longer than the other answer but is more generic ( can be used with values that are not strings ) . 
I coded my own routine with Python / Numpy , and it is giving me a little bit different results from the MATLAB code somebody else did , and I am having hard time finding out where it is coming from because of different random draws . 
How can 1,000,000 4-byte ints be compressed any smaller ? 
If this number is less than a third of the total I'll use my answer above . 
I have done 7 of the problems on Project Euler ( nothing to brag about , but it might give you a better idea of where I stand in skills ) . 
Apologies if this is a wrongly framed question or if this question was already asked earlier ( I couldn't find it ) 
If you can choose , I strongly recommend pandas : it has " column indexing " built-in plus a lot of other features . 
But this will iterate through the entire array and allocate a new array in memory containing the all the results , and only then check to see if it is empty . 
Since some askers and some answers both avoid that constraint , I encourage anyone who's here and doesn't mind having PIL to look below , and any non-PIL answers ( new or old ) to mention that they're a PIL-is-used type of answer , to distinguish themselves from answers meeting the original constraint . 
As I understand your question , you have a 2D array of " z " values that ranges from some xmin to xmax , and ymin to ymax in each direction . 
The covariance matrix of a dataset A is : 1 /( N-1 ) * AA^T 
I have a large ( 500k by 500k ) , sparse matrix . 
I can't comment on a numpy array as I haven't used one before , but for using a list of lists Python already has built in support . 
If ` finer_fxy ` is stored in the probably-default ` float64 ` s , this would take about 64 GiB of memory ; not surprising that you're running out . 
Sebastian's solution for a way around the integer-values-only restriction and big-values problem . 
This allows the column to hold float values at first , and strings later . 
Efficient slicing of matrices using matrix multiplication , with Python , NumPy , SciPy 
Is there a more compact way to operate on array elements , without having to use the standard for loop .? 
Please look at this answer : #URL 
When I tried this , I got sort of similar shaped " tiles " of different colors rather than 3 Gaussian humps . 
I created the first array like this : 
( Note that I can't imagine any reason why this should be necessary . ) 
SOLUTION : i have some scattered points ( i don't know how many ) and i want to reduce it to a 8 meaning point . one of the technique i can use is to clusterize them with some cluster algorithms . 
that blas is reference blas from netlib - the slowest blas around . install atlas or mkl instead . 
EDIT : Answer updated for a 2D array . 
But you lose a lot of NumPy power that way . 
Because I view doesn't really have to do with filtering , but rather with different representation of the same data . 
@USER , not sure what you mean by " changing original values " . 
this could also be achieved elegantly with numpy's ` where ` function 
I need to specify datatypes for all numerical types since I care about int 8/ 16 / 32 , etc , but I would like to benefit from the auto string length detection that works if I don't specify datatypes . 
In the following trivial function , I have declared the numpy array argument ` arr ` using the buffer syntax . 
I remember that there was a smart trick about turning on and off the right intersections of rows and columns to turn off one-by-one all the lightbulbs , but it wont come back to my mind ... 
However this code is to slow in the current version , and I am wondering wheater there is a faster solution . thanks ! 
This would probably be the most efficient way to access a numpy array stored on disk . 
hmmmmm , probably it will help some others to sort dictionarys or to prevent from using commands like sorted = sorted ( ... ) . 
The ticket simply spoke of random number seeding with 64-bit , perhaps its referring to a different random number generator . 
Not really elegant at all but you can get close to what you want using a tuple to store pointers to the arrays . 
For example I am looking for 4.2 but I know in the array there is no 4.2 but I want to return the index of the value 4.1 instead of 4.4 . 
Print ' Length of together ' goes just before the matrix line . 
solve a nonlinear equation at several intermediate points of a calculation , not just as the final result . 
Find where they're located at ( assumes the data is sorted !! ): #CODE 
You need Python to keep track of your vector so that it can be deleted * after * the numpy array . 
I find that I have to first build a list and then cast it ( using " array ") to an array . 
I have an numpy one dimensional array c that is supposed to be filled with the contents of 
but the issue now , when I am trying to save the name of the file as well in the csv file like this : #CODE 
After you do this no matter where the template object is in a calculation . 
So the easiest thing to do would be to take a sample of say , 1000 points , from your data : #CODE 
Your array consists of : #CODE 
The final DF should have as many columns as all the df columns added together , so it grow additively and not be combinatorial . 
I'm sorting the cells of the matrix by the float value , producing a list of ` ( row , col , value )` tuples . 
Is it essential that you need a numpy array ? 
Mh . but look at this : 
All variables are dependent on each other and I am only looking for local minima from the initial guess . 
The basic idea is to simply run all the usual steps of a root finder in parallel on a vector of variables , using a function that can be evaluated on a vector of variables and equivalent vector ( s ) of parameters that define the individual component functions . 
Hence , with NetworkX , you can put in an adjacency matrix and find out which authors are clustered together . 
The issue your having more likely is a python mmap issue , since python mmaps handle all the memory mapping and file closing for numpy memmaps . 
So far , I'm sticking with C++ - on my tests , at least 2 orders of magnitude faster ! 
Sorting ends up being the slowest step but it's still faster if m is large because the n*log ( n ) sort is faster than ( n*m ) . 
Basically , I am getting a memory error in python when trying to perform an algebraic operation on a numpy matrix . 
Surely there must be a way to populate a boost :: python :: numeric :: array with data from a simple std :: vector without having to get some 3rd party library . 
Here again a if statement could do , but I am wondering if there is a workarouns and a Python library where negative exposant is allowed . 
The key point here is that Tabular and NumPy set certain standards for what counts as " fast " or " slow " -- and then , force you to be explicit about operations that are going to be slow . 
Asume that your numpy module is located at / Users / Me / python / modules directory . 
I am not responsible from any brain damage resulting from attempting to understand this code . 
There a plenty of places where you're inadvertently creating additional temporary arrays , but they're mostly irrelevant , as they're overwhelmed by what goes on during the call to ` select ` . 
The fact that you are using ` object ` arrays ( not very common and not very memory-efficient ) presents a particular problem when trying to determine the index of non-None array items . 
where things improve as the number of bits increases . 
Really , 4D arrays are just 1D arrays in memory anyway ( Unless you really have view objects , but it should still work with those as well ) 
I'll add comments to explain things in a bit . 
I was assuming that the rgb and ycc matrices were just a matrix that had as many rows as pixels and a column per colour component . 
For example , suppose ` a = ones (( 3 , 3 ))` . 
Therefore , n and m correspond to indices in the array , but I'm not sure how ? 
Update : As mentioned in my comment below , I should have stated that I'm trying to do this on 2D arrays , and therefore get a set of 2D indices back . 
Need to add a check for that , but otherwise thanks ! 
I think you just want ` label == num ` where ` num ` is the number of the object in ` label ` ( the labeled array ) . 
My question is how can I go thru the array to access the object in the array ? 
The matrix in the example above is singular ( determinant ~ 0 ) . 
See the note at #URL 
would turn into either this array : #CODE 
Note that this is a bit more sophisticated than the simple do-it-yourself convolve-method , since it tries to handle the problems at the beginning and the end of the data by reflecting it ( which may or may not work in your case ... ) . 
Usually , in numpy , you keep the string data in a separate array . 
Any idea what might be happening ? 
but the size is wrong because i've assigned 1000 as the period size . 
This may not be perfectly pythonic ( perhaps someone can think of a nicer implementation using generators or itertools ? ) but it is hard to imagine any method that relies on searching one point at a time beating this in speed . 
Thanks , your post helped me solve this problem . 
Now imagine that the next time step some values change , so should this picture . 
Since get_probability is a function , so what value is being passed to count parameter here ??? 
taking the sum for each column . 
You should be able to just load the entire thing into memory on a modern machine . 
What I want to do is to calculate the geographic distances between rows ( with the special condition that the first element is always zero , at the starting point ) . 
We can simply use the leastsq function to find the best coefficients . 
If the list of python objects doesn't grow at all from frame to frame , the leak is probably in the C code or the python-to-C link 
Any and all advice is greatly appreciated . 
Numpy : Is there an array size limit ? 
Then do this after each calculation : for i in range ( len ( array )): array [ i ] [ i ]= 0 
I know the random functions and numbers seem odd , but conceptually this still should work , as it worked when both were set to variables individually . 
@USER are your numbers in the range of -128 to 127 before you convert them to 8b it ? 
In the future , how should I go about trying to find routines like this ? 
At 20,000 elements , your method is about 25% faster . 
I'll fix it just for you :P 
Then I convert it to a numpy array : #CODE 
Just throwing in my two cents you could do this pretty simply using list comprehension if it's always a 2d array like that #CODE 
While its expected value here is zero , the particular realizations will fluctuate around that expected value . 
Then if each item is weighted with weight w_i , the " summed histogram " would have weight sum ( i in items ) w_i D_ij . 
This approach will take an overhead because of crating a new array in memory . 
" Eric's suggestion for revising this question is a good start , but I think the question " Given a Cartesian plane , how to discretize it in a matrix form ? 
it is the same as long as you ignore precision issue - which matters quite often when you start taking exponential of numbers . 
Google Protocol Buffers support self-describing too , are pretty fast ( but Python support is poor at present time , slow and buggy ) . 
Not all people can install NumPy ( or even Python :D ) as many Blender users are just artists . 
All possible solutions are mentioned in the comments . 
I've also refined your approach to allow zooming in over a section of the data and to produce better results at the borders . 
I need to specify datatypes for all numerical types since I care about int 8/ 16 / 32 , etc , but I would like to benefit from the auto string length detection that works if I don't specify datatypes . 
I would appreciate any assistance you can offer . 
Let's say for example I have a matrix X which is my input . 
@USER - By the way , indexing returns a view ( essentially a pointer ) into the array . 
Note that ` view ` holds the same data as the original array ! 
EDIT : What sort of sequence is it you're making ? 
The relative error is less than 2 -24 , which is 1 / 2 ULP divided by the smallest the value could be ( the smallest value in the interval for a particular ULP , so the power of two that bounds it ) . 
This is called matrix transposition . 
@USER The solutions there all make use of the fact that only a 3x3 sliding window is needed , but I need something that works for all sizes of templates . 
( 0008 , 103e ) Series Description LO : ' Screen Save ' 
@USER khanSever 20k wouldn't be a problem for modern computers , if you are really thresholded by speed in this kind of computation , I would say that you shouldn't have had an inhomogenous data array to begin with . 
@USER Eweiwi : Did you find my answer anyway useful ? 
You can now compute the function ` f ( x )` at any point ` x ` . 
BTW : this is a neat workaround , but if it were possible to use the ` in ` operator would have preferred , as in my " real case " I have a pool of roughly 10 values , non only ` ( 6 , 8) ` . 
In this example I want to return an array of [ 202 203 206 210 ] 
So f ( x , y ) = 0 
I present below a sample silhouette implementation in both MATLAB and Python / Numpy ( keep in mind that I am more fluent in MATLAB ): 
Python import Column Data from MySQL as Array 
This is just the partial count due to the 34 1-chips . 
I want to know how I should index / access some data programmatically in python . 
There is a short comment at the end of the introduction to SciPy documentation : 
What about the maximum value in the array ? 
If you use a list of ` True / False ` , NumPy will interpret that as a list of ` 1 / 0 ` as integers , that is , indices , meaning that you ' either get the second or first element of your array . 
But it's still an array and there is no difference in asymptotic complexity . 
Here's one way ( same matrix as before ): #CODE 
assume i have 100 points whose coordinates are random , 
If you just want the first one , use next with the list comprehension as a generator expression . 
So I am able to plot what I want onto my matrix 
By X3D , are you referring to the x3d standard for 3d content , as at #URL If so , I would very much like to learn more of what you are doing -- thanks 
Would it be prohibitvely wasteful to save them with a fixed width ? 
BSD-licensed Python source code for surface fits can be found at 
... which returned ` True ` on each value of the array . 
I have two ordered numpy arrays and I want to interleave them so that I take one item from the first array , then another from the second , then back to the first - taking the next item that is larger than the one I just took from the second and so on . 
Did you look at the link in my answer to the SciPy page on Performance Python . 
If you want the column indices instead of the resulting square matrix , just replace ` return B ` with ` return colset ` . 
At the end of it all : #CODE 
Is there no equivalent function that gets the index of the last occurrence ? 
I want to get a cartesian product of a [: : i ] and b [: : j ] from c . 
python / numpy : how to get 2D array column length ? 
Your example works for me if I sample around 2**6 points . 
NumPy's main object is the homogeneous multidimensional array . 
Pythonic way to import data from multiple files into an array 
The only thing I was going to add was this : #URL Indicated that this is not likely to change . 
i have a numpy array like the following #CODE 
I want to write a Boost-Python program to take a symbolic python function from user and evaluate its derivative in my program . 
Is there a way around this ? 
Not sure if I explained this all really well , but just print out a_strided and you'll see what the result is and how easy this makes the operation . 
But when I start calling columns by their field names , screwy things happen . 
all I get is very high or inf numbers . 
If you're iterating through , and applying the function to _each_ item , then , yeah , the numpy functions will be slower . 
Slicing does not copy the array into new memory ( unlike delete ) . 
And here's the filled version : #CODE 
This is a little bit annoying to do , but at least you can remove that annoying ` == ` easily , using sorting ( and thats probably your speed killer ) . 
I still haven't found an entirely satisfactory solution , but nevertheless there is something one can do to obtain the pointer with a lot less overhead in CPython . 
I also tried using NumPy masked arrays , with NaN fill_value , which also did not work . 
cartesian ( split ( a , 3 ))` . 
I did a little further experimenting and found a numpy specific way to solve this : #CODE 
When you need to deal with exponential , you quickly go into under / over flow since the function grows so quickly . 
Long story short , not only does tabular not act like a spreadsheet out of the box , I can't find a way to make it work . 
What do you mean " two significant figures " ? 
We put it in a list and double it . 
For example for value 255 the coordinates of the box around the value 255 will be upper left ( 0 , 0 ) and lower right ( 4 , 6 ) . 
Like in a java program , you can choose to start it up with , say , 5GB of memory . 
However , due to the way the data points lie it does not give me a y-axis interception at 0 . 
I'd like to sort it such that my points are ordered by x-coordinate , and then by y in cases where the x coordinate is the same . 
Of course this will slow the program down , but at least it'll finish . 
Im writing it here because i cant put image in comment . 
In looking at ` fill ` , I saw that ` repeat ` suits my needs even better . 
Note that an array's base will be another array , even if it is a subset : #CODE 
If you have float data , or data spread over a huge range you can convert it to integers by doing : #CODE 
@USER , plaes recommend using a generator ( parenthesis ) instead of a list ( brackets ) in order to save memory and gain speed when managing high amounts of data . 
I want to divide this array into 3 blocks of size 2x4 , and then find the mean of all three blocks ( so that the shape of the mean is 2x4 . 
( Have a look at the comments above the code for that portion . ) 
That is because ` fsolve ` thinks it is looking for an array of length 17 that solves ` p ` . 
When there's a choice between working with NumPy array and numeric lists , the former are typically faster . 
Wait ... why do you need the negative ? 
But if a dense 3d array representation isn't that much bigger , storing it as a chuncked and compressed hdf5 array is probably the way to go . 
Index datetime in numpy array 
Is there an " expandable " matrix data structure available in a well tested module ? 
You can make this one-liner reusable if you are going to repeat it a lot : #CODE 
Here's my array ( rather , a method of generating representative test arrays ): #CODE 
We need more information on your array . 
@USER : If the code all F77 , why is the question tagged Python ? 
It does that without densifying the matrix right ? 
To speed up the program , I want to pass the index through a subroutine , but I cannot pass ` [ index [ 0 ] , : , index [ 1 ] , index [ 2 ]]` through a subroutine because I cannot pass the colon ' : ' . 
Any thoughts on what I'm doing wrong ? 
This will be far , far faster than constantly reallocating the array inside the loop . 
How can I get a new array containing the values of specific attributes of those objects ? 
Seriously , at least leave a note , but given the " complexity " of your actual request I'd say that you'll have better chances with a new question . 
and find the roots with numpy : #CODE 
I need to create a numpy array of N elements , but I want to access the 
I have allocated a chunk of double in a C library and I would like to create a numpy 1D array based on that data ; ideally I would like two versions one which only wraps the c_ptr readonly - letting the C layer retain ownership of the data , and one which copies the data . 
The code included in pypy is a new array class which tries to be compatible with numpy , IOW , it is a reimplementation from scratch , without many features from numpy . 
Like I say , I'm honestly struggling , any help would be much appreciated . 
with array . 
I'm not sure that I understand the difference between copying the matrix ( example 1 ) and copying the data ( example 2 ) . 
Does anybody know of a ( common case ) faster-than-linear way to find the endpoints of a boolean property of an array . 
Any unrecognized type will work this way , so you might want to use ` myclass ` instead of ` object ` . 
Iterate over vectors in a multidimensional numpy array 
This way you can load a large dataset from a textfile memory-efficiently while retaining all the convenient parsing features of the two functions . 
you may win few cycles if you multiply by inverse instead of dividing in floating-point performance . 
Without knowing the size or quantity of the images or the application of the algorithm ( computer vision ? ) , I can't say how big a deal that kind of speedup is . 
Is there an easy way to sort these eigenvalues ( and associated vectors ) in order ? 
You can pass a numpy array or matrix as an argument when initializing a sparse matrix . 
( For most common applications of quadratic forms q A , the matrix A is symmetric , or even symmetric positive definite , so feel free to assume that either one of these is the case , if it matters for your answer . ) 
I think you might find the ` flat ` method useful . 
Now that we have both the starting and ending values , we can use the indices function from this question to get an array of selector indices : #CODE 
10 ( i ? 1 ) K , where K = k / ( n ? 1 ) . 
This identifies which rows have any element which are True #CODE 
Broadcasting is a more general way to fill an array and I would guess is slower or equal to the very narrow use case of ` fill ` . 
By " not replicating data " I am assuming you mean " not allocating more memory " . 
Can you post all / more of the data ? 
The scoring matrix would be trivial , as the " distance " between two numbers is just their difference . 
Contours around scipy labeled regions in a 2D grid 
Why doesn't the shape of my numpy array change ? 
Mind also the indexing starts at ` 0 ` 
I have a vague feeling that I might have seen a question addressing this problem , but I can't find it now . 
If you know which rows are to be deleted , just extract the other rows ( you need ) and create a new array . 
If there is any other way I guess I have to do that . 
I have a matrix , say #CODE 
You might find out the distribution information using ` cat / etc / *-release ` ;) 
I was wondering if anyone found a good workaround , as my real-world problem of iterating over the Cartesian-product of the rows in very large arrays is so slow it's impeding progress . 
I have an array of x , y , z distances and I need to find the differences between each vector from one another . 
The code above finds parts where there are at least MIN_SILENCE consecutive elements smaller than SILENCE_THRESHOLD . 
The list of indices will always be ascending , never have duplicates , but may have gaps like the example . 
Any ideas ? 
sum function in python 
All globals hold either values referenced by those tuples or are lists of tuples . 
You can pass a list or an array as indexes to any np array . 
The array I'm using is quite large ( 3500x3500 ) , so I'm wondering where the best place to load it is for repeated use . 
Basically , it comes down to checking before you add . 
I have a simple function called get_gradient which takes a numpy array of [[ x , y , Vx , Vy ]] and returns ( should return ) an array of [[ Vx , Vy , Ax , Ay ]] . 
I found this post : Python : finding an element in an array 
So , are VBOs simply not meant to be that big ( I somehow doubt that VBOs could only have around 17k triangles each ) ? 
` flags ` parameter leads to ` TypeError ` if input array is not contiguous . 
I then have a 2nd array similar to #CODE 
Convert a list of 2D numpy arrays to one 3D numpy array ? 
I'm currently a grad student at Harvard and a good friend of mine went there ( he would have graduated two or three years ago , as he is currently a second-year grad student here at Harvard with me ) . 
I'm not clear on how you are wanting to plot it , but it sound like you'll need to select some values of a column . 
The issue I am running in to is that the array can be larger than 3gb in size ( these are huge images ) and I need to segment them prior to ingesting them . 
The latter might be faster because it doesn't produce the intermediate ` x**2 ` array . 
Any suggestions ? 
" A copy of arr with the elements specified by obj removed . 
is not it another copy ? 
NOTE : the row has " : " , but the " : " does mean the dict ' : ' . 
If , for some reason , I would only save one dictionary then every script loading this file with pickle would mess up the order of the stored variables . 
You might also want to take a look at Anvil , announcement here . 
The other way that I know is to convert Y to list iteratively . 
This is especially helpful since it includes the import commands and info on how to write to file . 
But actually I am not so sure that from where you are now , using sparse matrices will gain you any speed-up . 
Upon deeper examination of the relationship between the python printout and the structure of my underlying data , I see that the python print command is saying that there are two empty columns at the end of the array . 
How to convert a simple list of lists into a numppy array ? 
Django has a library for encapsulating all the database work into Python classes , so you don't have to mess with raw SQL until you have to do something really clever . 
So I got numpy , scipy , IPython , and matplotlib working ( I can import all four with " import _ )" . 
@USER ` new type not compatible with array . 
Is there any way to do this in Python ? 
Then you can choose many methods to visualize it . 
Numpy Array to base64 and back to Numpy Array - Python 
In each iteration of Gibbs sampling , we remove one ( current ) word , sample a new topic for that word according to a posterior conditional probability distribution inferred from the LDA model , and update word-topic counts , as follows : #CODE 
I am getting weird errors when I try to convert a black and white PIL image to a numpy array . 
Numpy arrays have a ` copy ` method which you can use for just this purpose . 
Actually I could not test with big K , d and N as I was going out of memory . 
With all of these options you have to pay a JNA tax ... all of your data has to be copied before it can be processed . 
Useless because it ignores the " cross platform issues , proprietary tool chains , certification gates , licensed technologies , and stringent performance requirements on top of the issues with legacy codebases and workforce availability " ( John Carmack ) that op is probably facing . 
And that the values of all ( x , y ) pairs are given . 
Is is possible to have a 3-D record array in numpy ? 
However , the evidence suggests that you've encountered an issue of this sort . 
There's _way_ less overhead this way . 
I'm having trouble figuring out what kind of test I need here , and the best numpy / scipy / R function to use for these kinds of issues . 
I have see people using dictionaries , but the arrays are large and filled with both positive and negative floats . 
How can I speed up iteration through this transformed numpy array ? 
This is may not be the best way to solve this but have a look at the following ... 
All in all , I would go with the #CODE 
This is not a matter of style . without the list ( _ ) it does not even work at last for the case i have that y is an array itself 
( at least it gives me an error stating that the ' as ' is reserved in python 2.6 ) Am I correct ? 
Did you try looking at numpy for matlab users manuals , like : #URL 
I would not try to process ` arr ` in place - it seems that a new array is created under the hood in most cases anyway . 
Now you must initialize each element of the numpy array to be an 1-d numpy array : #CODE 
The easiest way around this is to just use a numpy array , instead of a numpy matrix : #CODE 
I am trying to create an affinity matrix for an image . 
to handle the error cases and the return value , they are not related to the array assignment . 
Saving a Numpy array as an image ( instructions ) 
Using this , I know I am calculating r-squared correctly for linear best-fit ( degree equals 1 ) . 
No expert on the topic , but this is some kind of adjency matrix ( #URL ) . 
about 15 times faster using broadcast 
Arrays to Matrix numpy 
but it appears to only take square matrices . 
Any idea how that can be done ? 
In your code , ` a [ condition ] [ index ]` returns the value in a , but I want the INDEX in a , so that ` a [ INDEX ] = a [ condition ] [ index ]` . 
Any database that can create an index will provide relatively fast look-ups ( depending on how many millions of records you're storing ) . 
Actually , the best way to manage packages on OS X is [ Homebrew ] ( #URL ) ( not Fink or MacPorts :)) - which unfortunately lists neither NumPy now SciPy at the current time . 
I would like to keep ` xcoords ` a numpy array if possible . what do you mean ' adding them to the object before it is returned ' ? 
But I just need to sort out which points to send for a complete graph . 
how do I calculate that an array of python numpy or me of all the calculate decimals and not skip like . 
It will support it on the next release . 
Python lists are defined with square brackets , and we want to generate a list of lists ( where each piece contains one of your defined segments ) . 
The biggest gotcha for me was that almost every standard operator is overloaded to distribute across the array . 
I want to combine the two into a mutli-dimensional numpy array . 
where ` nlooks ` and ` dfactor ` are scalars and ` Ic ` is the unfiltered array . 
In a 10x5x5 matrix with ` x [ 0 , : , :] = 0 ` I would expect a result of : #CODE 
For example : I have a = array ([ 123 , 412 , 444 ]) 
While it often results in a massive speedup to eliminate for loops and take advantage of numpy built-ins / vectorization . 
If I understand correctly you have a three dimensional array , something like : #CODE 
@USER : where is a new array created ? 
array ([ 41 , 32 , 41 , 33 , 42 , 32 , 42 , 33 ]) 
Any idea when it will be ready ? 
I see you've taken care of my edge issues , although your filter size is hardcoded ;) . 
If you open idle and type ` import matplotlib ` it shouldn't return an error 
No expert on the topic , but this is some kind of adjency matrix ( #URL ) . 
Edit : If it's a floating point issue , what sort of floating point error mistakes a number much less than 1 as one around 8 ? 
The question was about how to slice if the rank is not known at the time I write the code . 
I think a typical method is to always double the size , when you really don't know how large things will be . 
This script is mainly intended to demonstrate building an independent python in your home directory , and assumes the system you're building on has the proper dependencies already installed , but it at least points you in the right direction . 
and use the information on the size inclued in the filename to restore the initial shape 
Hmm I added for first example , did you know how to copy from IDE exactly with commas and everything ..? 
@USER : Your answer will give false positives in the event that one or more ( but not all ) of the elements in B matches with one of the rows in A . 
I would like to average the 2 different arrays contained within ` record ` . 
I need to constrained minimization of some data ( ie so that I get the minimum value within a certain range ) . 
In this case , I would like to return the index 2 ( 2nd row ) . 
a 32 bits process can only access around 4 GB of memory . 
How do I find out , if the numpy BLAS libraries are availalbe as dynamically-loadable ? 
( they are at same scale ) 
Now simply create a new array and multiply : #CODE 
Take a look at this Project Euler problem : #URL 
Python : how to store a numpy multidimensional array in PyTables ? 
How can i load all 24 joblib files in one program without any errors ? 
Where I'm stuck is what the wrapper code should then look like to pass a MxN numpy array to the ** coords1 and ** coords2 arguments . 
I have created a numpy 2d array of type string called ' minutes_array ' with the first column as unix timestamps rounded to the nearest minute covering every minute from the start of the sensor timeseries to the end with three empty columns to be filled with data from each of the 3 sensors where available . 
Which can be done in O ( n ) , but your answer requires O ( mn ) , where m is size of window . 
Somehow I always thought you can load the shared library compiled with any compiler . 
` array =[ ' NaN ' , ' 20 ' , ' 383.333 ' , ' NaN ' , ' NaN ' , ' NaN ' , ' 5 ' , ' 100 ' , ' 129 ' , ' 122.5 ' , ' NaN ' , ' NaN ']` 
array , and then use ` view ` to turn it into a structured array , and then use 
and so all we need to do is : #CODE 
Any clue to why this is happening ? 
I think the definition used in the field of statistics is the value in the middle of your data array after it has been sorted . 
Dense covariance matrices of that size suggest operations that run forever ! 
In this case , I'd like it to return a density that's essentially peaked completely at a difference of 0 , with no mass everywhere else . 
If the array is doubles ( remember python floats are C doubles by default ) then you have to think a bit harder as == is not really safe or what you want for floating point values . 
They all have their strengths and weaknesses . 
numpy array of chars to string 
matrix rank : #CODE 
This slows down for large sigma , at which point using FFT-based smoothing might be faster . 
What is the fastest way to iterate through all one dimensional sub-arrays of an n dimensional array in python . 
This works , but it's really slow . 
If I create a simple array like this in Python I'm able to read the values in the C code : 
In an ideal world , the function or class would support overlap between the divisions in the input matrix too . 
My problem is different because I need to find ** all ** the roots of my function , on a given interval . 
How can I create a PyArrayObject from this structure , specially how I can create a numpy array that hold 3 object ( off course 3 is an example here ) ( each of them is an array ) 
x : a numpy 2d array 
Thanks for the info . 
How would you avoid the loop in the case that all entries in ` repl ` are the same ? 
Pulling data from a numpy array 
There's no effective difference ( they both return views into the original array ) . 
Thanks for all the tips ! 
remove zero lines 2-D numpy array 
Instead of using ` PyInt_AsLong ` , use the ` PyArray_* ` functions provided by Numpy's C API to access the data ; in particular , see section Array API . 
Well , I tried dividing by the largest place value . 
All of those numpys are linked to the system Accelerate framework : #CODE 
and I wish to create a third array with each element from ` b ` appearing ` a ` times in the new array , as : #CODE 
I can imagine a number of approaches to storing both of these data formats , ranging from storing the metadata with the ` AttributeSet ` class for each ` Array ` / ` CArray ` to using a ` Table ` for all of the metadata . 
I want to calculate the average of four neighbors in a huge array . 
Suppress Scientific Notation in Numpy When Creating Array From Nested List 
I want to find the vector x ' such that Ax ' is as close as possible to 
And the dataset in question is beyond doubt particular : There certainly is an upper bound and a precision . 
Only integers can be used as array or matrix indices . 
I can't find it online anywhere . 
I will try your code , but I am also going to try writing a simple C extension to simply do the reading , math , and drawing all in one place . 
Are there any good greedy implementations to solve this or am I on my own to implement this ? 
The problem is that for the array input , SWIG complains that there is no typemap . 
Is ` column_array_to_add ` another 2D array , or is it a 1D column array , as the name implies ? 
the sum of a triple-product ( element-wise ) . 
I ran a simple speed test comparing numpy and python list comprehension , and apparently list comprehension was faster . 
That is why your sample loop has been collapsed to read in the full sample for the receiver and channel in one large read . 
Something like the following iterator should get around both of these problems : #CODE 
I appreciate any input on this ... 
Do you really need to find such a weird thing ? 
Any particular reason you don't want to use a straightforward approach ? 
The advantage of numpy is the support of slicing at different levels . 
An implementation , however , is not really open to interpretation . 
Python numpy masked array initialization 
You can further optimize by exploiting array-order alignment to reduce excess memory consumption caused by copying the original arrays . 
For example , any vector ( of the appropriate dimension ) can be an eigenvector of the identity matrix . 
The normal 64-bit double-precision floating point has least positive normal value 2.2E-308 ; storing logs gives you an effective least positive normal 1E- ( 1.7E308 ) . 
index set for each position in the index arrays . 
I am wondering if reassigning temp [ ] to a 1-element shorter vector each time is slow , would it be faster to pre-allocate a 96-3 length list of vectors of length 96 , 95 , 94 ... to 3 ? 
What would we do , if we wanted to change values at indexes which are multiple of given n , like a [ 2 ] , a [ 4 ] , a [ 6 ] , a [8 ] ..... for n=2 ? 
Thanks for all the python guidance ! 
I'm not really pro Matlab , but surely Stata can't be so bad as to require ` adoedit ` just to know what algorithm it is using ? 
This can be found relatively easily by just looking at points where the potential exceeds a certain threshold . 
Negative indices are interpreted as counting from the end of the array 
I was using unsigned int indices to speed up access according to : #URL 
I've tried to vectorise it using numpy but I'm not really sure how to do it given that the matrix / 2D array gets changed on each iteration . 
Numpy slicing x , y , z array for variable z 
I would like to convert ( a more complicated form of ) the follwing Matlab code #CODE 
I have a NumPy array ' boolarr ' of boolean type . 
If you have only integers that are between 0 and n ( if not its no problem to generalize to any integer range unless its very sparse ) , the most efficient way is the use of take / fancy indexing : #CODE 
Instead of 2D coordinates , I use index for every elements in the matrix . 
I already tried converting the cols to int but that didn't solve it . 
Although I'm sure there are methods for applying RK to an equation such as this , I didn't find any evidence of them in _Numerical Recipes_ , which I think qualifies that topic as relatively obscure ;-) 
When facing a big computation , it will run tests using several implementations to find out which is the fastest one on our computer at this moment . 
Use an array of floating point numbers instead . 
` numpy ` slicing operations probably involve ` for ` loops at some level , but they're implemented in c , and provide a linear time solution for this . 
I have one question : Is there only one way to do addition of two matrix ? 
@USER It is now supported , at least in my version ( 1.7.1 ) . 
I know I could start a number of times at random locations but I'm not able to do that with what I am currently working on and have to use on of these minimisers out of the box . 
For small displacements of around 4-5 pixels , the direction of vector calculated seems to be fine , but the magnitude of the vector is too small ( that's why I had to multiply u , v by 3 before plotting them ) . 
However , I will need to access all waveforms at some point . 
I've find this : #URL but when I try to install this I get an error : #CODE 
yes , I can assume either that I have g explicitly or that I can sample x according to g . 
` example ` is a structured array consisting of two elements ( ` ( 1 , 2 , 3 )` and ` ( 4 , 5 , 6 )`) , each element ( or ' record ') having 3 fields . 
If i have two variables - where they either are a 1d array of values length n , or are a single value , how do i loop through them so that I get n values returned . 
For each point in array A , I need to find how many points in array B are within a certain distance of it . 
It does , but somehow it is 8 times slower than copying to numpy array :( I suppose the regular python overhead slows things down much more than a copy ... 
It all depends on its dependencies . 
Is there a way to make an array of such strings ? 
` grid [ 1 ]` can be used as a proxy for the index ` j ` . 
After doing so , I discovered that if I tried to open the IPython HTML Notebook I got the error message : #CODE 
( the new matrix would have n-2 rows m-2 columns ) . 
and duplicate index values at the correpsonding sites within 
I found that the best way to produce small pdf files is to save as eps in matplotlib and then use epstopdf . 
You could rearrange the image to put the ( 0 , 0 ) in the middle with some matrix manipulation . 
Please , see the next example : 
A function that broadcasts a scalar operation over an array is called a universal function , or ufunc . 
may not exist until the datasets get quite big ( maybe you'll need at least 10,000 rows per data set ) . 
Magic answers like this are not really helpful because they don't solve the problem . 
I think what I was missing is that I really have a 3 dimensional array , 48x365x3 . 
I load a some machine learning data from a csv file . 
So I have it running ( or at least that assignment isn't throwing an error and it's compiling ) ! 
@USER The first function is taking chunks of 200 items from your huge array , and copying those chunks to a new , even more ginormous array . 
@USER : With ` where ` it looks definitely nice , but have you consider also the implications to performance when implementing with ` where ` ? 
Anyone any idea what this means ?! 
Assuming you are using g++ to compile ... have you had different results in any way when experimenting with compiler optimization flags ? 
With the overhead of the data structure you could be looking at usage much higher than that -- I can't say how much because I don't know the memory model behind SciPy / numpy . 
I have serious doubt that adding two numpy arrays is a bottleneck that you can solve rewriting things in C . 
Where exactly is the error occurring ? 
I frequently convert 16-bit grayscale image data to 8-b it image data for display . 
Reduce it to a 1 / 10 resolution , find the one white pixel , and then you have a precise idea of where to search for the centroid . 
I ran a test to compare the times , and found that my method is faster by quite a bit , but Freddie Witherdon ' s suggestion is even faster . 
I couldn't find it in the OLS recipe ( #URL ) . 
convert binary string to numpy array 
How to know where warning come from in Python 
How do I add a title to my MatPlotLib basemap ? 
It might help issue of things refusing to update , and at the least it will be faster . 
For future use you should really find out why it is installed in a location that isn't searched by python by default . 
I've done that and I'm still getting the same error - it can't find the sip module . 
ImportError : DLL load failed : The specified module could not be found . 
This has happened previously ( and I was not able to get around it ) . 
@USER SObolev : Histograms of the values falling within a range of keys where each bin is of size 0.1 
Also , now that I look at it , the y-axis limits haven't been set right . 
I've looked for various solutions to my problem but can't get any help . 
I normally use something akin to the following for this purpose and the result is a very nice plot where x1 , x2 and x3 are defined as follows 
And I have all of them installed . 
When I save these plots to pdf , it takes a long time to write , and reading the pdf is even worse . 
Seems like a security setting on OS X is preventing it , and the quick fix seems to be to run apache as root . 
Any ideas how to deal with this problem ? 
If I remove the function and put its body back under the " if name == ' main '" block , it works as expected ( like in the original code ) . 
Seems like a bit of searching would have turned up [ how to set custom ticks ] ( #URL ) , where the defined ` t11 ` is where you would write out A , B , C , and D . 
@USER , that would require to have root access ( sudo ) which i dont have ! 
Any suggestions how this can work ? 
Any help would be appreciated in understanding what I am missing . 
How can I add the ` P_g ` values from each section to their respective column ? 
Is there any other python package that would help ? 
I'm having a problem , where I can't do a step graph of 2 lists I have , in which I need list ` x ` to be the x values , in which each ` x [ j ]` value will add with ` x [ j+1 ]` value for each step . 
The best thing would we if one could integrate it into the label of the colorbar ( to say the color means A in units of B where B includes the order of magnitude ) . 
ive tried to fix it with setting ax limits but that does not work . 
@USER : I added some more info and also a code used . 
If I wanted to make a combined image like the one shown below ( original source here ) , 
When you ** add ** two ** random variables** , the resulting density is the ** convolution ** of their ** densities** . 
This made the program work just as in Windows , without any other modifications necessary . 
Using PySide in place of PyQt4 results in the same behavior . 
However , the first item appears to be a float #CODE 
The code below loads the image data hosted on github and plots it : #CODE 
I reduce the values to months using : #CODE 
That would solve Question 1 . 
I modified you code , and it can product the same ticks now . 
`' $\xi$ ( r ) M$_{200}$ 13.4 '` : several ` $ ` , where you should have only one . 
I just completed a brute force test running through all permutations of [ 0 , 1 , 2 , 3 , 4 , 5 ] for the tmp_planes , a total of 720 different arrangements . 
The way to fix it is using pip , as mentioned by @USER . 
Do you mean a scatterplot of your dataset instead ? 
Plotting subplots with secondary continuous y-axis across all subplots 
However , I want to be able to call something or write some code which then skips any value in the array which would cause this error , then ignoring that row altogether . 
Any help would be appreciated . 
You are typing your code in at the bash ( terminal ) prompt , not the Python interpreter prompt . 
Also , if you take a look at Rob's answer , its far simpler than the example shown on the website . 
MPL is a big project with lots of moving parts , any help keep them up is appreciated . 
I am less certain if I am solving the correct problem . 
I am having a problem getting matplotlib to work well with interactive plotting ... what I see is that after displaying a few frames of my simulated data matplotlib hangs-and doesn't display any more . 
If anyone has any idea , I appreciate the help . 
You can populate a general 2d array by : Zhu shows in the answer below . 
Have a look at this example : #CODE 
That way , it is possible to see 2 dots on the same place . 
This error shows up when the size of a and b ( taking from above example ) is not the same - so , 128 x-values here should be plotted against 128 y-values . 
[ These should all be installed into / usr / local / Cellar , which is the default install location for homebrew . ] 
I'm trying to make an exponential fit to find the Lyapunov Exponent of this data , however , I keep getting this error : #CODE 
So I wrote an argument ` origin= ' lower '` . 
Note that I don't reduce to 2 , I reduce to 10000 , and then extract the first 2 . 
which you can run with this sample code : #CODE 
Try converting your the pandas data series to lists or numpy array before plotting . 
If I de-homogenize only the last column I can find the center of where the ellipse was projected , but I would like to see some shape information as well . 
Ignoring a problem doesn't fix it . 
Will add the code . 
I found that in the folder C :\ Program Files\Python27\DLLs I cann't find the _socket file 
The index is zero-based . 
The image ggd4 for the test can be downloaded from : 
Load the data sets individually , and then plot each one individually . 
Have you looked at mplot3d on matplotlib ? 
The columns are floats , I'm not using the index of integers . 
Incidentally ( for someone reading this in the future as you know yours ) if you need to find your current Python version you can simply type ` python -V ` in the command line and it'll return the details , for example mine returns : 
The code is located at 
find the file from the link 
@USER : What do you mean ? 
Thank you in advance for any help you can give me 
how i can add vector arrows starting from 0 , 0 and end at the points that i want ? 
If you want the sum ( not the integral ) to be one #CODE 
As I have no idea how long ` data_all ` is , I created a ( random ) array , which in my case turned out to give a 2D array with 512 columns ( lucky shot ) . 
Do you mean something like this : 
Can you copy and paste the code lines described in your comment , please ? 
When I print this in A3 ( good size ) it still almost unreadable because the letters are stacking in each other . 
I mean my grid is 20 units in x direction however , my resulting plot is 10 units in x direction with replica plot from 10 to 20 units . 
Now how to plot [ M , F + other ] 
To specify that you want the first interval filled with " pure " black , set ` vmin=mean ( lvls [: 1 ])` in your example . 
Your Data looks pretty good without any filtering , your " outlieres " all have a very big error and won't affect the fit much . 
This is a terribly unclear answer , at least some comment in the code or explanation of why this helps floating point error would be nice . 
I produce a histogram where I put the weights in the parameter #CODE 
Besides , I see that you use ` numpy ` in your code , so you can save and load file by ` numpy ` with a more compact codes like this without using the ` csv ` module . 
Usually these sorts of simple array reshaping operations can be done in a couple of lines with Numpy , so I feel like I'm missing something . 
I don't see any way I could do that . 
Look at the docs : #CODE 
To fix that we turn the frame off on ` ax1 ` ( so we can see ` ax2 `) . 
At one point I was able to trick it into working from the console by installing some thing in the virtualenv , but other things only in the global namespace , but I forgot how I did it . 
( Technically we could skip the ` FigureCanvas ` , but it will be needed as soon as we want to save the plot to an image , etc . ) #CODE 
But how can I make plt do all this stuff ? 
From your piece of code it seems to me that ` clust_data ` is already a list of lists with the correct shape and that ` cellText ` after being filled is going to be the same of ` clust_data ` . 
Anaconda doesn't by default load pylab into IPython so you can choose the backend after launching IPython . 
Most likely I'll have graphs with longer labels and if I put them near lines , it'll look messy . 
The lines are random within ranges that cause clustering of lines ; a behavior I wanted to verify . 
have those lists all the same length aka would it be directly compatible witha 2d numpy array ? 
Any assistance will be appreciated . 
I have another working project where the toolbar is in the same panel as the canvas , and I think that might be the key difference between the two . 
any of #CODE 
Which way is the easiest to load all txt files ? 
( Note that F is not given in the data , but you gave it as an example ) 
Now I need to put this plot in a PyQt form . 
Where is the URL API documented ? 
Are there any other option ? 
You need some criteria to select out the " best " in your context . 
What do you mean when you say ` the first histogram will be the first column as the x values plotted against the second column as the y-values ` . 
Unfortunately I don't have any experience doing this so won't be able to help . 
A sample of my data to plot : #CODE 
Also , I do not want user intermediation at all ! 
Rather than put together an example from scratch , there's an excellent example of this written by Paul Ivanov in the matplotlib examples ( It's only in the current git tip , as it was only committed a few months ago . It's not on the webpage yet . ) . 
what to do next ? 
I think you'll find you get better help if you describe * what * you're trying to do , rather than * how * you want to do it . 
( ie save etc ) . ill have a look at the toolkits :D 
in place of #CODE 
You could cast the array to a list : #CODE 
Add as many symlinks to the directory as you wish , it gives you fine-grained control over which packages you want to make accessible . 
In the end , I found out that the problem was simply that matplotlib could not find any backend for plotting . 
Please change your title to describe your _actual_ issue . 
If you did not create one when you installed matplotlib , you can get this template from the matplotlib source , or from the matplotlib website . 
I am sorry to be so needy , but I really cant find any useful information anywhere . 
Or is there another way to change the size ? 
I did find the way to make my images almost what I wanted with : #CODE 
any idea ? 
@USER : I suppose the specific way I have it set up would work better if you had , say , a 1000x1000 matrix , as it lets numpy pick how the tics should be spaced . 
As you can see it only covers half of the matrix . 
You iterate the items in ` glass ` , but then compare to the whole list instead of to the current ` item ` 
If that helps , this is the process that I am doing to save and plot : #CODE 
Please consider answering your own question or removing it all together if you feel it does not add . 
Now " print zi " gives me an array like before but with nan everywhere . 
Im not by computer to test . 
I want to put my custom datetime format but when I tried my custom date format is overlapped over the date given by default by pandas plot . 
Change formatting on datetime ticks when plotting daily mean with Pandas / matplotlib 
Any other way to get a similar effect ? 
If you see the right side , the size is changed . 
How can I select multiple data points with matplotlib and export it ? 
Two steps to sketch an idea to solve this : 
This is not necessarily the nicest possible way of doing things , and you may bump into resolution problems , as well ( check the size of the ` mayavi ` window ) . 
Is there a way I can get y=x to show up even if I don't have a list of all the points that I plotted ? 
Knowing all that , your example becomes : #CODE 
Any Ideas what might be the problem ? 
It appear that these disappear when I add the ` facecolors=UWR ( heatmap )` property to the ` surf ( ... )` . 
Can you post the full traceback ? 
The full list of positions is given here : #URL 
unutbu's answer should fix it everywhere . 
how can i insert the plot returned by create_predef_plot into f2 #CODE 
And the formatted ones ( Used as _id of the documents after using aggregation ): #CODE 
Any packages you accidentally installed for other Python installations , just reinstall them for MacPorts . 
However , other programs seem to be ok with filling almost all the memory before the swap is used . 
I'm not sure how to tackle this problem and any help or direction would be very appreciated . 
i looked at sage , the source code is huge , does it support python out of the box ? 
If any one know about it then please reply me . 
I'll have a look at it 
The main issue is that you can't make UI changes from a separate process from the main UI thread ( the one that all of your Qt calls are in ) . 
I say " quick fix " because there are more changes that could be recommended . 
Maybe look at the scikit-learn docs for inspiration ? 
Labels are located at the right place , but label is not correct . 
I ran the following code to get two plots next to each other ( it is a minimal working example that you can copy ): #CODE 
here is the code where I create the plot and a picture of this : 
Admittedly , I'm new to pandas but it seems like at every turn I am stopped , by my ignorance no doubt , but it's getting tiresome . 
Are all of your modes needed to demonstrate the problem ? 
Note that the explicit masking is no longer necessary in matplotlib master as arrays are now masked automatically internally . 
( The disadvantage is that this method is much less flexible . ): #CODE 
Python & Matplotlib : creating a chart first and showing it at a second moment 
Put #CODE 
If you require a completely manual installation , you should give those instructions a thorough read . 
Take a look at this example . 
I tried something like that already , but when I plot each data point individually in a for-loop , I see no way to use a colormap and colorbar for all of them . 
Here's how to do it : if you find that an answer has been helpful to you , then move your mouse over the upper left-hand corner of that answer ( where the answer " score " appears between two gray triangles ) . 
Can you recommend any tutorial for MATLAB speakers ? 
The second plot keeps rescaling the x-axis for me , losing the overview of the full column 1 plot . 
Thanks for the answer , I searched for event handling examples but could not find it ( I am newbie to python ) , can you please send me link to at least one such example . 
You can find it here . 
( I know I add nothing new , but the straightforward answer should be visible ) . 
So , I don't even know where to start looking 
When I try to run the example script at 
Thanks for your comment , I have put everything , I wanna plot ( years , Frequency_accidents_years , regions ) 
Via ` apt-get ` , I've tried installing ffmpeg , every codec imaginable , and even tried to compile ffmpeg from source . 
@USER This isn't a bug , but a design choice . 
The left-hand side can , however , contain any number of elements , and provided it is a tuple of variables the unpacking will take place . 
You want to look at the OO interface , not the state machine interface for matplotlib . 
This would be an interesting thing to add to the emerging thoughts about semantic objects in mpl . 
Maybe you could post a sample of your data array ? 
I believe this is because I'm plotting the image wrong and so it is plotting all the data as 0 . 
Try running with ` python -X faulthandler ` and see if you get a useful traceback at the segfault . 
Notice how only one body has been coloured , all other disjoint polygons remain white : 
Thanks @USER , All I want set a plot title based on the what user is selected on the plot , for example if the plot shows a data from ' Jan 2012 ' to ' July 2014 ' dynamically want to set the title to ' Years ' and if the user selects from ' May 2014 ' to ' July 2014 ' then dynamically want to set the title to ' Months ' and if the user selects from ' July 2014 ' to ' July 2104 ' then dynamically want to set the title to ' Days ' . 
I would like to know if I have a triangular marker , is it possible to control its orientation ? 
I like to sort these points row by row . 
Before comparing between these two PDFs , I would like to compare original time series and the series of Gaussian distributed numbers with the standard deviation and mean from original wind velocity series . 
I will like create a alpha channel , for explanation I put a example : 
The function is supposed to be smooth and connect at 0 and 2 pi in the y range of ( 0 , 2pi ) not touching 0 and 2pi . 
I have run a similar script before , and I think I've imported all the relevant modules . 
The sample image makes things clear . 
Take Care , George 
I don't know of any movie format that is vector based ( but I don't know a lot about video codecs ) . 
I can't find anything like that in the galery and all my attempts do deal with the colorbar failed . 
I encourage you to try all of the great ideas people are telling you , and I'm interested to see if any of them work ! 
Have you tried setting your backend to SVG so that all of your images are in vector format ? 
I appreciate any help :) 
Would be better to change the title 
My csv is like : id , author , title , language 
After making all of the changes listed in steps 2 and 3 ( and correcting the spelling of the title ) , I get the following very-reasonable-looking plot : 
When I look at the plot , the x range ends up being from 0 to 12000 . 
It was still at -1 when I saw it . 
when the bad points are deleted , the user can click on different buttons to classify the source ( lets say " star " , " galaxy " , " whatever ") . 
Your ` Z ` ' array ' is actually a list of numpy arrays of different lengths . 
when the 2D array is viewed as a color 
I looked through the examples in MatPlotLib or R and they all seem to already start with random data to generate the image . 
Filtering spreads out the information and makes the inner contours disappear . 
This is probably related to upgrading the library , and having old versions of the javascript hanging around . 
can you explain your data format in a little more detail and give us some copy / paste-able code to create a toy version of it ? 
' To open " Python , " you need to install X11 ' . 
@USER : You can certainly change the exact shape by modifying the factor " 3 " in front of the lower branch . 
Downvotes are due to the lack of research effort ( if you look at the reasons for downvotes , it says " This question does not show any research effort ") . 
As for the matplotlib version 1.1.0 , it's a really easy install , at least on my by-now-quite old Lucid it's no problem at all . 
Numpy index out of range error 
Though I want to add that manually saving the chart from Qt Widget is OK . 
Without seeing any of the existing plotting functions , how am I supposed to know how similar or different they are ? 
You can use spline to fit the [ blue curve - peak / 2 ] , and then find it's roots : #CODE 
I had to add ` colorbar ( im2 )` right after the definition of im2 ... 
Creating sample data #CODE 
The numbers choose now many decimal places you want and the e chooses scientific notation . 
To clear up the code , I put the individual plotting functions for the six or so groups of subplots into separate submodules and call them with the arrays they need for plotting purposes . 
You have given us a list of requirements , but what have you tried to solve your own problem ? 
For that purpose , I have put the following command in a for loop : #CODE 
I work at a large company and do not have admin rights . 
Note that it only detect truly equal placements , if you would have values of ` [ 0 , 0.00001 , 2 , 10 ]` , they would probably still overlap . 
Suppose I have a triangle mesh defined by X , Y , and Z , the 3D coordinates of a point cloud , each a vector of length n , and UVW , a 2D m-x-3 matrix in which each row is a triplet of indices into the point cloud . 
I am trying to create a plot with vertical bars equal to score length , AND with a radiobutton which enables the user to choose which level scores he / she wants to see , as such : 
The 2nd set of data will also include the first as written in this snippet ( looking for good way to delete it that doesn't cause problems later down the line ) . 
So you need to add ` u ` : #CODE 
Regardless , once you get used to it , I find it much easier to write reusable plotting functions in python with matplotlib than in Matlab . 
Now , you should be able to get all the libraries you need just as easily , using , too , Macports . 
I believe that I'm using the TK backend ; I'm not married to that choice , but it's not clear how I should install other backends . 
What is this float , and how can I convert it to a datetime ? 
xtics and ytics labels because they appear for all values of xPoints and yPoints ( they overlap ) . 
matplotlib ( equal unit length ) 
Is there any reason why I can't do this ? 
I am using the following codes to generate the square wave format [ Eg : from 0 till 5 ] using for loop . 
Can you reduce the extent of you problem by calculating something akin to a density , reducing millions of line segments to maybe a few hundred or a few thousand data points ? 
array erroneously filled with the same value 
You can change the ` reduce_C_function ` option if you want something different than a simple average . 
The whole traceback is rather large but seems to revolve around this part ( if I read it correctly ): #CODE 
Matplotlib : WebAgg backend doesn't show any figures 
While this seems what I was looking for , I'm not lucky finding any proper documentation for webagg . 
If I were you , I'd start from this example to see where do these lines start showing up . 
According to efiring , matplotlib does not support NumPy datetime64 objects ( at least not yet ) . 
At least on my system ( OSX , Python 2.7 , mpl 1.1.0 ) , I don't have any issues with panning , etc . 
Yeah , I understand why Pylab exists , and why it seems so attractive to people moving over from MATLAB , but over the long term I feel like it's a net negative because it makes it harder to share code with and get help from everyone who isn't using it . 
How can I get around this ? 
Function of Numpy Array with if-statement 
By default it sets the ( 0 , 0 ) element of the array in the upper left corner . 
I appreciate any help with this . 
At the moment I do a for loop and plot each point : #CODE 
What I currently have to convert it : #CODE 
You should post your new code and the ** full ** traceback in another question . 
I don't want to make any translations / shifts to the data itself ( it's a large dataset , etc ) , so I'd be looking to do this with some matplotlib-trickery . 
Raster to Numpy Array - how to change a default color-scheme of a matplotlib plot 
or invert the Y-axis : #CODE 
So , a solution is simply to create a list of all objects that will be refreshed , and to call this ` remove ` method for everyone of them : 
The best choice is to make your widgets in the order you wish them to appear . 
Python : How to generate a power law graph 
Unfortunately the broken command is also preventing me from tryig this , so I'll have to fix that first . 
( ` 1 ` is the the start index , ` 2 ` is the increment ) In contrast , ` x [: : -1 ]` just specifies an increment of ` -1 ` , thus reversing the array . 
One way around this is not to use the pandas ` plot ` method , but to directly the matplotlib's ` plot ` function . 
where ` numctrl_ccm90 ` and ` numctrl_ucp90 ` are the widgets corresponding to the collimated counts at -90deg and the uncollimated counts +90deg etc . 
Any tips ? 
I would appreciate any comment or suggestion . 
I looked at ` masked_array ` which is different . 
Without having all the code listed , I had to make some assumptions about the data and about what you wanted . 
You mean ` Axes3D ` is different from ` axes3d ` ? 
If you already have python and gfortran and such , jump in at the point where you need . 
Maybe you know a fix for this ? 
How can I fix this ? 
The background is that the array's values represent the water depth of a square grid . 
And try debugging one step at a time : make sure you have good data before you try to plot it . 
What do you mean the " label " ? 
To do this we ll use the standard y=mx+b line equation where m is the line s slope and b is the line s y-intercept . 
If you check the Matplotlib example for the function you are using , you will notice they use a similar methodology : build empty matrix and fill it with strings built with the interpolation method . 
But the question still remains- how do I fix it ? 
Based on the plot selection I want to set the plot title , so I need to know the current x -axis value after the plot is shown . 
I have tried the following code but this creates 2 separate charts but I would like this all on one chart . 
Any advice would be greatly appreciated . 
How can I add colorbar to this code ? 
I'm reading in a file with several columns , each of which contains around 100 values between 1 and 10 . 
But for the all three plots We have same temperature T=4K . 
I've downloaded a number of nifty programs and utilities ( and info sites ) / quantum gis , virtual terrain project , grass , I'm looking to get an xyz csv file into a format that these tools can use . 
I have all the plots as several modules in python / matplotlib already coded , so that's perfect . 
At some level , this is purely a matter of taste and depends a bit on what you are doing . 
I really want to plot all the points in my matrix instead of the ( M-1 ) x ( N-1 ) . 
I want to set a colour scheme for my python plots , so that they don't repeat the same colour like they are for A and H in the top plot shown below . 
I've got a 2D numpy array with 1.0e6 as the no data value . 
Is this what you mean ? 
This is somewhat based on guesswork , since I can't test it with ` matplotlib ` , but something along these lines might work : #CODE 
Here is some sample code resembling my case : #CODE 
This is the result for printing every item in the xyz list and piping it into a file . 
Or is your plot supposed to be an average or something ? 
I just wasn't sure if changing some plotting functions around would help . 
I agree with you , but I just tried to make code which uses as few less well-known features of ` matplotlib ` . 
You could add a URL to the image , so someone with enough reputation can upload it to your post . 
' plot ' shows me a line interconnecting all data points , but not the data points themselves ( unless there's a way I don't know of ) . 
Did you add matplotlib to your PYTHONPATH ? 
I have successfully edited and put it in my code and works fine , but my problem is that I can't show multiple lines of data in the plot , I want to show up to 10 lines in my graph but I'm getting a lot of errors when I try to add them . 
That is correct , it draws a line from the lower left to the upper right of the plot as stated in my answer . 
The problem arises when I select a subset of the data based on the time , ie : #CODE 
Do you have any suggestion to fix this ? 
I think I'm going to start from scratch so that I know what I'm doing but I need help on where to start . 
It was a simplified example since I didn't have your code at the time . 
The question does not define matrix very well : " matrix of values " , " matrix of data " . 
The algorithm returns an array of ranges , then I create several histograms for the same dataset using different ranges . 
( Since all you're saying now is " I have this equation and it doesn't work " , we don't have enough info to answer this and it's not a question that can be answered . ) 
This method generalizes to any number of rows ( or dimensions ) . 
Another possibility would be to put a tuple into flist : #CODE 
I downloaded the 100meg datafiles from the full Basemap distro . 
Do you have any other ideas to handle that ? 
This controls whether the data limits or the bounding rectangle's shape are changed when the aspect / limits are changed . 
Could you be able to give me some hints how to solve this ? 
like you could in MATLAB , as well as having all the features of 
I tried setting all x-values to ' 1 ' , but it turns out as kind of a 2d-diagram , anyway : 
This is the hacky way to fix your aspect ratio on the image . 
Is Anaconda's main advantage then that one has essentially all one would need , without needing continuously to pip this and pip that ? 
Just for the record , there's no need to jump through all of those hoops with PIL . 
I'll play with Dermen's link as well , but at the moment the most pressing problem is not having the data points be all squished together because the spacing between the ticks is very very small . 
This is perfect , though I'm not sure exactly where I went wrong ... 
you can manually set the size of the colorbar using : #CODE 
Plotting values From a Array in Real time using matplotlib in Python 
I will also gladly accept any examples where someone has built up any other distribution . 
How sure are you that the code * runs at all* ? 
I was temporarily disappeared at my day job ! 
In addition , you'll find that ` pylab ` leaves a generous , often undesirable , whitespace around the image . 
Force plot to have a special size or to increase the distance between subplots 
If you do all the data processing beforehand , and provide the ( nearly ) finished product . 
To make it clearer I want to get the x-axis and the y-axis range so as to find the mid point of the grid . 
and you might want to add the ax to the colorbar . 
The question is : Why does the ` matplotlibrc ` file have an invalid backend ( ` pyside `) value in the first place ? 
Is that a bug ( could not find any reports to this ) , a feature ( ?! ) or 
I'm not sure how you want to convert your data , but if you have irregular line lengths , the easiest way would be something like this : #CODE 
No worries , I was coming at this from a radio perspective not a gamma-ray one . 
How can I fix this ? 
Is not there any wx example in your iPython distribution ? 
Hmm , sorry , maybe I wasn't clear - colours [ li , :] only substitutes the tuple with index " li " , but I need to substitute all tuples with indices in this list called " indices " ( and li=length ( indices )) 
I've also tried installing 1.3.0 through pip as well as source . 
Any help would be much appreciated . 
Maybe you need to rescale with 1 / 30000 to be at the right place on your graph . 
Changing this would require low-level tweaking . 
I've also refined your approach to allow zooming in over a section of the data and to produce better results at the borders . 
Any help would be appreciated . 
Can someone help me sort out the issue resulting to such an error . 
Is there any way , I can save a magnified version for proper visualisation later ? 
So let's say I only need ` 4402 x 4410 ` but I don't know the index . 
I had to upgrade to most recent MPL to find the subplots method of pylab . 
averaging elements in a matrix with the corresponding elements in another matrix ( in python ) 
We consider now that ` -- pylab ` was a mistake , but that it was still really usefull at the beginning of IPython . 
Is there a way to increase the thickness and size of ticks in matplotlib without having to write a long piece of code like this : #CODE 
Or you could define column ' date_obj ' as the index of your data : #CODE 
Am I missing something , or have you left out all the plot commands ? 
This is my first attempt at python , however , and am still finding out how it works , so please put me right if this idea is fundamentally flawed ! 
Finally , here's a sample portion of an old ( bad ) data file : #CODE 
If you you have any questions , please leave a comment . 
Where exactly is the problem ? 
The last dutch sentence does mean something like : The system can't find the specified file . 
This is nice of them , as not all Windows users have C++ compilers available . 
The root of the problem is that matplotlib tries to put all bins on the plot . 
This question has probably a totally simple solution but I just can't find it . 
Any suggestions would be awesome . 
In non-interactive mode , display all figures and block until the 
This is all very new to me . 
1 ) Let's assume I take 1 sample ` x = [ 1 , 2 ]` from the dataset 
I have a 4d or higher ellipsoid in matrix form ( which form is important ) . 
I didn't look into the documentation , simply used the dot completion on the handle 
Here is your example with those modifications and also with a single layout computed at the beginning that is reused in the loop . 
I now understand that I need to generate all the triplets across the three tables ( to cover all the combinations ) . 
i dont know how array concatenation works on numpy arrays 
I can't seem to get my head around why it doesn't show the value 128 as grey with I have chosen the cmap to be gray-scale . 
what do you mean with %s ? 
If that isn't what you want , I would recommend doing something where you create a PNG image of what you do want , create a child QLabel , and move the child widget around in your mouseMoveEvent . 
Any suggestion is welcome . 
I was trying to add a custom widget into qtdesginer using following code #CODE 
source : #URL 
As has been pointed out above , this will get more complicated if the both the y graphs were defined at different x points . 
@USER Simoes , what happens is you have Nan's in various places in your array ? 
An example script with generated data rather than the real data is : #CODE 
Also , it's harder to reproduce across multiple machines ( even if that just means " my current machine , and the new laptop I'll have 18 months from now when I've forgotten how to solve it ") . 
The words ' non-default ' and ' normal ' are mine - I'm not sure what they mean yet . 
( think of it as a unit square ) 
This code uses a Counter to count the number of instances that an object occurs in an array ( in this case counting the number of times an integer is in your list ) . 
In the image above I'm trying to extract iso-value 25 from the scalar field of f ( x , y )= x^3+y^3 . 
Is there any other way I can plot my result continuously ? 
at lag 0 , the ACF is 0.5 . 
The example talks about a fit to power law , but clearly a straight line could be done as well . 
As I see it , I could resolve and make it work if I could adjust the line spacing / height to be less than what the font requests . 
tells the writer to temporarily save the frames to disc before composing the movie , which side-steps the problem . 
Select starting color in matplotlib colormap 
I know this question is a couple years old , but since there is no accepted answer , I'll add what works for me . 
include the piece of the code where you make call to ` mpl_connect ` 
Is there any ready to use function based on python's matplolib ? 
-2 is high of indices range and then low of indices range 
Any insight you could provide that would help get me started the proper way would be fantastic . 
( I can open a new question if that's perferable ) 
For example , here's a simple bit of code that sets up a list of colors based on whether the data is positive or negative : #CODE 
With that example data I was getting " shape mismatch " errors . 
Then , I'd use POV-Ray or Blender to model the planes at whatever angles , spheres for the round things ( planets ? ) . 
So , thus far , all I produce is : 
I'm hoping I can find a funciton ( or build one ) that does new_ids_arr = new_ids * new_ids^Transpose ... or similar 
Any help much appreciated . 
But there are still white edges around ( I have added a screenshot to my original question ) 
How could I use ` matplotlib ` to represent such a matrix as a grid of red and black squares ? 
The thick grey line in the middle of the violin is the bootstrapped 99% confidence interval of the mean , which is the white horizontal line , both from pointplot . 
@USER I up voted you but add ` import numpy as np ` 
So , when you reduce to 2 dimensions , the power is the sum of the first 2 eigen values . 
Indexing starts with ` 0 ` in Python ( and most any other programming language I know ) . 
I have a matrix file of Hurricane data and it plots with the command : #CODE 
If they enter 30 , they will look at a 30-minute time window , if they type 5 , matplotlib picks this up , data gets trimmed , and only 5 minutes worth of data gets shown . 
In the meanwhile does any one has a better suggestion ? 
I want it at y= 10.5 , specifically . 
I then plot each of these values , but to get a more sensible result , I need to take the mean of all of these files for each cell , and plot that with the standard deviation obtained as error bars . 
I'm using data of the form : ` [ num1 , num2 ,..., numk ]` ( an array of integers ) . 
For the time steps where convergence is reached Id like to have the x-axis green and for non-convergence starting from light red for not so bad to red for bad . 
For instance you can add #CODE 
I don't want to get a newer one just incase any of the practice problems / lecture problems are incompatible with a newer version of IDLE . 
All of these frames are loaded at the start of the application . 
if you suspect one specific builtin , try checking it's type ( ` type ( dict )`) , or look at the properties / functions it has ( ` dir ( dict )`) . 
In some work I have done we used a simple approximation of the derivative , when this changes sign you have a peak ( in 1D data ) , one can then add some parameters to remove peaks due to noise . 
Here's a working code sample that shows you how to get it working . 
How can I change this behavior and use Arial fonts for all of my plots ? 
How to divide ytics to a certain number in matplotlib ? 
Matplotlib cannot find facefile , is using old Python interpreter location 
do you get any message when trying plotting it ? 
I have mac os x 10.5.8 and was wondering if anyone could recommend where I could find the proper package . 
Well , this isn't a wxPython answer but I've used Chaco for this sort of thing and it's pretty straight forward . 
The point IS at its position namely ( 1 , 1 , 1 ) . 
All is fine again . 
The next plot to show has the same clusters and colors but a different location . 
@USER The first approach would help only if the internal representation was the problem , but it doesn't solve the underlying size issue . 
After hours analyzing the source code of basemap's ` tissot ` function , learning some properties of ellipses and lot's of debugging , I came with a solution to my problem . 
You'd need to refactor the code to break your worker method up into smaller pieces that multiple processes can work on at the same time , and then pull it back together once all the pieces are ready . 
are you just trying to open a separate window or display that window inside of your main window ? 
The one catch : if you embed a PDF inside a textbox , Word will rasterize this when you save it to a PDF . 
How can I address a specific line ( by name , by number , by reference ) throughout the program and delete that line ? 
And also I need to do that for all variables simultaneously . 
` newtitle = " %s\n$%s$ " % ( ' text\ below\ the\ main\ title ' , newtitle )` 
Have you had a look at the ` webagg ` matplotlib backend ? 
find the file from the link 
If you want separate figures , as the title of your question suggests the ` subplots=True ` argument might get the job done . 
I suggest removing all the domain specific stuff . 
Just change ` shape ` to ` ( 10 , 10 , 10 , 10 , 10 )` and see . 
Any help would be appreciated . 
However , ( at least what I have seen is that ) sometimes after the plot has been displayed , it goes into a " Not Responding " mode and you just have to kill the process anyways . 
It consists in a floating-point parameter P , with dimension NxM , and each pixel is geolocated by the fields latitude and longitude ( each of size NxM ) . 
In order to change the order of the bars , you should change the order in the index . 
This does not work when I replace the random plot data with an actual color PNG image . 
For example , say you had data between 0 and 1 but didn't like the colors used at the extremes of the colormap for 0 and 1 . 
After searching through multiple posts , I basically want to use a generic colormap ( rainbow ) and multiply my third array by the colormap in order to display different colors for each of the xy points . 
Hi , Thanks Tobold , I will check the source code you mention . 
This next block is what does what you want with the ` for ` loop . 
Say that I have 6 variables that I want to sort into these 3 groups and plot like a venn diagram . 
ValueError : setting an array element with a sequence . 
My need is that I want to plot these nominal values of both actual and predicted ones on the same plot so that It will be easy to compare how good was the prediction as compared to actual values . 
So , open up your favorite Terminal emulator and enter #CODE 
Also , the lists need to be equal length . 
Because matplotlib will convert things to numpy arrays regardless , there are more efficient ways to do it . 
And here is where I grab the data and I try to update the limits of the plot #CODE 
Any help is appreciated . 
Which is the why , conveniently , ` colors ` keyword exists . 
How can I find the code to support multiple plotting scales and X-axis on seperate panels using sizer routines ? 
I'm trying to build Matplotlib from source with Tkinter . 
This means that you can install matplotlib locally in your ` virtualenv ` and it will find all of its backend dependencies in the system-wide ` site-packages ` . 
I am trying to create a simple GUI using tkinter which will just open a window displaying the plot . 
Web convinces me using this is no good idea after all : For example #URL " Avoid 3-D charts at all costs . 
Using pure Python , you can extract columns ` 1 ` , ` 5 ` and ` 7 ` by using the following nested list comprehension : #CODE 
Is there any way of getting some input from a user in matplotlib ? 
` theta ` is in radians instead of degrees . 
' plot ' shows me a line interconnecting all data points , but not the data points themselves ( unless there's a way I don't know of ) . 
I could divide the ` peak_top ` by 2 to find the half height and then try and find y-values corresponding to the half height , but then I would run into trouble if there are no x-values exactly matching the half height . 
why can't it find the file ? 
However , the resulting image always has the same resolution , around ( 250x250 ) . 
Installing those packages from the source can be a pain , especially on Windows and OS X . 
Sort arrays by two criteria 
However , I am kinda confused , any help would be appreciated . 
Unfortunately if you search this array for all subnodes given some index you only get some of the subnodes contained by the original triangle because most nodes belong to several triangles and they are only added to one of these . 
If I set shrink 1.0 and fraction to anything , it shrinks the graph , not affecting the colorbar size at all , until changing fraction causes it to be exactly what I already have , at which point changing them stops doing anything . 
It doesn't add anything else . 
Now I just wish they'd add some kind of drop down menu to matplotlib ... 
Thanks for any help 
The source data's list is called xyz . 
I have been looking around and maybe it could be a version-of-Pyhton problem . 
( I want to reduce the size of the outlier ) #CODE 
This should work for any number of columns and length of data . 
Any assistance would be greatly appreciated , this has been doing my head in . 
where axes_square is simply : #CODE 
I can put together a more detailed answer based on that package if desired . 
It's fairly easy to speed it up by making an approximation -- just take the 2D histogram and blur it with a guassian filter of the right radius and covariance . 
I've tried the solutions posted by Marek and qarma attempting to obtain the coordinates of the bins rather than the index of them , like so : #CODE 
What do you mean by make sure the ' backend setting for pylab is not set to inline ? 
sets the initial x , v of every particle in an array 
Post more information on structure of your array . 
Here is a sample code if anyone wants to play with it . 
Are all these settings really necessary to achieve the result I'm looking for , or is there perhaps a more compact way to accomplish my goal : #CODE 
I want to select ( in the SQL sense ) just columns ` b0 ` through ` b9 ` from the array , giving the structure #CODE 
I can't find any method to tell mpl to plot all of the data , it seems that there are only methods for drawing every n'th element or similar ( and passing 1 does not help ) . 
I haven't checked any of the details and you should therefore not rely on it to be correct . 
You can access members and slices of the array as you would with normal numpy arrays . 
Running 1000 simulations at the same time might be a bit expensive though . 
I need to look at other formats that allow create . 
finding element of numpy array that satisfies condition 
All the results ( percentage ) are the comparison between the described condition and the reference which here is the packaged ATLAS library . 
how do you use ` slicing ` to extract something from ` x ` ? 
Mapping a numpy array pairwise 
If that is all you want to do , it should just work otherwise look at #URL since subclassing an array is not that simple . 
If you want to access an individual element using 2D notation , you can create a view and work with that . 
Regarding 3 , I don't think this would be necessary since the above trick can also be applied to an array allocated by ` shmarray ` . 
And here a list with the names of all distribution functions available in Scipy 0.12.0 ( VI ): #CODE 
I get this error whenever i try to use any functions of matplotlib such as graph etc ... 
Numpy is designed for repeated application of the exact same operation in parallel across an array . 
Each B i is a ` k ` -by- ` n ` matrix . 
I created a copy of the initial array in the hope that it would sort it out but it still doesn't work ! 
So far , all the solutions I found required converting to IPLImage . 
However , I haven't tested this very systematically , and it's likely that for smaller matrices the additional overhead would outweigh the performance benefit from a higher thread count . 
but why dimA+dimB to begin with , that just leaves you 0s at the end . 
If you select a list of actual fitness entries , ` indices ` , the corresponding points are given by ` A [ ' point '] [ indices ]` , which is a simple ` ( n , 3 )` array . 
It's also possible to generate an array of indices without using ` enumerate ` . 
If you want to keep the array allocated , and with the same size , you don't need to clear the elements . 
It'll print out all methods and properties of the object . 
Search numpy array inside numpy array 
Assigning a view to a structured array also copies the data , soyour suggestion doesnt work . 
If you know there are not many different values ( relative to the size of the input " itemArray ") , something like this could be efficient : #CODE 
When I import numpy in a script , I don't have all the functions of numpy available , only few of them ( not a lot , and not array ) ? 
It allows to have your custom system inside the home directory accessible via proot and , therefore , you can install any packages without root privileges . 
Hey , is there an optimal size for a block ? 
Numpy array , how to select indices satisfying multiple conditions ? 
I'm still learning git and this whole open source thing . 
A further problem is that a list index can't contain duplicates - I couldn't simultaneously read pixels ` [ 1 , 2 ]` and ` [ 1 , 3 ]` , since my list of pixel x-coordinates would contain ` [ 1 , 1 ]` . 
Your interpretation is , of course , quite correct : ` count ` refers to the number of elements in the ` float* ` " array " . 
This all works . 
Of course it may _not_ be acceptable , but it seemed like it was at least trying . 
I think what should happen is for R to be overwritten with an upper triangular matrix . 
@USER Is there a way to get it to return all possible solutions ? 
Thank you all for some great insight ! 
as numpy array of 0 and 1 values . 
By construction of the problem , there can not be any non-unique values lying within one another . 
Both functions are kind of equal , but are different . 
For a sparse csr matrix ( X ) and a list of indices to drop ( index_to_drop ): #CODE 
This will be relatively slow , and requires you to have twice the free memory space required to store the array . 
I need numpy for this because I don't want to loop through the array n-times for n groups , since my array sizes can be arbitrarily large . 
I wonder what the speed delta between native Numpy arrays and an ` mpfr ` array , seeing as ` mpfr ` is a relatively low-level C wrapper class . 
In this particular case , each 1D column ( ` column = myarray [ i , j , :] `) of the 3D array can be treated independently . 
Any ideas ? 
Then take element 3 and 1 from second row , etc . 
print len ( index ) 
I don't know how to label rows and columns in numpy , so I just made a dict mapping the row label to the row index and another doing the same for the columns . 
It's the same as a normal solve except I know some of the solution to begin with . 
yeah , this is the best answer anywhere where edge cases are important . 
With this , I solve the problem ( may help you ): #CODE 
Teach a man to fish and all that . 
Looks like this would be a good place to use a context manager , so you can say " with fullprint " . 
What do you mean by ' random sort ' ? 
However , it indeed does not allow you to exponentiate any matrix directly : #CODE 
The first one would probably be slow for large data , and the second one does not seem to offer any other interpolation method except splines . 
EDIT : For the record , here is example code that demonstrates the issue : #CODE 
In terms of functionality , it's not a metaclass at all , it's a function which takes a class ( along with some others stuff ) and returns a new class . 
or from source #CODE 
Didn't do any timings here , but it's possible this version has reasonable performance . 
I'll just test whether or not there's a significan space gain on my data with protocol 2 . 
Just wanted to add that the moving average function has been extracted into the [ Bottleneck ] ( #URL ) library if pandas seems too heavyweight as a dependency . 
` numpy ` can use ` malloc ` / ` realloc ` for creating an array of objects of ` sizeof ( int )` . 
If you read through the documentation of those engines you will often find statements saying that they are optimized for speed ( 30fps - 60fps ) . 
I know I have to look at each row , but I don't want to do it with loops . 
I'm hoping this can at least save someone a few hours of hopeless research for this topic . 
What does " doesn't work " mean ? 
You might look at your code that generates the ` y ` values , and see if that would benefit from the use of additional numpy or scipy functions . 
For example , give the list [ ( .3 , ' a ') , ( .4 , ' b ') , ( .3 , ' c ')] I'd like to sample ' b ' 40% of the time . 
The square bracket idea you mentioned works for my current problem . 
However , when I try to use a weighted average #CODE 
If I scale that to a 2000 X 2000 np array , here is what I get : #CODE 
Unfortunately , I'm not aware of a numpy implementation , but I did find this : #URL 
We can do this quite neatly using numpy , without having to worry about the channels at all ! 
It's not exactly the most efficient way , but it will work , and not require keeping a copy of the file in memory ( or two ) . 
I'm trying to find the fastest way to find the first non-zero value for each row of a two dimensional sorted array . 
Convert the numpy array into a list first . 
@USER do you mean that grid dictonary holds the results in memory or something else ? 
@USER : This solution is indeed designed to give the set of all the numbers found in the array . 
This way at most one line is in memory at any one time . 
It's low efficiency , I want to know are there any builtin functions that can do this in NumPy . 
The raw hardware data is 32-bit signed integer , which becomes float when I convert it to normal physics units ( m / s ) 
Calculate subset of matrix multiplication 
EDIT : as to what DSM pointed out , OP is infact using a numpy array . 
As for the second question , ` delete ` has been suggested before : #CODE 
The remaining rows of the matrices are all linear combinations ( in fact exact copies for almost all submatrices ) of these rows . 
I'm printing the contents of an array with a header . 
Edit : If you need to save memory try radix sort , which is much faster on integers than quicksort ( which I believe is what numpy uses ) . 
The data was in following order ( with sample data ) #CODE 
definitely a good solution , nevertheless I'd like to solve this without a range , but the nearest neighbour . 
For my recent project that works on the order of 20000x20000 matrix entries , I will quickly and disastrously use up all of my workstation's 8GB of RAM and more . 
Well , this might give a small speed-up just because it uses less memory . 
Try using ` all ` ( edited to return ` int `) : #CODE 
: I don't care if the statement modifies array or not . 
Rather than doing nans I put in -1 and then filtered on match = b > = 0 . 
Removing duplicate columns and rows from a NumPy 2D array 
arrays to extract arrays of the same 
as i have to test if it works or not so minimum i have to try with 10-15 different types of images . its not specific images it can be any image of people . 
So having a python loop , and having to sum all the results together , is taking 390 ms more than 200 times what it takes to solve each of the 200 systems that have to be solved . 
You could then sample pixels at the locations of the new points . 
I could use an idea about either how to fix the compilation errors or another way to convert my python objects . 
I tried every possible combination resulting with a 0 bytes file if the extension was mpg , and 5.5kb if it was avi . 
Fortran 90 DOES support arbitrary lower bounds on arrays , and borrowing from that paradigm sounds quite plausible . 
I want to perform an operation on a that increments all the values inside it that are less than 0 and leaves the rest alone . for example , if I had : #CODE 
I like how this one uses straight python slicing and doesn't require numpy 
I select the first value using ` ys [ 0 ]` . 
While waiting for the next buffer to fill , I'd like to process the most recent buffer with numpy and save the result . 
( They all do the same thing , in this case . ) 
As of PIL 1.1.6 , the " proper " way to convert between images and numpy arrays is simply #CODE 
So first you need to construct an array that represents the rows you wish to select . 
Any ideas how to improve this ? 
If this is the case , you can easily plot a known asymmetric shape and the plot will tell you everything . 
Apart from the compression part , this shouldn't be any slower than normal . 
Acquiring the Minimum array out of Multiple Arrays by order in Python 
first copy #CODE 
Getting all points where y=2 . 
moving average function on numpy / scipy ? 
for a N dimensional array : #CODE 
From looking at #URL it seems that it was originally required because indexing with ` ... 
In other words ... yes you can trust it to be faster ... but don't think python is that slow that you cannot do a for loop over 3 items without waiting for ages ... 
And all that while you're getting lunch . 
If you do want to raise some sort of exception for invalid data ( not type checking ) , either let an existing exception propagate , or wrap it in your own exception type . 
casting are used in Numexpr , in contrast with NumPy , where array types 
I check mathexchange and while making the tags for the post , it didn't have any the ones that would seem relevant like scipy and numpy or even sparse . 
Any help would be appreciated . 
Say not that efficiency is a secondary priority ; say instead that I want to perform bivariate optimization : pythonicity + efficiency ( hence the post title ) . 
Does numpy have any constructs to make this easier ? 
If you view ` P ` as a rank-2 tensor then only three options exist for the product of ` P ` with itself , 1 ) either all the indexes cancel leaving you with a rank-0 tensor ( a scalar ) , 2 ) 1 set of indexes cancels and you are left with a rank-2 tensor ( a matrix ) , or 3 ) none of them cancel and you're left with a rank-4 tensor . 
At the moment I made a custom iterator class that builds a list of lists . 
This basically sees whether two circles ( with coordinates that correspond to the indices n and m ) connect . 
It would mean Gaussian quadrature using points along the line that are easily evaluated . 
I see one potential problem - beta is defined as 1-dimensional , but its value is given as 2-dimensional ( dimensions of size 1 still count as dimensions ) 
Can you provide an algorithm for computing them based on block size ? 
If ` X ` is your array , #CODE 
You could cast the array to a list : #CODE 
But if you for example self compiled from the development version an update may fix most of it . 
In VTK I am able to use the following snippet to save the render window as an image . 
In other words , the 4th row in A_sorted was the 1st row in the original array , A , etc . 
Add a ` return ` statement at the end of the method . 
However , with different input sizes , using fft's to do a convolution can be considerably faster ( Though I can't seem to come up with a good example , at the moment ... ) . 
That is a shallow copy ... 
@USER The solutions there all make use of the fact that only a 3x3 sliding window is needed , but I need something that works for all sizes of templates . 
A masked array is useful here : #CODE 
Second I would like it to be easily expandable , that I can add new functions easily . 
If i have two variables - where they either are a 1d array of values length n , or are a single value , how do i loop through them so that I get n values returned . 
I think masked arrays have been in numpy for a few years now . 
I have an array defined in this way ( extracting the third column of a dataset ): #CODE 
I wasn't aware of the option of using ` data [ list ]` to select multiple columns . 
Beware that if the type of the output array is 
Any ideas how I would do this calculation in a simpler way ? 
Those take up about 15MB of space which , considering that I get about 1000 result files in a series , is unacceptable to save . 
I'm trying to implement a logic where I'm trying to subtract each element of an array from every other element of the array and then find the minimum difference of the result . 
My final matrix should have 5416 rows with 500 000 column each . 
I am attempting to process data saved to CSV that may have missing values in an unknown number of columns ( up to around 30 ) . 
Actually the surprise is still hidden in a way , because in your examples ` a [ indices ]` is the same as ` a [ indices [ 0 ] , indicies [ 1 ]]` but ` a [ indicies , :] ` is ` a [( indicies [ 0 ] , indicies [ 1 ]) , :] ` which is not a big surprise that it is different . 
Slice numpy array wth list of wanted rows 
Here is a slightly more complex version that always returns a view into the original array ( of course provided that you don't do any advanced indexing ; this should be guaranteed by your specification of valid indices ): #CODE 
If the array is one-dimensional , this means it has no effect . 
The problem with using ` view ` , however , is that a 32-bit integer becomes viewed as 4 8-b it integers , and we only care about the value in the last 8-b its . 
So this won't solve OP's question ( unless his nD is 2 or 3 ) . 
Note : this was meant as a comment , not an answer ... just needed more room to put in the example above . 
If I wanted to change the data type of a numpy array permanently , is reassignment the best way ? 
The shifted shm-allocated array is indeed accessible from other processes . 
The second solution you propose is better from that point of view . 
It checks for nans and empty input strings . 
The ability to extract columns and rows by header name ( as in the above example ) 
With Python 2.6.5 or Python 2.7 , and Numpy 1.5.0 , I don't get any error . 
Returns a boolean array the same length as ` ar1 ` that is True 
If the objects in the array are not fixed size ( such as your MultiEvent ) the operations can become much slower . 
@USER : True the code I gave would not be all the efficient 
You print ` Length of together 2708000000 ` - where is that ` print ` statement in your code ? 
Is there a way to copy just the reference to b , so that when I change b , the change is reflected in ` a [ ' B '] [ i ]` ? 
If the matrix is not symmetric be careful about the order in dot . 
So I changed the " array " to matrix . 
config paths , cflags . 
gnibbler : That completely misses the point of the algorithm , this is a simple example , what I am doing is Gauss-Seidel iteration , which infers information about a location in a matrix by using data that has already been inferred in previous entries . 
how do i find the smallest then if this gives me the n greatest values ? 
@USER : you can wrap the insides the prange loop in ` with nogil : ` to use any Python constructs . 
For each ` Xi ` greater than ` lower_limit_X ` and less than ` upper_limit_X ` , I would like to get the number of ` Yi `' s that are greater than ` lower_limit_Y ` and less than ` upper_limit_Y ` . 
and find the roots ? 
I'd like to generalize this to an ellipsoid , that could ideally have any rotation . 
Is there any way to integrate the entire array at once , or do I need to integrate element-by-element ? 
I need to sum 
with ' f ' and ' F ' , or before and after the decimal point for a floating 
You can write that as a matrix : #CODE 
In fact , if you use tuples as Justin suggested and iterate directly over the rows of the array ( ` for row in data : `) , it's actually faster than my method below . 
Using the ` ctypedef ` keyword in Cython will make it add the C / C++ ` typedef ` statement with the given types in the compiled Cython-code . 
Here is how I would compute a subset of the elements of C given a list of tuples of C index values . 
In my specific problem ` A , B ` are slices out of a bigger 3-dimensional array ` Z ` , 
The solution to get all the data you need as you build the list , by using the accessor on each iteration . 
Edit : Actually you could also sort both arrays into one ( and remember which one belongs to which class ) , then go from there by checking where two of the different class are next to each other . 
How can I integrate it from a given value ` a ` to another value ` b ` so that the output is a corresponding array ? 
I want to multiply a sparse matrix A , with a matrix B which has 0 , -1 , or 1 as elements . 
If you turn a ` dict ` into an array , you'll get an object array . 
` Holy CPU cycles batman ! 
Note : Xarray and Yarray are each single-column vectors with data at each index that links the two arrays as sets of x , y coordinates . 
Yeah , my rule-of-thumb is ** numpy ** for anything that can handle small amounts of latency but has the potential to be very large , ** lists ** for smaller data sets where latency critical , and of course ** real benchmarking ** FTW :) 
You need to know what kind of information is stored in each field for the ` struct ` module to make sense of each field . 
Once you have this array , you can get your sums for each vertex as #CODE 
Where are ` plot ` and ` show ` coming from ? 
But it might be easier ( and more understandable when you look at the code in the future ) to just drop into Cython to get this done . 
I can't comment on the Perl code , I simply don't know it at all . 
I updated the answer with a full example . 
There you will find C code which provides the same functionality as ` fmincon ` . 
so for instance , for an array the code looked like #CODE 
I've been trying to find a solution for hours . 
Note that as implemented here , the first method gives incorrect results according to my test . 
This is my test code : #CODE 
This will short-circuit if you just want to determine if any match exists . 
It also helps to know that a resource like Wolfram Alpha is available to you at all times . 
What I meant was if we had three arrays ` X =[ 0 1 2 ] , Y =[ 0 1 1 ] , and Z =[ 0 2 2 ]` there would be six values in the range of greater than or equal 1 and and less than or equal to 2 . 
You should really split this into two separate questions since each part is distinct . 
This doesn't answer your problem exactly , but I think especially with the sum issue that you should see significant speedups with these changes . 
Basically you're iterating through each item in ` Xa ` and omitting the ones that don't fall with the range . 
This code has been written following the tips ( and copy / pasting ): #URL 
I would look at this question : #URL 
As @USER -Anderson asked , why not avoid making an array ? 
to create such an array . 
In short , I find class C to provide an implementation that is over 60x faster than the method in the original post . 
I suggest allocating an array of the correct size up-front , then populating it with data in each iteration . 
edited my solution to include what I would do . let me know if this work , as i cannot fully test because i dont have test data . 
The problem is if i don't want 0:10 , but an arbitrary set of indices . 
and it occurs at the line : ` del innerAry [ j ]` 
Do you have any references to back up that ` NaN handling is much slower than " normal " float at the CPU level ` ? 
What do you mean by the line ? 
For example if we stick with a linear search we can at least start at the appropriate end ( search backwards to find last value matching a condition ) . 
Yes , but you don't get a numpy array out , do you ? 
This seems like a simple question , but I haven't been able to find a good answer . 
@USER : it could also mean they are both strictly periodic and sinusoidal , but their frequencies are integer-independent ( ie . the interference wave is not periodic ) 
Python : How to rotate an array ? 
@USER Your edited sample input is still not consistent with your expected output . 
You can calculate the variance yourself using the mean , with the following formula : #CODE 
Populate numpy matrix from the difference of two vectors 
where #CODE 
Interleaving two numpy index arrays , one item from each array 
I was worried about the performance , but the difference in load time is tiny for me . 
I looked for an online reference but couldn't find one . 
Numpy / Python : Array iteration without for-loop 
If you want it printed with commas , you could convert it to a Python list : #CODE 
I have included my code to see if you can help me implement some kind of ' fminsearch ' to find the optimal parameter values k0 and k1 that will fit my data . 
Any ideas what this could be all about ? 
What does " IIUC " mean ? 
I can't seem to find examples that don't rely on the former syntax . 
Any help would be greatly appreciated . 
Where blocks is a 3 dimensional numpy array . 
The science / engineering application I'm working on has lots of linear algebra matrix multiplications , therefore I use Numpy matrices . 
when I print Chao , the product of this loop I currently have this : 
pyopengl buffer dynamic read from numpy array 
I also have an array which is my desired subset of ages . 
It does automatically expand the array , but now every item in the array is ` None ` and cannot be changed . 
I'm liking fortran more at the moment because by the time you add all the required type annotations in cython I think it ends up looking less clear than the fortran . 
( There are also chances that Python stores the number on the heap and all you get is a pointer to it , approximately doubling the footprint , without even taking in account metadata but that's slippery grounds , I'm always wrong when I talk about Python internals , so let's not dig it too much . ) 
Do you have any clue ? 
your method works , @USER . put it into an answer to get some acceptance points . 
that blas is reference blas from netlib - the slowest blas around . install atlas or mkl instead . 
The ` not ` operator implicitly tries to convert its operand to ` bool ` , and then returns the opposite truth value . 
The recommended way to do this is to preallocate before the loop and use slicing and indexing to insert #CODE 
If the simple sort solution is good enough , clearly go for that . 
Basic idea being , I know the actual value of that should be predicted for each sample in a row of N , and I'd like to determine which set of predicted values in a column of M is most accurate using the residuals . 
Fill in missing values with nearest neighbour in Python numpy masked arrays ? 
Any tips on what I'm doing wrong ? 
SO I have a file having three columns ; Frequency , Power spec . 
In numpy , your array is 2 x 5 , isn't it ? 
For small-ish problems , I would certainly just create the new array . 
If each element takes up 4 bytes , it would require 4,000,000,000,000 bytes of memory . 
TypeError : must be str , not bytes 
Error while computing probabilities of an array list 
This produces the array , but I don't know which row corresponds to which year-disease . 
I'm not sure what you mean by " all from numpy " , but you should never need to use more than one form of ` import ` at a time . 
The problem is that you have an array of strings , not an array of numbers . 
how can i effectively check items of a list of tuples against all the items of another using numpy or tabular ? 
Replace part of numpy 1D array with shorter array 
Python 2.6 numpy interaction array objects error 
So for square matrices it is basically syntactic sugar for the exact same operation . 
If we evaulate the ij th element of the matrix U*A*V , then it must equal both #CODE 
How can I get new array ` B ` such as if ` row_set = [ 0 , 2 , 5 ]` , then ` B = [ A_row [ 0 ] , A_row [ 2 ] , A_row [ 5 ]]` ? 
I have tried the following to fix it : 
The weighted sum result is approximated by the multiple passes and actually after very few of them the output is already smooth . 
Basically , it comes down to checking before you add . 
Any ideas ? 
Do you really have matrix type or just list of lists from python ? 
zI [ N-1 ] = f ( xI [ N-1 ] , yI [ N-1 ]) . 
The actual size of the numpy array is 514 by 504 and of the list is 8 . 
` array ([[ a , a , a , a , 0 , 0 , 0 , 0 , 0 , b , b , b ,... ] , 
What's wrong with just separating it out into real and imaginary parts ? 
I have an N-dimensional array and a set N index arrays , who's values I want to increment . 
As @USER suggests in a comment , if you really want a 3D array -- which is not entirely clear to me from your code sample -- you can use : #CODE 
In another question , other users offered some help if I could supply the array I was having trouble with . 
The other thing is changing the size of the ticklabels in the colorbar which I haven't figured out . 
Unfortunately , these lines are fast already , but I will take any speedup to offset the IO issues I have using GDAL . 
First you need to write a function that when given an array of values , with the middle one being the element currently examined , will return some computation of those values . 
Thanks in advance for any help . 
so what you want is some sort of recursive assignment -- but i don't believe there is any guarantee that this will settle down into a constant value . sure it does in your case , but not in general -- for example : ` a [: ] = 2*a [: ]` would loop forever . 
Just initialize the array of ` float* ` to point to each of the rows in the 2-D array . 
Do things go wrong gradually and more and more , or all at once ? 
If you don't find anything useful then try " R " . 
Unfortunately , when I tried it I got the error : " ValueError : array is too big . 
EDIT 2 : This raises another question - What is ` env ` and why does ` make ` add it ? 
However it doesn't give you negative overflows , probably because ` uint32 ` fits inside the positive values of the ` int64 ` . 
Consider for example the array : #CODE 
One difference could be the result of python having to take extra steps to resolve the float64 types . 
How do I turn this into a numpy matrix ? 
I want to rotate an array but not as a whole , only small portion of it . 
is so much more readable than any dot ( a , b ) equivalent . 
The array looks like : #CODE 
Constructing an n-by-n matrix in Numpy is easy and fairly efficient . 
Numpy array broadcasting with vector parameters 
It only works like this for numpy ` array ` s . 
@USER : Also there's a mistake in your code I think : because when you set elements to NaN in each iteration , the elements are not restored to their pre-NaN values for the next iteration ! 
Quick question : is there any reason why you use 
Now the question is , which equal area projection shall I choose in order to have comparable area sizes for the polygons . 
numpy array multiplication issue 
( 2 ) When I change the connection keywords to check_same_thread=False , then the full pool of workers is used , but then only some queries succeed and some queries fail . 
It also has the advantage of being able to load and store transparently with HDF5 . 
Bah : " operates on two n-dimensional arrays " should be " operates on an n-dimensional array " above . 
Instead , they expect the user to either pass an array of shape ` ( r , c )` exactly , or for the user to pass a 1-D array that broadcasts up to shape ` ( r , c )` . 
Though , it isn't so straight forward because I don't necessarily know how many duplicates of each lon or lat there are which determines the shape of the array . 
geom function takes an n+1 X 2 array and n as input , i guess i'm doing something really stupid ( which i think i am ) or i don't understand this behavior #CODE 
When I tried this , I got sort of similar shaped " tiles " of different colors rather than 3 Gaussian humps . 
There was a comment here saying that the Apple version of python 2.7 comes with numpy so you shouldn't have to install it at all . 
For more general solution , you could use somekind of edge detection method to find only the edge points . 
If you see any errors ; provide a link to the code that can be run . 
What I am looking for is a quick and easy way to find the closest ( nearest neighbor ) of some multidimensional query point in an 2D ( numpy ) array of multidimensional points ( also numpy arrays ) . 
How do I get the ` Image ` part only and how do I convert it to Numpy Array ? 
Maybe it'll save you some frustration =) 
Is there any way to rewrite this functions with Numpy ? 
Is there any easy way to speed my calculation up ? 
A variable in Python is just a label for an object ; giving the object a new label doesn't change the object itself at all . 
But this is actually where the doc belongs . 
So given the sorted version , you can reconstruct the original by " putting items back where they came from " : #CODE 
Note that this all assumes that your values are normally distributed . 
( Bounding box intersections are actually a rather poor way of deciding where to place labels . What's the point in writing a ton of code for something that will only work in one case out of 1000 ? ) 
The python code outputs eleven 0's , eleven 1's all the way to eleven 39's . 
@USER ; in above example z is ( 5 , 2 ) array created by another function , with first dimension from the number of True ( at least one True in x > y ) , here 5 , and second dimention as the first dimension of x , here 2 . 
I have a NumPy array of values . 
I just need the total of all the values instead of the actual values themselves . 
Didn't think about multiplying my array of number by an array of booleans to extract my data . 
not to convert floats to floats would be the first step . 
make a list of all the days 
This way , you could access all the ` A ` through ` result [ ' label '] [ ' A ']` ... 
picking out elements based on complement of indices in Python pandas 
Joran , could you please explain what you mean more ? 
Anyone have any clues for what I can do , or approaches I should research ? 
Appending data to an existing array is a natural thing to want to do for anyone with python experience . 
Note that you get a sorte copy of the array . 
I'm not sure that this is the way that you should do things as I'd expect numpy to have a much more efficient method of going about it , but do you just mean something like this ? 
` KMID ` is a function , not an array , so you can't index it with ` : ` . 
Insert to original code in question : #CODE 
Multiple conditions using ' or ' in numpy array 
Usually , it's best to avoid the matrix class ( see docs ) . 
I have tried two different methods but both of them are slow . 
Not really , you can construct the Counter from any iterable . 
( I have the code ready , but as i'm new to stackoverflow , i don't know where to put it . Here , in this comment field ? Or rather making a new answer ?? ) 
For example , one simple method to generate at most rank ` k ` ( when ` m ` is ` k+1 `) is to get a random valid B 0 , keep rotating all rows of this matrix up to get B 1 to B m-2 , set first row of B m-1 to all 1 , and the remaining rows to all 0 . 
Glad you saw around it ! 
@USER : basically I am converting some matlab code into python , I can not write actual code because that is confidential , ( 1+float ( 100 )) Here 100 is coming from two dimension string matrix , that why I have written float to convert string variable . 
instead of call plot ( test [ " x "] [ 5:10 ]) , you can call the plot method of Series object : #CODE 
The size of a slice with ` 0:5 ` is not 6 as you say : it's 5 . 
This ensures proper display and syntax highlighting - right now someone who would usually fix your formatting is likely to not do it because he'd have to remove all the HTML linebreaks on his own . 
I believe you've reduced the problem to a one of finding roots . 
In order to make sure it is still multiprocessor safe , I believe you will have to use the ` acquire ` and ` release ` methods that exist on the ` Array ` object , ` a ` , and its built in lock to make sure its all safely accessed ( though I'm not an expert on the multiprocessor module ) . 
So , given your matrix M , your problem asks to maximize the PB function #CODE 
I would like to find all elements within a specific range . 
I cannot seem to find how to do that . 
Any thoughts ? 
If you want the PRNGs to be independent , do not seed them with anything . 
In the end I'll probably take n randomly selected samples . 
This is a mystery to me , though I would guess that there must be more overhead associated with accessing an array element than with appending to a list . 
If it's not reasonable , you can always decompose the matrix multiplication yourself . 
If the vectors do not have equal dimension , or if you want to avoid numpy , then perhaps , #CODE 
In particular , you can't index a 2D matrix with a single integer , because -- well -- it's two dimensional and you need to specify two integers , hence the need for the extra 0 index in the second example . 
nearly all of which the author responded to and in some cases , 
I added the slow Python code to the description . 
The easy way - pick a random number q [ 0 , 1 ] . 
will be the number of bytes which the pattern of streams will repeat after . 
Any idea how I can later make 1.6.2 in / usr / local / lib work with python-dbg ? 
Because I'm still not quite grasping the method and there seems to be simpler ways to solve the problem , I'm just going to put this here : #CODE 
