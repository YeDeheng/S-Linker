At	O
the	O
first	O
step	O
I	O
used	O
`	O
df.T	B-API
`	O
to	O
transpose	O
the	O
dataframe	O
,	O
and	O
tried	O
something	O
like	O
`	O
df.value_counts()	O
`	O
,	O
however	O
I'd	O

Replace	O
NaN	O
in	O
a	O
dataframe	O
with	O
random	O
values	O

I	O
want	O
to	O
replace	O
all	O
the	O
NaN	O
with	O
some	O
random	O
values	O
like	O
.	O

#CODE	O

Resample	B-API
function	O
throwing	O
error	O
with	O
Twitter	O
Data	O

I	O
then	O
try	O
to	O
resample	O
for	O
analysis	O
#CODE	O

I'm	O
writing	O
several	O
pivot	O
tables	O
using	O
pandas	O
.	O

For	O
many	O
of	O
them	O
,	O
I	O
need	O
to	O
return	O
unique	O
values	O
.	O

In	O
a	O
two-dimensional	O
pivot	O
table	O
,	O
the	O
below	O
code	O
works	O
as	O
it	O
should	O
.	O

When	O
I	O
add	O
a	O
third	O
dimension	O
,	O
the	O
code	O
returns	O
the	O
count	O
rather	O
than	O
the	O
unique	O
count	O
.	O

I	O
suspect	O
this	O
has	O
something	O
to	O
do	O
with	O
the	O
aggfunc	B-API
,	O
but	O
can't	O
determine	O
to	O
what	O
it	O
should	O
be	O
changed	O
.	O

Use	O
a	O
groupby	B-API
to	O
get	O
at	O
each	O
combination	O
of	O
`	O
col_1	O
`	O
and	O
`	O
col_3	O
`	O
,	O
then	O
unstack	B-API
to	O
get	O
the	O
`	O
col_3	O
`	O
values	O
as	O
columns	O
:	O
#CODE	O

Python	O
pandas	O
merge	O
or	O
concat	O
dataframes	O

The	O
data	O
is	O
for	O
2	O
products	O
(	O
BBG.XAMS.UL.S_pnl_pos_cost	O
and	O
BBG.XAMS.UNA.S_pnl_pos_cost	O
)	O
by	O
date	O
,	O
in	O
the	O
future	O
there	O
will	O
be	O
more	O
products	O
.	O

I	O
want	O
to	O
concat	O
or	O
merge	O
(	O
not	O
sure	O
which	O
)	O
the	O
list	O
of	O
dataframes	O
into	O
one	O
data	O
frame	O
(	O
called	O
result	O
)	O
so	O
they	O
look	O
like	O
:	O
#CODE	O

where	O
axis	O
is	O
the	O
date	O
.	O

It	O
looks	O
like	O
the	O
data	O
is	O
merged	O
by	O
date	O
,	O
but	O
I	O
am	O
missing	O
the	O
data	O
for	O
the	O
week	O
beginning	O
2015-03-23	O
.	O

My	O
current	O
concat	O
result	O
dataframe	O
looks	O
like	O
:	O
#CODE	O

Try	O
using	O
axis=0	O
.	O

This	O
should	O
concat	O
column-wise	O
,	O
assuming	O
each	O
dataframe	O
has	O
the	O
same	O
column	O
names	O
.	O

possible	O
duplicate	O
of	O
[	O
Pandas	O
join	B-API
/	O
merge	B-API
/	O
concat	B-API
two	O
dataframes	O
]	O
(	O
#URL	O
)	O

Also	O
,	O
how	O
do	O
you	O
join	O
this	O
back	O
to	O
original	O
dataframe	O
?	O

We	O
can	O
resample	O
this	O
to	O
days	O
;	O
it'll	O
be	O
a	O
much	O
longer	O
timeseries	O
,	O
of	O
course	O
,	O
but	O
memory	O
is	O
cheap	O
and	O
I'm	O
lazy	O
:	O
#CODE	O

How	O
do	O
I	O
merge	O
the	O
birth	O
rate	O
back	O
to	O
the	O
original	O
table	O
?	O

Indexes	O
aren't	O
compatible	O
...	O

Turns	O
out	O
size	O
isn't	O
such	O
an	O
issue	O
.	O

Python	O
&	O
Pandas	O
:	O
Unable	O
to	O
drop	O
columns	O

I	O
try	O
to	O
drop	O
the	O
data	O
,	O
but	O
it	O
reports	O
some	O
column	O
does	O
not	O
exist	O
.	O

#CODE	O

@USER	O
,	O
that's	O
possible	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
deal	O
with	O
it	O
.	O

In	O
my	O
previous	O
experience	O
with	O
pandas	O
,	O
it	O
will	O
automatically	O
turn	O
the	O
second	O
`	O
Q	O
`	O
into	O
`	O
Q.1	O
`	O
when	O
reading	O
the	O
data	O
.	O

However	O
,	O
in	O
my	O
case	O
,	O
it	O
failed	O
to	O
do	O
it	O
,	O
and	O
I	O
don't	O
know	O
why	O
.	O

However	O
,	O
This	O
it	O
cannot	O
`	O
drop	B-API
`	O
`	O
NCDC	O
`	O
either	O
.	O

You	O
can	O
then	O
drop	O
your	O
columns	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
take	O
a	O
given	O
row	O
from	O
a	O
DataFrame	O
and	O
prepend	O
or	O
append	O
to	O
the	O
same	O
DataFrame	O
.	O

Rather	O
than	O
concat	O
I	O
would	O
just	O
assign	O
directly	O
to	O
the	O
df	O
after	O
`	O
shift	O
`	O
ing	O
,	O
then	O
use	O
`	O
iloc	B-API
`	O
to	O
reference	O
the	O
position	O
you	O
want	O
to	O
assign	O
the	O
row	O
,	O
you	O
have	O
to	O
call	O
`	O
squeeze	B-API
`	O
so	O
that	O
you	O
assign	O
just	O
the	O
values	O
and	O
lose	O
the	O
original	O
index	O
value	O
otherwise	O
it'll	O
raise	O
a	O
`	O
ValueError	O
`	O
:	O
#CODE	O

To	O
insert	O
at	O
the	O
end	O
:	O
#CODE	O

I'm	O
not	O
sure	O
exactly	O
what	O
you're	O
expecting	O
,	O
but	O
you	O
could	O
replace	O
your	O
lists	O
with	O
numpy	O
arrays	O
(	O
I	O
don't	O
think	O
it'll	O
improve	O
your	O
specific	O
code	O
):	O
#CODE	O

What	O
is	O
the	O
best	O
way	O
for	O
me	O
to	O
get	O
this	O
data	O
into	O
Pandas	O
?	O

Is	O
there	O
a	O
standard	O
way	O
I	O
could	O
use	O
`	O
read_table	B-API
`	O
or	O
some	O
similar	O
function	O
to	O
read	O
this	O
file	O
directly	O
?	O

Should	O
I	O
write	O
a	O
script	O
to	O
insert	O
commas	O
where	O
all	O
the	O
column	O
breaks	O
are	O
and	O
then	O
read	O
it	O
in	O
as	O
CSV	O
?	O

(	O
I'd	O
just	O
do	O
the	O
latter	O
,	O
but	O
I'm	O
also	O
interested	O
in	O
becoming	O
better	O
with	O
Pandas	O
so	O
if	O
there's	O
an	O
out-of-the-box	O
way	O
I'd	O
like	O
to	O
know	O
it	O
.	O
)	O

Any	O
ideas	O
on	O
how	O
to	O
get	O
this	O
file	O
to	O
load	O
?	O

Unfortunately	O
I	O
can't	O
just	O
strip	O
out	O
accents	O
,	O
as	O
I	O
have	O
to	O
interface	O
with	O
software	O
that	O
requires	O
the	O
proper	O
name	O
,	O
and	O
I	O
have	O
a	O
ton	O
of	O
files	O
to	O
format	O
(	O
not	O
just	O
the	O
one	O
)	O
.	O

Thanks	O
!	O

pls	O
show	O
your	O
input	O
and	O
what	O
is	O
the	O
expected	O
output	O
,	O
in	O
a	O
copy-pastable	O
form	O
.	O

What	O
you	O
are	O
doing	O
is	O
very	O
inefficient	O
.	O

A	O
groupby	B-API
should	O
try	O
to	O
use	O
vectorized	O
functions	O
when	O
possible	O
.	O

Then	O
join	O
them	O
up	O
at	O
the	O
end	O
.	O

Hi	O
Tom	O
,	O
it	O
doesn't	O
look	O
like	O
this	O
works	O
.	O

It	O
outputs	O
just	O
one	O
array	O
and	O
is	O
equivalent	O
to	O
df2	O
[	O
'	O
array	O
']	O
.sum()	B-API
.	O

But	O
you	O
have	O
given	O
me	O
an	O
idea	O
with	O
apply	B-API
.	O

Let	O
me	O
see	O
if	O
I	O
can	O
figure	O
something	O
out	O
.	O

You	O
need	O
`	O
apply	B-API
(	O
your_func	O
,	O
axis=1	O
)`	O
to	O
work	O
on	O
a	O
row-by-row	O
basis	O
.	O

#CODE	O

Another	O
way	O
would	O
be	O
to	O
call	O
`	O
unique	B-API
`	O
on	O
the	O
transpose	O
of	O
your	O
df	O
:	O
#CODE	O

Drop	O
values	O
satisfying	O
condition	O
plus	O
arbitrary	O
number	O
of	O
next	O
values	O
in	O
a	O
pandas	O
DataFrame	O

So	O
my	O
final	O
goal	O
is	O
to	O
drop	B-API
values	O
in	O
one	O
column	O
of	O
a	O
`	O
pandas	O
`	O
`	O
DataFrame	O
`	O
according	O
to	O
some	O
condition	O
on	O
another	O
column	O
of	O
the	O
same	O
`	O
DataFrame	O
`	O
,	O
plus	O
several	O
next	O
values	O
e.g.	O
:	O
#CODE	O

So	O
this	O
will	O
drop	O
the	O
records	O
where	O
the	O
condition	O
is	O
satisfied	O
,	O
but	O
how	O
do	O
I	O
drop	O
the	O
next	O
3	O
records	O
after	O
the	O
condition	O
was	O
satisfied	O
too	O
?	O

My	O
desired	O
output	O
would	O
look	O
something	O
like	O
this	O
:	O
#CODE	O

We	O
can	O
use	O
the	O
boolean	O
condition	O
index	O
to	O
slice	O
the	O
df	O
using	O
`	O
loc	B-API
`	O
and	O
set	O
the	O
following	O
values	O
:	O
#CODE	O

Panda's	O
boxplot	O
but	O
not	O
showing	O
the	O
box	O

Note	O
,	O
`	O
showbox	O
`	O
and	O
`	O
whiskerprops	O
`	O
are	O
the	O
`	O
kwds	O
`	O
of	O
boxplot	O
,	O
which	O
are	O
in	O
turn	O
passed	O
to	O
`	O
matplotlib.boxplot	B-API
`	O
.	O

Applying	O
aggregate	B-API
function	O
on	O
columns	O
of	O
Pandas	O
pivot	O
table	O

I	O
generated	O
the	O
following	O
pivot	O
table	O
via	O
taking	O
maximum	O
of	O
values	O
in	O
`	O
Z	O
`	O
column	O
:	O
#CODE	O

Here's	O
a	O
fairly	O
general	O
solution	O
you	O
can	O
apply	O
to	O
multiple	O
columns	O
.	O

The	O
'	O
To	O
'	O
column	O
doesn't	O
need	O
to	O
be	O
rounded	O
,	O
I	O
just	O
included	O
it	O
for	O
the	O
generality	O
of	O
two	O
columns	O
rather	O
than	O
one	O
:	O
#CODE	O

428	O
base	O
,	O
mult	O
=	O
_gfc	O
(	O
freq	O
)	O

-->	O
429	O
return	O
tslib.dt64arr_to_periodarr	O
(	O
data.view	O
(	O
'	O
i8	O
')	O
,	O
base	O
,	O
tz	O
)	O

I	O
could	O
do	O
a	O
left	O
merge	B-API
,	O
but	O
I	O
would	O
end	O
up	O
with	O
a	O
huge	O
file	O
.	O

Is	O
there	O
any	O
way	O
to	O
add	O
specific	O
rows	O
from	O
df2	O
to	O
df1	O
using	O
merge	B-API
?	O

Unclear	O
why	O
you	O
think	O
a	O
left	O
merge	B-API
would	O
produce	O
a	O
huge	O
file	O
,	O
by	O
performing	O
a	O
left	O
merge	B-API
on	O
the	O
product	O
id	O
you	O
are	O
stating	O
that	O
you	O
are	O
only	O
interested	O
in	O
matches	O
in	O
the	O
product_id	O
column	O
only	O

Just	O
perform	O
a	O
left	O
`	O
merge	B-API
`	O
on	O
'	O
product_id	O
'	O
column	O
:	O
#CODE	O

What	O
would	O
be	O
the	O
Python	O
equivalent	O
?	O

I	O
cannot	O
think	O
of	O
a	O
way	O
to	O
translate	O
this	O
where	O
statement	O
into	O
pandas	O
syntax	O
.	O

The	O
only	O
way	O
I	O
can	O
think	O
of	O
is	O
to	O
add	O
an	O
arbitrary	O
field	O
to	O
people_usa	O
(	O
e.g.	O
`	O
people_usa	O
[	O
'	O
dummy	O
']	O
=1	O
`)	O
,	O
do	O
a	O
left	O
join	B-API
,	O
then	O
take	O
only	O
the	O
records	O
where	O
'	O
dummy	O
'	O
is	O
nan	O
,	O
then	O
delete	O
the	O
dummy	O
field	O
-	O
which	O
seems	O
a	O
bit	O
convoluted	O
.	O

Does	O
this	O
work	O
only	O
on	O
the	O
index	O
of	O
the	O
dataframe	O
?	O

I'd	O
like	O
the	O
option	O
to	O
specify	O
the	O
field	O
(	O
s	O
)	O
to	O
apply	O
this	O
to	O

Is	O
there	O
any	O
easy	O
way	O
to	O
do	O
this	O
if	O
you	O
have	O
multiple	O
columns	O
to	O
check	O
/	O
join	O
?	O

You	O
could	O
do	O
a	O
`	O
merge	B-API
`	O
and	O
then	O
eliminate	O
the	O
rows	O
that	O
exist	O
in	O
the	O
merged	O
df	O
otherwise	O
you'd	O
have	O
to	O
build	O
a	O
boolean	O
condition	O
for	O
all	O
the	O
columns	O
you	O
want	O
to	O
compare	O
but	O
presumably	O
when	O
checking	O
the	O
multiple	O
columns	O
you're	O
stating	O
that	O
it's	O
unique	O
for	O
those	O
columns	O
,	O
correct	O
?	O

For	O
instance	O
it's	O
not	O
a	O
match	O
if	O
say	O
col1	O
and	O
col2	O
match	O
but	O
col3	O
does	O
not	O

Yes	O
merge	B-API
is	O
what	O
I	O
have	O
been	O
doing	O
but	O
it	O
feels	O
like	O
a	O
hassle	O
.	O

I've	O
come	O
up	O
with	O
this	O
,	O
using	O
itertools	B-API
,	O
to	O
find	O
mid-day	O
timestamps	O
and	O
group	O
them	O
by	O
date	O
,	O
and	O
now	O
I'm	O
coming	O
up	O
short	O
trying	O
to	O
apply	O
imap	O
to	O
find	O
the	O
means	O
.	O

#CODE	O

Since	O
not	O
sure	O
what	O
your	O
end	O
output	O
should	O
look	O
like	O
,	O
just	O
create	O
a	O
time-based	O
grouper	B-API
manually	O
(	O
this	O
is	O
essentially	O
a	O
resample	B-API
)	O
,	O
but	O
doesn't	O
do	O
anything	O
with	O
the	O
final	O
results	O
(	O
its	O
just	O
a	O
list	O
of	O
the	O
aggregated	O
values	O
)	O
#CODE	O

You	O
can	O
get	O
reasonable	O
fancy	O
here	O
and	O
say	O
return	O
a	O
pandas	O
object	O
(	O
and	O
potentially	O
`	O
concat	O
`	O
them	O
)	O
.	O

and	O
I	O
want	O
to	O
pivot	O
it	O
like	O
this	O
:	O
#CODE	O

I	O
am	O
calling	O
a	O
function	O
from	O
within	O
a	O
'	O
for	O
each	O
loop	O
'	O
which	O
attempts	O
to	O
insert	O
values	O
into	O
a	O
Pandas	O
DataFrame	O
based	O
on	O
a	O
specified	O
column	O
start	O
and	O
end	O
location	O
.	O

The	O
function	O
is	O
this	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
despite	O
the	O
same	O
starting	O
conditions	O
when	O
I	O
call	O
this	O
function	O
it	O
seems	O
to	O
generate	O
a	O
list	O
of	O
inconsistent	O
length	O
.	O

e.g.	O
with	O
values	O
of	O
srowb	O
=	O
1	O
and	O
erowb	O
=	O
18	O
it	O
will	O
generate	O
a	O
list	O
(	O
tmp_brollb	O
)	O
which	O
has	O
either	O
len	B-API
(	O
tmp_brollb	O
)	O
=	O
17	O
or	O
len	B-API
(	O
tmp_brollb	O
)	O
=	O
18	O

Use	O
`	O
max	B-API
`	O
and	O
check	O
for	O
equality	O
using	O
`	O
eq	B-API
`	O
and	O
cast	O
the	O
boolean	O
df	O
to	O
int	O
using	O
`	O
astype	B-API
`	O
,	O
this	O
will	O
convert	O
`	O
True	O
`	O
and	O
`	O
False	O
`	O
to	O
`	O
1	O
`	O
and	O
`	O
0	O
`	O
:	O
#CODE	O

Thanks	O
@USER	O
.	O

Did	O
you	O
try	O
my	O
original	O
post	O
?	O

I	O
would	O
be	O
interested	O
to	O
know	O
how	O
much	O
time	O
this	O
one	O
is	O
taking	O
compared	O
to	O
yours	O
?	O

`	O
for	O
i	O
in	O
range	O
(	O
len	B-API
(	O
df	O
)):	O
...	O

df.loc	B-API
[	O
i	O
]	O
[	O
df.loc	B-API
[	O
i	O
]	O
.idxmax	B-API
(	O
axis=1	O
)]	O
=	O
1	O
...	O

df.loc	B-API
[	O
i	O
]	O
[	O
df.loc	B-API
[	O
i	O
]	O
!	O
=	O
1	O
]	O
=	O
0	O
`	O

I	O
am	O
trying	O
to	O
normalize	O
the	O
missing	O
values	O
in	O
matrix	O
.	O

Here	O
is	O
the	O
code	O
.	O

#CODE	O

Last	O
line	O
should	O
replace	O
the	O
values	O
in	O
dataset1	O
by	O
mean	O
values	O
from	O
`	O
ds2_mean	O
[	O
1	O
]`	O
.	O

But	O
it	O
does	O
not	O
do	O
.	O

Anything	O
wrong	O
here	O
?	O

And	O
after	O
that	O
can	O
I	O
replace	O
NaN	O
with	O
the	O
average	O
value	O
of	O
it's	O
neighbours	O
in	O
dataset1	O
?	O

it	O
does	O
wrong	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

It	O
should	O
replace	O
all	O
values	O
of	O
x	O
in	O
ds1	O
by	O
mapped	O
value	O
.	O

But	O
this	O
also	O
does	O
not	O
do	O
it	O

Sorry	O
can	O
you	O
explain	O
clearer	O
,	O
what	O
are	O
you	O
mapping	O
from	O
what	O
to	O
what	O
exactly	O
?	O

By	O
default	O
fillna	B-API
will	O
use	O
the	O
index	O
so	O
how	O
do	O
you	O
want	O
the	O
mapping	O
from	O
`	O
ds2	O
`	O
to	O
map	O
to	O
the	O
missing	O
values	O
in	O
`	O
ds1	O
`	O
?	O

Are	O
you	O
wanting	O
to	O
map	O
using	O
the	O
values	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
as	O
the	O
index	O
lookup	O
?	O

So	O
use	O
the	O
index	O
from	O
`	O
ds1	O
`	O
find	O
value	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
and	O
return	O
`	O
ds2	O
[	O
1	O
]`	O
?	O

yes	O
,	O
I	O
want	O
to	O
use	O
the	O
index	O
from	O
ds1	O
find	O
value	O
in	O
ds2	O
[	O
0	O
]	O
and	O
replace	O
it	O
with	O
ds2	O
[	O
1	O
]"	O
sorry	O
for	O
inconvenience	O

I	O
want	O
to	O
add	O
a	O
new	O
column	O
which	O
contains	O
values	O
based	O
on	O
df	O
[	O
'	O
diff	O
']	O

When	O
using	O
`	O
DataFrame.apply	B-API
`	O
if	O
you	O
use	O
`	O
axis=0	O
`	O
it	O
applies	O
the	O
condition	O
through	O
columns	O
,	O
to	O
use	O
`	O
apply	B-API
`	O
to	O
go	O
through	O
each	O
row	O
,	O
you	O
need	O
`	O
axis=1	O
`	O
.	O

But	O
given	O
that	O
,	O
you	O
can	O
use	O
`	O
Series.apply	B-API
`	O
instead	O
of	O
`	O
DataFrame.apply	B-API
`	O
on	O
the	O
`'	O
diff	B-API
'`	O
series	O
.	O

Example	O
-	O
#CODE	O

You	O
can	O
just	O
set	O
all	O
the	O
values	O
that	O
meet	O
your	O
criteria	O
rather	O
than	O
looping	O
over	O
the	O
df	O
by	O
calling	O
`	O
apply	B-API
`	O
so	O
the	O
following	O
should	O
work	O
and	O
as	O
it's	O
vectorised	O
will	O
scale	O
better	O
for	O
larger	O
datasets	O
:	O
#CODE	O

this	O
will	O
set	O
all	O
rows	O
that	O
meet	O
the	O
criteria	O
,	O
the	O
problem	O
using	O
`	O
apply	B-API
`	O
is	O
that	O
it's	O
just	O
syntactic	O
sugar	O
for	O
a	O
`	O
for	O
`	O
loop	O
and	O
where	O
possible	O
this	O
should	O
be	O
avoided	O
where	O
a	O
vectorised	O
solution	O
exists	O
.	O

Then	O
you	O
can	O
`	O
stack	B-API
`	O
(	O
first	O
by	O
`'	O
Marker	O
'`	O
then	O
by	O
`'	O
mrk	O
'`)	O
:	O
#CODE	O

Python	O
DataFrame	O
-	O
apply	O
different	O
calculations	O
due	O
to	O
a	O
column's	O
value	O

You	O
could	O
do	O
this	O
using	O
2	O
`	O
loc	B-API
`	O
calls	O
:	O
#CODE	O

There	O
are	O
two	O
reasons	O
whiskers	O
length	O
vary	O
from	O
one	O
boxplot	O
to	O
any	O
other	O
boxplot	O

Are	O
you	O
asking	O
why	O
the	O
top	O
whisker	O
isn't	O
the	O
same	O
length	O
as	O
the	O
bottom	O
?	O

I	O
think	O
the	O
whiskers	O
are	O
actually	O
the	O
lowest	O
or	O
highest	O
data	O
point	O
within	O
1.5	O
IQR	O
.	O

So	O
if	O
there	O
are	O
no	O
data	O
points	O
between	O
Q3	O
and	O
Q3	O
+	O
1.5	O
IQR	O
,	O
then	O
the	O
top	O
whisker	O
won't	O
show	O
up	O
.	O

For	O
the	O
one	O
boxplot	O
where	O
the	O
are	O
outliers	O
beyond	O
the	O
whiskers	O
on	O
both	O
the	O
top	O
and	O
the	O
bottom	O
,	O
the	O
whiskers	O
do	O
look	O
about	O
the	O
same	O
size	O
.	O

``	O
hist	B-API
``	O
->	O
``	O
histogram	O
``	O
(	O
``	O
hist	B-API
``	O
is	O
pyplot	B-API
or	O
something	O
)	O
.	O

There	O
is	O
a	O
pandas	O
equivalent	O
to	O
this	O
`	O
cut	B-API
`	O
there	O
is	O
a	O
section	O
describing	O
this	O
here	O
.	O

`	O
cut	B-API
`	O
returns	O
the	O
open	O
closed	O
intervals	O
for	O
each	O
value	O
:	O
#CODE	O

Pandas	O
Dataframe	O
,	O
Apply	B-API
Function	O
,	O
Return	O
Index	O

Then	O
I	O
can	O
apply	O
the	O
function	O
to	O
my	O
dataframe	O
,	O
grouped	O
by	O
I	O
D:	O
#CODE	O

If	O
I	O
resample	O
this	O
DataField	O
by	O
any	O
frequency	O
,	O
the	O
timezone	O
is	O
kept	O
:	O
#CODE	O

their	O
are	O
a	O
couple	O
of	O
outstanding	O
bugs	O
w.r.t	O
to	O
resample	O
and	O
extra	O
binning	O
:	O
#URL	O
if	O
you	O
would	O
like	O
to	O
investigate	O
and	O
try	O
to	O
pinpoint	O
(	O
or	O
better	O
yet	O
fix	O
)	O
would	O
be	O
appreciated	O
!	O

you	O
can	O
comment	O
on	O
that	O
issue	O
directly	O

@USER	O
;	O
You	O
mean	O
to	O
the	O
stack	O
exchange	O
answer	O
?	O

I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

After	O
each	O
operation	O
,	O
drop	O
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	B-API
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	B-API
`	O

I	O
have	O
tried	O
a	O
some	O
join	B-API
/	O
merge	B-API
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	B-API
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	B-API
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	O
with	O
outer	O
product	O

How	O
to	O
join	O
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	B-API
`	O
needs	O
to	O
return	O
a	O
`	O
Series	B-API
`	O
,	O
not	O
a	O
`	O
DataFrame	O
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	O
`	O
into	O
a	O
`	O
Series	B-API
`	O
is	O
to	O
use	O
`	O
stack	B-API
`	O
.	O

`	O
stack	B-API
`	O
this	O
DataFrame	O
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	B-API
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	B-API
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	B-API
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

`	O
apply	B-API
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	B-API
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	B-API
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	B-API
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	B-API
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	B-API
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	O
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	B-API
method	O
?	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	B-API
`	O
or	O
`	O
iloc	B-API
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	O
string	O
to	O
DateOffset	B-API
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	B-API
"	O
when	O
speaking	O
of	O
freq	O
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	O
/	O
unstack	O
/	O
pivot	O
dataframe	O
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	B-API
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	B-API
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	O
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	O
each	O
chunk	O
into	O
HDFStore	B-API
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

If	O
you	O
replace	O
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	O
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	B-API
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	B-API
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	B-API
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	O
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	O
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	O
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	O
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	O
insert	O
rows	O

I	O
want	O
to	O
insert	O
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	O
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	O
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	O
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

After	O
reading	O
one	O
line	O
I	O
append	O
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	B-API
/	O
merge	B-API
/	O
join	B-API
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	B-API
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	O
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

You	O
then	O
want	O
to	O
apply	O
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	O
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	B-API
`	O
.	O

`	O
loop_iter	O
=	O
len	B-API
(	O
A	O
)	O
/	O
max	B-API
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	O
transformation	O
from	O
`	O
pandas.get_dummies	B-API
`	O
to	O
new	O
data	O
?	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	B-API
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	B-API
.	O

Call	O
`	O
transform	B-API
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	B-API
`	O
,	O
transform	B-API
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	O
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	B-API
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	B-API
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	O
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	O
maybe	O
.	O

the	O
documentation	O
to	O
concat	B-API
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	B-API
`	O
on	O
the	O
`	O
groupby	B-API
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	B-API
`	O
:	O
#CODE	O

pandas	O
groupby	B-API
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	O
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	B-API
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	B-API
`	O
instead	O
of	O
`	O
iloc	B-API
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	O
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	O
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	O
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	B-API
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	O
files	O
.	O

Use	O
`	O
shift	B-API
`	O
and	O
`	O
np.log	B-API
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	O
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	O
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	O
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	B-API
method	O
.	O

I	O
tried	O
to	O
map	O
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	O
it	O
to	O
a	O
correct	O
row	O
.	O

Do	O
you	O
know	O
if	O
append	B-API
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	O
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	O
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	B-API
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	B-API
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	O
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	B-API
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	B-API
so	O
much	O
slower	O
than	O
put	O
?	O

pandas	O
merge	B-API
with	O
MultiIndex	B-API
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	O
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	O
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	B-API
levels	O
.	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	B-API
?	O

loc	B-API
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	O
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	O
?	O

it'll	O
be	O
difficult	O
to	O
translate	O
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	B-API
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	O
to	O
python	O

If	O
you	O
drop	O
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

Append	O
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	O
/	O
join	O
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	O
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	B-API
and	O
join	B-API
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	B-API
or	O
reset	B-API
the	O
index	O
so	O
they	O
align	O

Use	O
`	O
concat	B-API
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	B-API
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	B-API
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	O
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	O
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	O
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	B-API
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

And	O
you	O
could	O
always	O
drop	O
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	B-API
(	O
0	O
,	O
len	B-API
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	B-API
(	O
int	O
)`	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	B-API
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	O
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	B-API
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	B-API
I	O
think	O

After	O
they	O
are	O
done	O
,	O
merge	O
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	O
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	B-API
`	O
on	O
`	O
merged_id	O
`	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	O
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	O
the	O
dataframe	O
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	O
trace	O
?	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	O
table	O

With	O
pivot	O
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	O
question	O

For	O
each	O
group	O
in	O
the	O
groupby	B-API
object	O
,	O
we	O
will	O
want	O
to	O
apply	O
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	O
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	B-API
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
join	O
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	O
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	O
on	O
.	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	O
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	O
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	O
of	O
about	O
10378	O
rows	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	B-API
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	B-API
`	O
function	O
,	O
as	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	B-API
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	B-API
(	O
len	B-API
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	O
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	O
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	B-API
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	O
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	B-API
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	B-API
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	B-API
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	O
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	O
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	O
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	B-API
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	B-API
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	B-API
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	B-API
as	O
desired	O

I	O
can	O
strip	O
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	O
two	O
DataFrame	O
columns	O
and	O
apply	O
pandas.to_datetime	B-API
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	O
two	O
columns	O
,	O
and	O
apply	O
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	O
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	B-API
`	O
and	O
`	O
diff	B-API
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	B-API
`	O
is	O
used	O
to	O
apply	B-API
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	B-API
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	B-API
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	B-API
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	B-API
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	O
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	B-API
in	O
the	O
dataframe	O
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	O
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	B-API
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	B-API
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	B-API
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	B-API
to	O
do	O
that	O
with	O
`	O
polyfit	B-API
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	B-API
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	B-API
function	O
.	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	B-API
does	O
?	O

No	O
,	O
apply	B-API
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	B-API
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

*	O
apply	B-API
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	O
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	O
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	B-API
`	O
methods	O
to	O
replace	O
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	B-API
`	O
creates	O
a	O
new	O
dataframe	O
,	O
`	O
ix	B-API
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	B-API
`	O
and	O
`	O
iloc	B-API
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	B-API
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	B-API
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	B-API
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	B-API
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	O
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	B-API
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	O
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	O
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O

I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

After	O
each	O
operation	O
,	O
drop	O
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	B-API
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	B-API
`	O

I	O
have	O
tried	O
a	O
some	O
join	B-API
/	O
merge	B-API
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	B-API
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	B-API
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	O
with	O
outer	O
product	O

How	O
to	O
join	O
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	B-API
`	O
needs	O
to	O
return	O
a	O
`	O
Series	B-API
`	O
,	O
not	O
a	O
`	O
DataFrame	O
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	O
`	O
into	O
a	O
`	O
Series	B-API
`	O
is	O
to	O
use	O
`	O
stack	B-API
`	O
.	O

`	O
stack	B-API
`	O
this	O
DataFrame	O
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	B-API
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	B-API
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	B-API
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

`	O
apply	B-API
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	B-API
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	B-API
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	B-API
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	B-API
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	B-API
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	O
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	B-API
method	O
?	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	B-API
`	O
or	O
`	O
iloc	B-API
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	O
string	O
to	O
DateOffset	B-API
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	B-API
"	O
when	O
speaking	O
of	O
freq	O
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	O
/	O
unstack	O
/	O
pivot	O
dataframe	O
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	B-API
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	B-API
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	O
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	O
each	O
chunk	O
into	O
HDFStore	B-API
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

If	O
you	O
replace	O
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	O
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	B-API
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	B-API
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	B-API
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	O
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	O
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	O
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	O
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	O
insert	O
rows	O

I	O
want	O
to	O
insert	O
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	O
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	O
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	O
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

After	O
reading	O
one	O
line	O
I	O
append	O
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	B-API
/	O
merge	B-API
/	O
join	B-API
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	B-API
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	O
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

You	O
then	O
want	O
to	O
apply	O
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	O
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	B-API
`	O
.	O

`	O
loop_iter	O
=	O
len	B-API
(	O
A	O
)	O
/	O
max	B-API
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	O
transformation	O
from	O
`	O
pandas.get_dummies	B-API
`	O
to	O
new	O
data	O
?	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	B-API
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	B-API
.	O

Call	O
`	O
transform	B-API
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	B-API
`	O
,	O
transform	B-API
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	O
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	B-API
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	B-API
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	O
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	O
maybe	O
.	O

the	O
documentation	O
to	O
concat	B-API
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	B-API
`	O
on	O
the	O
`	O
groupby	B-API
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	B-API
`	O
:	O
#CODE	O

pandas	O
groupby	B-API
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	O
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	B-API
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	B-API
`	O
instead	O
of	O
`	O
iloc	B-API
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	O
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	O
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	O
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	B-API
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	O
files	O
.	O

Use	O
`	O
shift	B-API
`	O
and	O
`	O
np.log	B-API
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	O
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	O
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	O
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	B-API
method	O
.	O

I	O
tried	O
to	O
map	O
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	O
it	O
to	O
a	O
correct	O
row	O
.	O

Do	O
you	O
know	O
if	O
append	B-API
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	O
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	O
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	B-API
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	B-API
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	O
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	B-API
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	B-API
so	O
much	O
slower	O
than	O
put	O
?	O

pandas	O
merge	B-API
with	O
MultiIndex	B-API
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	O
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	O
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	B-API
levels	O
.	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	B-API
?	O

loc	B-API
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	O
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	O
?	O

it'll	O
be	O
difficult	O
to	O
translate	O
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	B-API
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	O
to	O
python	O

If	O
you	O
drop	O
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

Append	O
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	O
/	O
join	O
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	O
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	B-API
and	O
join	B-API
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	B-API
or	O
reset	B-API
the	O
index	O
so	O
they	O
align	O

Use	O
`	O
concat	B-API
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	B-API
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	B-API
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	O
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	O
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	O
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	B-API
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

And	O
you	O
could	O
always	O
drop	O
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	B-API
(	O
0	O
,	O
len	B-API
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	B-API
(	O
int	O
)`	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	B-API
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	O
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	B-API
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	B-API
I	O
think	O

After	O
they	O
are	O
done	O
,	O
merge	O
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	O
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	B-API
`	O
on	O
`	O
merged_id	O
`	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	O
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	O
the	O
dataframe	O
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	O
trace	O
?	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	O
table	O

With	O
pivot	O
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	O
question	O

For	O
each	O
group	O
in	O
the	O
groupby	B-API
object	O
,	O
we	O
will	O
want	O
to	O
apply	O
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	O
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	B-API
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
join	O
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	O
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	O
on	O
.	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	O
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	O
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	O
of	O
about	O
10378	O
rows	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	B-API
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	B-API
`	O
function	O
,	O
as	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	B-API
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	B-API
(	O
len	B-API
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	O
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	O
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	B-API
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	O
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	B-API
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	B-API
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	B-API
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	O
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	O
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	O
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	B-API
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	B-API
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	B-API
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	B-API
as	O
desired	O

I	O
can	O
strip	O
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	O
two	O
DataFrame	O
columns	O
and	O
apply	O
pandas.to_datetime	B-API
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	O
two	O
columns	O
,	O
and	O
apply	O
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	O
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	B-API
`	O
and	O
`	O
diff	B-API
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	B-API
`	O
is	O
used	O
to	O
apply	B-API
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	B-API
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	B-API
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	B-API
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	B-API
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	O
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	B-API
in	O
the	O
dataframe	O
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	O
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	B-API
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	B-API
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	B-API
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	B-API
to	O
do	O
that	O
with	O
`	O
polyfit	B-API
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	B-API
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	B-API
function	O
.	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	B-API
does	O
?	O

No	O
,	O
apply	B-API
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	B-API
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

*	O
apply	B-API
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	O
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	O
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	B-API
`	O
methods	O
to	O
replace	O
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	B-API
`	O
creates	O
a	O
new	O
dataframe	O
,	O
`	O
ix	B-API
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	B-API
`	O
and	O
`	O
iloc	B-API
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	B-API
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	B-API
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	B-API
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	B-API
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	O
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	B-API
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	O
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	O
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O

i	O
am	O
facing	O
this	O
strange	O
behaviour	O
,	O
i	O
got	O
a	O
HDFStore	B-API
containing	O
DataFrames	O
.	O

What	O
happens	O
if	O
you	O
do	O
`	O
ax.legend()	B-API
`	O
and	O
`	O
plt.draw()	B-API
`	O
?	O

E.g.	O
,	O
you	O
can	O
call	O
`	O
pandas.isnull	B-API
(	O
the_frame	O
)`	O
.	O

do	O
this	O
:	O
``	O
df.ix	B-API
[	O
0	O
,	O
'	O
a	O
']	O
+=	O
1	O
``	O
.	O

Using	O
unstack()	B-API
is	O
one	O
way	O
.	O

Clearly	O
I'm	O
missing	O
something	O
as	O
to	O
why	O
df.loc	B-API
[:	O
,	O
tuple	O
]	O
is	O
different	O
than	O
df	O
[	O
tuple	O
]	O
.	O

datetime	O
dtypes	B-API
in	O
pandas	O
read_csv	B-API

If	O
you're	O
using	O
version	O
`	O
0.17.0	O
`	O
or	O
higher	O
then	O
you	O
can	O
call	O
this	O
using	O
`	O
.dt	B-API
.strftime	B-API
`	O
which	O
is	O
vectorised	O
:	O
#CODE	O

Looks	O
like	O
you	O
need	O
histogram()	B-API
of	O
months	O
.	O

You	O
want	O
`	O
.dt	B-API
.time	B-API
`	O
see	O
the	O
docs	O
for	O
some	O
more	O
examples	O
of	O
things	O
under	O
the	O
`	O
.dt	B-API
`	O
accessor	O
.	O

pd.rolling_apply	B-API
(	O
df.exma	O
,	O
2	O
,	O
(	O
df.alpha	O
*	O
df.exma.shift	O
(	O
1	O
))	O
+	O
((	O
1	O
-	O
df.alpha	O
)	O
*	O
df.outperf	O
))	O

I	O
suggest	O
not	O
using	O
`	O
file	O
`	O
for	O
your	O
`	O
open()	B-API
`	O
object	O
.	O

If	O
you	O
want	O
every	O
value	O
from	O
row	O
2	O
you	O
can	O
just	O
do	O
`	O
df.iloc	B-API
[	O
2	O
]`	O
or	O
`	O
df.iloc	B-API
[	O
2	O
]	O
.values	B-API
`	O
respectively	O
.	O

dtype	B-API
:	O
object	O

Check	O
out	O
the	O
glob	O
module	O
and	O
pandas	O
read_csv()	B-API
and	O
concat()	B-API

`	O
np.dtype	B-API
(	O
'	O
datetime64	O
[	O
ns	O
]')`	O
and	O
`	O
np.dtype	B-API
(	O
'	O
M	O
8[	O
ns	O
]')`	O
:	O
#CODE	O

Doing	O
`	O
pd.crosstab	B-API
(	O
rows	O
=[	O
df	O
[	O
'	O
A	O
']	O
,	O
df	O
[	O
'	O
B	O
']]	O
,	O
cols	O
=[	O
df	O
[	O
'	O
C	O
']]	O
,	O
margins=True	O
,	O
aggfu	O

DataFrame.drop_duplicates	B-API
and	O
DataFrame.drop	B-API
not	O
removing	O
rows	O

`	O
pd.rolling_mean	B-API
(	O
df	O
,	O
num	O
)	O
.dropna()	B-API
.plot()	B-API
`	O

This	O
is	O
a	O
replacement	O
for	O
`	O
np.array_equal	B-API
`	O
which	O
is	O
broken	O
for	O
nan	O
positional	O
detections	O
(	O
and	O
object	O
dtypes	B-API
)	O
.	O

It	O
looks	O
like	O
`	O
pd.unique	O
`	O
does	O
not	O
respect	O
the	O
`	O
datetime64	O
`	O
dtype	B-API
:	O
#CODE	O

df	O
[	O
'	O
NumActivity	O
']	O
=	O
pd.factorize	B-API
(	O
df	O
[	O
'	O
Activity	O
'])	O
[	O
0	O
]	O
+1	O
works	O
.	O

calling	O
`	O
as_matrix()	B-API
`	O
on	O
`	O
df	O
`	O
returns	O
a	O
`	O
numpy.ndarray	B-API
`	O
object	O
#CODE	O

And	O
using	O
pandas.Series.map	B-API
:	O
#CODE	O

works	O
,	O
but	O
I	O
think	O
`	O
df.select_dtypes	B-API
`	O
should	O
be	O
preferred	O
since	O
it	O
uses	O
the	O

Then	O
you	O
can	O
access	O
the	O
relevant	O
rows	O
using	O
groupby's	O
`	O
get_group	B-API
`	O
:	O
#CODE	O

It's	O
not	O
in	O
the	O
online	O
docs	O
,	O
you	O
have	O
to	O
check	O
the	O
`	O
help	O
(	O
pd.bdate_range	B-API
)`	O
:)	O

dtype	B-API
:	O
int64	O
`	O

dtype	B-API
:	O
object	O

df.drop	B-API
([	O
'	O
one	O
'	O
,	O
'	O
two	O
'	O
,	O
'	O
three	O
']	O
,	O
axis=1	O
,	O
inplace=True	O
)	O

I	O
tried	O
make	O
these	O
'	O
filenames	O
'	O
dataFrames	O
using	O
`	O
pd.DataFrame	B-API
`	O
but	O
wasn't	O
able	O
to	O
do	O
so	O
.	O

Try	O
using	O
.loc	B-API
[	O
row_index	O
,	O
col_indexer	O
]	O
=	O
value	O
instead	O
`	O
.	O

Good	O
question	O
,	O
`	O
plt.hist	B-API
(	O
hour_list	O
,	O
bins=	O
np.arange	B-API
(	O
24	O
)	O
-	O
0.5	O
)`	O
will	O
work	O
.	O

what	O
is	O
the	O
-	O
0.00343	O
(	O
ah	O
I	O
see	O
,	O
a	O
consequence	O
of	O
not	O
using	O
right=False	O
)	O
,	O
pd.cut	B-API
very	O
neat	O

2	O
)	O
Alternatively	O
,	O
don't	O
create	O
levels	O
for	O
`	O
aa	O
`	O
and	O
`	O
bb	O
`	O
using	O
`	O
as_index=False	O
`	O
and	O
`	O
pd.merge	B-API
`	O
#CODE	O

`	O
df.groupby	B-API
(	O
level	O
=[	O
'	O
major	O
'	O
,	O
'	O
minor	O
'])	O
.mean()	B-API
.dropna()	B-API
`	O

Argh	O
,	O
that	O
still	O
gives	O
me	O
`	O
dtype	B-API
(	O
'	O
datetime64	O
[	O
us	O
]')`	O
.	O

or	O
better	O
yet	O
,	O
just	O
don't	O
specify	O
a	O
dtype	B-API
:	O
#CODE	O

I	O
was	O
trying	O
to	O
do	O
this	O
with	O
`	O
df	O
[	O
df.shift()	B-API
!	O
=	O
df	O
]	O
.dropna()	B-API
.reset_index	B-API
(	O
drop=True	O
)`	O
but	O
`	O
shift()	B-API
`	O
is	O
not	O
behaving	O
in	O
the	O
way	O
I	O
meant	O
.	O

`	O
median_search_query	O
=	O
np.median	B-API
(	O
df.srch_query_affinity_score	O
)`	O

Example	O
using	O
df.asfreq	B-API
(	O
'	O
Q	O
')	O
:	O
#CODE	O

```	O
In	O
[	O
15	O
]:	O
pd.to_timedelta	B-API
(	O
s.str.replace	O
(	O
'	O
hrs	O
'	O
,	O
'	O
h	O
'))	O

Python	O
Dict	O
,	O
Lambda	O
x	O
,	O
map()	B-API
doesn't	O
work	O

I	O
have	O
tried	O
to	O
modify	O
your	O
answer	O
to	O
make	O
it	O
an	O
example	O
about	O
modifying	O
two	O
columns	O
like	O
I	O
asked	O
(	O
for	O
one	O
the	O
`	O
df.loc	B-API
[:	O
,	O
(	O
'	O
A	O
'	O
,	O
'	O
a	O
')]`	O
works	O
like	O
a	O
charm	O
)	O
.	O

I	O
thought	O
np.ma.average	B-API
is	O
just	O
what	O
I	O
need	O
,	O
but	O
that	O
also	O
gives	O
me	O
NaN	O
as	O
a	O
result	O
.	O

Although	O
```	O
pandas.cut()	B-API
```	O
is	O
the	O
better	O
and	O
more	O
general	O
answer	O
,	O
it	O
looks	O
like	O
in	O
this	O
case	O
you	O
could	O
do	O
```	O
df	O
[	O
'	O
B	O
']	O
=	O
(	O
df.A	O
/	O
500	O
)	O
.astype	B-API
(	O
int	O
)	O
+	O
1	O
```	O
.	O

outcome	O
:	O
displays	O
good	O
in	O
`	O
df.head()	B-API
`	O
,	O
but	O
reverts	O
to	O
scientific	O
notation	O
upon	O
coercion	O
to	O
string	O
concatenation	O
using	O
+	O
operator	O

The	O
`	O
ewma	B-API
`	O
case	O
can	O
be	O
solved	O
similarly	O
:	O
`	O
df.groupby	B-API
(	O
level=0	O
)	O
.apply	B-API
(	O
lambda	O
x	O
:	O
pd.ewma	B-API
(	O
x	O
,	O
com=2	O
))`	O

I	O
didn't	O
realise	O
that	O
`	O
.resample()	B-API
`	O
wasn't	O
inplace	O
!	O

I	O
can't	O
think	O
of	O
a	O
way	O
to	O
overload	O
the	O
`	O
.loc	B-API
`	O
method	O
properly	O
!	O

Do	O
I	O
have	O
to	O
specific	O
the	O
dtypes	B-API
to	O
make	O
this	O
work	O
?	O

It	O
appears	O
that	O
the	O
`	O
applymap	B-API
`	O
is	O
the	O
culprit	O
here	O
:-)	O

"	O
"	O
.join	B-API
(	O
header.split()	O
)	O
.split	B-API
(	O
'	O
')	O

I	O
want	O
df.append	B-API
(	O
df2	O
)	O
in	O
this	O
case	O
to	O
be	O
:	O
#CODE	O

I	O
completely	O
forgot	O
about	O
.loc	B-API

It	O
sounds	O
like	O
maybe	O
you	O
want	O
`	O
pandas.concat	B-API
`	O
?	O

Is	O
there	O
anything	O
in	O
pandas	O
that	O
is	O
the	O
opposite	O
to	O
`	O
.dropna()	B-API
`	O
?	O

Datetimes	O
are	O
handled	O
if	O
they	O
can	O
properly	O
be	O
converted	O
(	O
e.g.	O
they	O
have	O
a	O
dtype	B-API
of	O
'	O
datetime64	O
[	O
ns	O
]'	O
,	O
notably	O
datetimes.date	O
are	O
NOT	O
handled	O
(	O
NaN	O
are	O
a	O
different	O
story	O
and	O
depending	O
on	O
usage	O
can	O
cause	O
the	O
entire	O
column	O
type	O
to	O
be	O
mishandled	O
)	O

The	O
`	O
args	O
`	O
and	O
`	O
kwargs	O
`	O
parameters	O
were	O
added	O
to	O
`	O
rolling_apply	B-API
`	O
in	O
Pandas	O
version	O
0.14.0	O
.	O

try	O
``	O
df	O
[	O
'	O
LastName	O
']	O
=	O
df.apply	B-API
(	O
updateDataframe	O
)``	O
,	O
but	O
even	O
better	O
is	O
:	O
``	O
df.ix	B-API
[	O
df	O
[	O
'	O
LastName	O
']	O
==	O
'	O
Joe	O
'	O
,	O
'	O
LastName	O
']	O
=	O
'	O
Black	O
'``	O

There	O
should	O
be	O
no	O
problem	O
;	O
`	O
plt.plot	B-API
`	O
and	O
`	O
plt.fill_between	B-API
`	O
accept	O
arrays	O
of	O
dtype	B-API
`	O
datetime64	O
[	O
ns	O
]`	O
just	O
fine	O
.	O

You	O
could	O
try	O
pandas	O
pivot()	B-API
.	O

What	O
happens	O
if	O
you	O
do	O
`	O
ax.legend()	B-API
`	O
and	O
`	O
plt.draw()	B-API
`	O
?	O

Pandas	O
Python	O
read_csv	B-API
error_bad_lines	O
producing	O
shell	O
feedback	O

Though	O
in	O
some	O
respects	O
,	O
the	O
np.clip	B-API
or	O
np.max	O
solutions	O
are	O
more	O
easily	O
read	O
,	O
I	O
think	O
this	O
is	O
the	O
most	O
precise	O
answer	O
to	O
my	O
original	O
question	O
.	O

Did	O
you	O
try	O
setting	O
the	O
delimiter	O
to	O
semicolon	O
in	O
your	O
`	O
read_csv	B-API
`	O
call	O
?	O

In	O
[	O
8]	O
:	O
%timeit	O
df	O
[	O
'	O
r	O
']	O
=	O
df	O
[[	O
'	O
minor	O
'	O
,	O
'	O
major	O
']]	O
.abs()	B-API
.max	B-API
(	O
axis=1	O
)	O

I	O
guess	O
my	O
confusion	O
stems	O
from	O
the	O
fact	O
that	O
the	O
Series.value_counts	B-API
doesn't	O
seem	O
to	O
fit	O
into	O
the	O
arguments	O
required	O
by	O
the	O
df.apply	B-API
method	O
.	O

The	O
problem	O
is	O
that	O
`	O
a	O
`	O
is	O
dtype	B-API
`	O
object	O
`	O
.	O

try	O
`	O
video_base	O
=	O
pd.merge	B-API
(	O
df_one	O
,	O
df_two	O
[[	O
'	O
count_watched_yeterday	O
']]	O
,	O
how=	O
'	O
left	O
'	O
,	O
left_index=True	O
,	O
right_index=True	O
)`	O

ax.xaxis.set_major_locator	O
(	O
MultipleLocator	B-API
(	O
10	O
))	O

Actually	O
I	O
think	O
you	O
want	O
:	O
`	O
pd.concat	B-API
([	O
df_may	O
,	O
df_jun	O
]	O
,	O
axis=0	O
,	O
ignore_index=True	O
)`	O

Maybe	O
there	O
is	O
a	O
better	O
approach	O
thats	O
takes	O
advantage	O
of	O
features	O
of	O
the	O
Pandas.DataFrame	B-API
class	O
?	O

Try	O
`	O
df.loc	B-API
[	O
'	O
a	O
']`	O
instead	O
.	O

Then	O
this	O
should	O
work	O
:	O
`	O
df	O
[	O
'	O
Gene.Symbol	O
']	O
=	O
df	O
[	O
'	O
Gene.Symbol	O
']	O
.str	B-API
.strip()	B-API
.str	B-API
.upper()	B-API
`	O

then	O
[	O
`	O
df.groupy	O
(	O
'	O
key	O
')	O
.agg	B-API
(	O
...	O
)`]	O
(	O
#URL	O
)	O
might	O
be	O
what	O
you	O
are	O
looking	O
for	O
.	O

DataFrame.apply	B-API
in	O
python	O
pandas	O
alters	O
both	O
original	O
and	O
duplicate	O
DataFrames	O

you	O
might	O
want	O
to	O
try	O
`	O
df.iloc	B-API
[	O
0	O
]`	O
rather	O
than	O
`	O
df.iloc	B-API
(	O
0	O
)`	O
.	O

and	O
,	O
`	O
ts.asfreq	O
(	O
'	O
H	O
'	O
,	O
method=	O
'	O
ffill	B-API
')`	O
to	O
have	O
hourly	O
frequency	O
.	O

I	O
tried	O
something	O
like	O
`	O
set_index	B-API
`	O
,	O
`	O
pd.factorize()	B-API
`	O
and	O
`	O
index_col	O
`	O
but	O
they	O
do	O
not	O
work	O
.	O

So	O
df	O
=	O
df.reindex()	B-API
results	O
in	O
the	O
same	O
indexing	O
...	O

I	O
think	O
this	O
solution	O
will	O
execute	O
faster	O
than	O
using	O
iterrows()	B-API
,	O
but	O
I'm	O
not	O
sure	O
.	O

One	O
option	O
using	O
`	O
df.reindex	B-API
`	O
:	O
#CODE	O

I	O
am	O
using	O
dtype	B-API
as	O
suggested	O
in	O
the	O
answer	O
there	O
.	O

You	O
might	O
be	O
interested	O
in	O
`	O
pd.cut	B-API
`	O
:	O
#CODE	O

df.groupby	B-API
(	O
'	O
A	O
')	O
.size()	B-API
.apply	B-API
(	O
lambda	O
x	O
:	O
float	O
(	O
x	O
)	O
/	O
df.groupby	B-API
(	O
'	O
A	O
')	O
.size()	B-API
.sum()	B-API
*100	O
)	O

`	O
df.apply()	B-API
`	O
works	O
off	O
a	O
single	O
row	O
,	O
`	O
shift()	B-API
`	O
doesnt	O
seem	O
to	O
work	O
.	O

Could	O
you	O
try	O
this	O
:	O
`	O
df	O
[	O
'	O
GC	O
content	O
']	O
=	O
df	O
[[	O
'	O
oligo_sequence	O
']]	O
.apply	B-API
(	O
lambda	O
row	O
:	O
GC	O
(	O
row	O
)	O
,	O
axis=1	O
)`	O

And	O
,	O
`	O
pd.eval()	B-API
`	O
works	O
well	O
with	O
expressions	O
containing	O
large	O
arrays	O
#CODE	O

The	O
eventual	O
goal	O
being	O
to	O
arrange	O
hierarchically	O
to	O
weekday	B-API
hour-range	O
,	O
something	O
like	O
:	O
#CODE	O

Reading	O
about	O
`	O
applymap	B-API
`	O
I	O
wondered	O
if	O
there	O
is	O
a	O
similar	O
way	O
of	O
defining	O
and	O
applying	O
operators	O
that	O
work	O
on	O
pairs	O
of	O
dataframes	O
.	O

Executing	O
your	O
code	O
with	O
pandas	O
0.16.2	O
yielded	O
the	O
two	O
columns	O
with	O
dtype	B-API
datetime64	O
[	O
ns	O
]	O
.	O

`	O
df	O
[	O
'	O
price_trend	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:[	O
i.split	O
(	O
'	O
:	O
')	O
for	O
i	O
in	O
x	O
])`	O
#CODE	O

Here	O
the	O
new	O
value	O
AND	O
the	O
existing	O
dtype	B-API
of	O
the	O
column	O
matters	O
.	O

how	O
about	O
using	O
the	O
`	O
pd.DataFrame.drop_duplicates()	B-API
`	O
method	O
?	O

dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]```	O

`	O
gg	O
[	O
'	O
cumt	O
']	O
=	O
gg.apply	O
(	O
lambda	O
x	O
:	O
x	O
[	O
'	O
tavg	O
']	O
+	O
x	O
[	O
'	O
tavg	O
']	O
.shift	B-API
(	O
1	O
)	O
[	O
1	O
:]	O
)`	O

"	O
"	O
.join	B-API
(	O
header.split()	O
)	O
.split	B-API
(	O
'	O
')	O

print	O
'	O
\nAfter	O
replace\n	O
'	O
,	O
df.replace	B-API
(	O
{	O
'	O
c1	O
'	O
:	O
c1_fromto	O
,	O
'	O
c2	O
'	O
:	O
c2_fromto	O
}	O
)	O

Profiling	O
shows	O
the	O
culprit	O
is	O
obviously	O
`	O
B.ix	O
[	O
row	O
[	O
0	O
]]	O
.irow	B-API
(	O
np.searchsorted	B-API
(	O
B.ts	O
[	O
row	O
[	O
0	O
]]	O
,	O
row	O
[	O
2	O
])))`	O
.	O

My	O
question	O
concerns	O
iterating	O
through	O
the	O
rows	O
of	O
a	O
data	O
frame	O
and	O
on	O
each	O
row	O
setting	O
a	O
field	O
based	O
on	O
information	O
in	O
a	O
different	O
data	O
frame	O
.	O

EDIT	O
:	O
Adding	O
logic	O
to	O
default	O
empty	O
strings	O
to	O
`	O
0	O
`	O
,	O
use	O
a	O
different	O
value	O
if	O
you	O
want	O
to	O
handle	O
empty	O
strings	O
in	O
`	O
years	O
`	O
colomn	O
differently	O
#CODE	O

I	O
would	O
suggest	O
that	O
you	O
use	O
2-dimensional	O
numpy	O
array	O
.	O

I	O
renamed	O
them	O
to	O
aa	O
,	O
ab	O
and	O
ac	O
but	O
still	O
get	O
the	O
same	O
error	O
.	O

In	O
this	O
last	O
case	O
,	O
RAM	O
usage	O
fits	O
the	O
equivalent	O
`	O
chunk	O
`	O
size	O
#CODE	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

It	O
gave	O
me	O
the	O
error	O
:	O
cqid	O
=	O
row	O
[	O
'	O
ClearQuest	O
ID	O
']	O
TypeError	O
:	O
string	O
indices	O
must	O
be	O
integers	O
,	O
not	O
str	O
...........	O

The	O
use	O
case	O
is	O
that	O
I	O
have	O
different	O
time	O
series	O
coming	O
from	O
different	O
data	O
sources	O
.	O

How	O
can	O
I	O
get	O
pandas	O
Timestamp	O
offset	O
by	O
certain	O
amount	O
of	O
months	O
?	O

I	O
managed	O
to	O
get	O
the	O
stats	O
by	O
placing	O
everything	O
in	O
nested	O
dictionary	O
,	O
but	O
I	O
feel	O
that	O
there	O
may	O
be	O
a	O
much	O
easier	O
way	O
to	O
the	O
approach	O
by	O
using	O
pandas	O
dataframes	O
and	O
groubpy	O
.	O

Just	O
to	O
get	O
a	O
sense	O
of	O
what	O
I'm	O
trying	O
to	O
achieve	O
.	O

Which	O
is	O
suspect	O
is	O
due	O
to	O
my	O
data	O
range	O
,,	O
but	O
it	O
may	O
well	O
be	O
that	O
I	O
don't	O
understand	O
the	O
other	O
parameters	O
.	O

Your	O
second	O
one	O
doesn't	O
really	O
make	O
sense	O
as	O
an	O
aggregation	O
.	O

How	O
can	O
I	O
get	O
the	O
index	O
of	O
certain	O
element	O
of	O
a	O
Series	O
in	O
python	O
pandas	O
?	O

(	O
Very	O
,	O
very	O
late	O
reply	O
-	O
apologies	O
.	O
)	O
That's	O
true	O
,	O
you'd	O
use	O
the	O
method	O
EdChum	O
suggested	O
for	O
longer	O
lists	O
of	O
columns	O
.	O

If	O
actual_sum	O
and	O
expected_to_date	O
are	O
equal	O
,	O
put	O
a	O
0	O

`	O
ts	O
[	O
ts	O
[	O
'	O
values	O
']	O
0	O
]`	O
should	O
produce	O
the	O
output	O
you	O
are	O
looking	O
for	O
.	O

And	O
I	O
get	O
the	O
counts	O
:	O
#CODE	O

The	O
standard	O
deviation	O
differs	O
between	O
pandas	O
and	O
numpy	O
.	O

I	O
would	O
like	O
to	O
get	O
rid	O
of	O
the	O
loops	O
,	O
if	O
that	O
is	O
possible	O
.	O

If	O
I	O
change	O
the	O
names	O
then	O
there	O
is	O
nothing	O
to	O
reference	O
.	O

I	O
even	O
tried	O
building	O
from	O
the	O
git	O
,	O
but	O
whatever	O
I	O
seem	O
to	O
do	O
,	O
I	O
get	O
the	O
same	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
do	O
the	O
following	O
operations	O
on	O
the	O
data	O
storage	O
:	O

How	O
do	O
I	O
get	O
it	O
to	O
actually	O
show	O
the	O
graph	O
?	O

#URL	O
shows	O
a	O
way	O
to	O
get	O
the	O
number	O
of	O
days	O
in	O
a	O
month	O
,	O
making	O
the	O
rest	O
more	O
or	O
less	O
trivial	O
as	O
they	O
don't	O
vary	O
.	O

to	O
create	O
average	O
values	O
with	O
an	O
equidistant	O
time-vector	O
.	O

I	O
get	O
something	O
where	O
all	O
"	O
newlines	O
"	O
are	O
escaped	O
.	O

Reproducing	O
without	O
a	O
data	O
file	O
,	O
using	O
Jeff's	O
suggestion	O
:	O
#CODE	O

However	O
,	O
I	O
also	O
want	O
to	O
get	O
it	O
on	O
the	O
basis	O
of	O
the	O
`	O
Group	O
`	O
variable	O
,	O
which	O
means	O
I	O
don't	O
want	O
to	O
get	O
`	O
Bob	O
`'	O
s	O
`	O
Value	O
`	O
based	O
on	O
the	O
`	O
Jared	O
`'	O
s	O
`	O
Value	O
`	O
,	O
since	O
those	O
two	O
records's	O
`	O
Group	O
`	O
value	O
is	O
different	O
-	O
I	O
only	O
compute	O
it	O
within	O
each	O
specific	O
`	O
Group	O
`	O
variable	O
.	O

I	O
try	O
to	O
use	O
jsonlint	O
to	O
validate	O
these	O
json	O
files	O
but	O
encounter	O
some	O
error	O
messages	O
.	O

The	O
logic	O
to	O
arrive	O
at	O
that	O
database	O
is	O
an	O
intricate	O
mix	O
of	O
Python	O
processing	O
and	O
SQL	O
joins	O
done	O
in	O
sqlite3	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
the	O
`	O
str	O
`	O
accessor	O
to	O
split	O
the	O
data	O
into	O
two	O
columns	O
,	O
such	O
that	O
the	O
first	O
column	O
is	O
,	O
Name	O
,	O
contains	O
the	O
actual	O
name	O
(	O
first	O
name	O
last	O
name	O
)	O
,	O
and	O
the	O
second	O
column	O
,	O
Email	O
,	O
contains	O
the	O
email	O
address	O
)	O
.	O

In	O
fact	O
the	O
only	O
really	O
relevant	O
data	O
needed	O
for	O
the	O
plot	O
is	O
the	O
first	O
and	O
second	O
column	O
,	O
namely	O
:	O
`	O
Compression	O
Force	O
`	O
and	O
`	O
Compression	O
Velocity	O
`	O
.	O

How	O
to	O
get	O
special	O
characters	O
from	O
Excel	O
to	O
screen	O
using	O
pandas	O
?	O

And	O
replace	O
`'	O
Month	O
'`	O
with	O
`'	O
Day	O
'`	O
below	O
.	O

But	O
if	O
you	O
have	O
a	O
huge	O
amount	O
to	O
data	O
,	O
it	O
*	O
might	O
*	O
be	O
interesting	O
to	O
think	O
of	O
a	O
more	O
complex	O
data	O
model	O
.	O

What	O
are	O
you	O
trying	O
to	O
do	O
where	O
this	O
is	O
the	O
bottleneck	O
?	O

How	O
can	O
I	O
change	O
that	O
and	O
use	O
insted	O
the	O
first	O
line	O
of	O
output	O
code	O
as	O
a	O
column	O
(	O
In	O
this	O
case	O
line	O
10	O
:	O
Sub-Data	O
Item	O
...	O
)	O

My	O
objective	O
was	O
to	O
have	O
a	O
DTM	O
like	O
the	O
one	O
you	O
get	O
in	O
R	O
tm	O
.	O

So	O
right	O
now	O
all	O
the	O
data	O
comes	O
from	O
each	O
iteration	O
group	O
,	O
and	O
all	O
of	O
its	O
is	O
transformed	O
into	O
one	O
column	O
vector	O
.	O

I	O
am	O
trying	O
to	O
get	O
to	O
the	O
point	O
where	O
I	O
can	O
run	O
#CODE	O

While	O
I	O
don't	O
get	O
that	O
warning	O
with	O
#CODE	O

Thanks	O
@USER	O
-	O
I	O
mean	O
that	O
,	O
if	O
we	O
do	O
`	O
A-B	O
`	O
we	O
should	O
only	O
get	O
the	O
NaNs	O
in	O
`	O
A	O
`	O
,	O
and	O
not	O
the	O
NaNs	O
in	O
`	O
B	O
`	O
.	O

I	O
have	O
three	O
columns	O
in	O
my	O
data	O
set	O
,	O
namely	O
"	O
age	O
"	O
,	O
"	O
race	O
"	O
,	O
"	O
sex	O
"	O
,	O
that	O
I	O
care	O
about	O
.	O

Cannot	O
get	O
the	O
average	O
date	O
using	O
pandas	O

Any	O
suggestions	O
?	O

All	O
values	O
ought	O
to	O
be	O
integers	O
,	O
no	O
floats	O
.	O

Note	O
:	O
this	O
will	O
get	O
tripped	O
up	O
by	O
some	O
strings	O
,	O
so	O
use	O
with	O
caution	O
.	O

The	O
purpose	O
of	O
all	O
these	O
stuff	O
is	O
a	O
geographical	O
representation	O
of	O
data	O
on	O
a	O
spatial	O
grid	O
.	O

Since	O
Name	O
`	O
C	O
`	O
does	O
not	O
have	O
`	O
3	O
`	O
or	O
`	O
5	O
`	O
in	O
the	O
column	O
`	O
Activity	O
`	O
,	O
I	O
do	O
not	O
want	O
to	O
get	O
this	O
data	O
frame	O
.	O

Data	O
has	O
to	O
be	O
collected	O
before	O
local	O
data	O
frame	O
is	O
created	O
.	O

PANDAS	O
:	O
Extracting	O
values	O
from	O
a	O
column	O
by	O
applying	O
a	O
condition	O
on	O
other	O
columnns	O

If	O
you	O
try	O
to	O
produce	O
the	O
groups	O
from	O
my	O
example	O
you'll	O
see	O
what	O
I	O
mean	O
.	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

Not	O
sure	O
how	O
to	O
get	O
around	O
this	O
...	O
pretty	O
new	O
to	O
pandas	O
.	O

Here's	O
the	O
product	O
:	O
#CODE	O

However	O
,	O
as	O
the	O
data	O
became	O
large	O
,	O
we	O
played	O
with	O
SQLAlchemy	O
/	O
SQLite3	O
.	O

But	O
this	O
time	O
I	O
get	O
another	O
error	O
:	O
#CODE	O

Makes	O
the	O
change	O
the	O
idea	O
of	O
trying	O
to	O
use	O
this	O
approach	O
all	O
together	O
.	O

For	O
a	O
generalized	O
scenario	O
where	O
there	O
are	O
many	O
different	O
combinations	O
of	O
values	O
under	O
'	O
COL1	O
'	O
and	O
'	O
COL3	O
'	O
,	O
this	O
works	O
but	O
is	O
probably	O
not	O
nearly	O
as	O
efficient	O
as	O
it	O
can	O
be	O
:	O
#CODE	O

Similarly	O
in	O
your	O
example	O
where	O
you	O
plot	O
`	O
col1	O
,	O
col2	O
`	O
differently	O
based	O
on	O
`	O
col3	O
`	O
,	O
what	O
if	O
there	O
are	O
NA	O
values	O
that	O
break	O
the	O
association	O
between	O
`	O
col1	O
,	O
col2	O
,	O
col3	O
`	O
?	O

For	O
example	O
,	O
I	O
want	O
to	O
take	O
values	O
from	O
`	O
col_3	O
`	O
and	O
`	O
col_4	O
`	O
and	O
use	O
them	O
to	O
generate	O
a	O
single	O
values	O
.	O

The	O
speed	O
difference	O
is	O
astonishing	O
.	O

The	O
summation	O
in	O
one	O
group	O
won't	O
reduce	O
the	O
size	O
of	O
the	O
result	O
,	O
the	O
summation	O
I	O
want	O
to	O
do	O
is	O
across	O
different	O
groups	O
.	O

If	O
you	O
really	O
prefer	O
`	O
1	O
`'	O
s	O
and	O
`	O
0	O
`'	O
s	O
replace	O
the	O
last	O
line	O
with	O
:	O
#CODE	O

So	O
traverse	O
the	O
data	O
once	O
and	O
generate	O
both	O
arrays	O
would	O
be	O
preferred	O
.	O

Im	O
not	O
fully	O
adjusted	O
to	O
how	O
Pandas	O
is	O
using	O
matplotlib	O
so	O
i	O
often	O
switch	O
to	O
matplotlib	O
myself	O
if	O
plots	O
get	O
more	O
complicated	O
,	O
eg	O
:	O
#CODE	O

The	O
table	O
that	O
gives	O
this	O
message	O
contains	O
a	O
few	O
columns	O
,	O
none	O
of	O
them	O
have	O
data	O
in	O
them	O
.	O

so	O
yes	O
later	O
i	O
have	O
open	O
the	O
file	O
but	O
thanks	O
to	O
pandas	O
i	O
can	O
use	O
the	O
`	O
chunksize	O
`	O
command	O
to	O
get	O
the	O
information	O
i	O
need	O
.	O

create	O
column	O
names	O
by	O
joining	O
two	O
labels	O
of	O
different	O
levels	O
with	O
pandas	O

@USER	O
so	O
how	O
should	O
i	O
write	O
it	O
so	O
that	O
the	O
program	O
gives	O
seq	O
to	O
'	O
Hsequence	O
'	O
column	O
when	O
'	O
Hcolumn	O
'	O
contains	O
the	O
title	O
from	O
fasta	O
file	O
?	O

Also	O
,	O
in	O
my	O
larger	O
directory	O
,	O
this	O
is	O
taking	O
forever	O
-	O
as	O
in	O
,	O
about	O
a	O
gig	O
of	O
CSVs	O
is	O
timing	O
out	O
for	O
me	O
(	O
by	O
my	O
hand	O
)	O
at	O
around	O
20	O
minutes	O
.	O

The	O
key	O
was	O
unstacking	O
the	O
data	O
first	O
:	O
#CODE	O

I	O
want	O
to	O
get	O
the	O
latitude	O
and	O
longitude	O
coordinates	O
for	O
any	O
one	O
of	O
the	O
columns	O
in	O
the	O
data	O
frame	O
below	O
.	O

Option	O
values	O
are	O
restored	O
automatically	O
when	O
you	O
exit	O
the	O
`	O
with	O
`	O
block	O
.	O

I	O
am	O
finding	O
difficulty	O
to	O
plot	O
reason	O
every	O
csv	O
file	O
starts	O
with	O
different	O
date	O
,	O
that's	O
the	O
reason	O
I	O
was	O
trying	O
to	O
convert	O
into	O
no	O
.	O
of	O
days	O
,	O
so	O
that	O
I	O
can	O
plot	O
all	O
in	O
one	O
go	O
with	O
starting	O
day	O
-	O
1	O
,	O
for	O
example	O
:	O
-	O
csv	O
file	O
2	O
fall	O
short	O
as	O
compared	O
to	O
csv	O
file	O
1	O
.	O

Most	O
of	O
the	O
time	O
you	O
can	O
get	O
away	O
with	O
using	O
something	O
else	O
...	O

In	O
that	O
case	O
the	O
index	O
is	O
composed	O
of	O
integers	O
from	O
0	O
to	O
n	O
:	O
#CODE	O

You	O
have	O
a	O
difference	O
between	O
a	O
mac	O
and	O
a	O
pc	O
,	O
and	O
*	O
presumably	O
*	O
the	O
same	O
code	O
.	O

Suppose	O
you	O
want	O
to	O
find	O
the	O
row	O
or	O
rows	O
where	O
`	O
beef	O
`	O
production	O
was	O
the	O
highest	O
.	O

The	O
number	O
of	O
columns	O
may	O
differ	O
and	O
so	O
does	O
the	O
column	O
names	O
.	O

How	O
do	O
I	O
avoid	O
that	O
and	O
rather	O
generate	O
it	O
in	O
a	O
sparse	O
matrix	O
CSR	O
format	O
?	O

I	O
download	O
and	O
scrape	O
a	O
webpage	O
for	O
some	O
data	O
in	O
TSV	O
format	O
.	O

You	O
can	O
set	O
parameter	O
`	O
labels=False	O
`	O
to	O
get	O
the	O
integer	O
representation	O
#CODE	O

it's	O
not	O
too	O
much	O
of	O
a	O
stretch	O
to	O
insert	O
NaN's	O
into	O
the	O
data	O
using	O
reindexing	O
so	O
that	O
i	O
get	O
this	O
:	O
#CODE	O

Any	O
suggestions	O
?	O

Data-driven	O
DOM	O
manipulation	O
(	O
maybe	O
the	O
hardest	O
thing	O
to	O
wrap	O
one's	O
head	O
around	O
):	O
your	O
data	O
gets	O
transformed	O
into	O
DOM	O
elements	O
.	O

Your	O
regex	O
is	O
matching	O
on	O
all	O
`	O
-	O
`	O
characters	O
:	O
#CODE	O

1	O
)	O
create	O
additional	O
columns	O
with	O
clock	O
time	O
headings	O
for	O
5	O
minute	O
intervals	O
between	O
9:30	O
and	O
4:00	O
pm	O
,	O
so	O
the	O
headings	O
of	O
the	O
data	O
frame	O
look	O
like	O
:	O

`	O
Index	O
([	O
u'id	O
opinion	O
']	O
,	O
dtype=	O
'	O
object	O
')`	O
Thanks	O
for	O
the	O
response	O

The	O
end	O
product	O
would	O
be	O
ten	O
timeseries	O
plots	O
with	O
charted	O
lines	O
over	O
time	O
for	O
each	O
TID	O
.	O

And	O
get	O
the	O
result	O
:	O
#CODE	O

However	O
,	O
I	O
still	O
don't	O
get	O
why	O
`	O
iconv	O
`	O
messes	O
it	O
up	O
.	O

If	O
you	O
have	O
huge	O
CSV	O
data	O
,	O
NYSOL's	O
mcmd	O
is	O
the	O
best	O
.	O

I	O
get	O
#CODE	O

If	O
I	O
use	O
a	O
tweaked	O
version	O
of	O
@USER	O
'	O
s	O
suggestion	O
below	O
,	O
I	O
get	O
this	O
error	O
:	O
#CODE	O

ValueError	O
:	O
Unknown	O
format	O
code	O
'	O
f	O
'	O
for	O
object	O
of	O
type	O
'	O
str	O
'	O
-	O
why	O
do	O
I	O
get	O
this	O
the	O
second	O
time	O
but	O
not	O
the	O
first	O
time	O
?	O

Any	O
suggestion	O
about	O
the	O
reason	O
?	O

I	O
have	O
a	O
data	O
set	O
which	O
has	O
multiple	O
columns	O
,	O
strings	O
and	O
integers	O

is	O
the	O
condition	O
,	O
returning	O
a	O
booleans	O
array	O
of	O
True	O
/	O
False	O
for	O
all	O
values	O
meeting	O
the	O
condition	O
or	O
not	O
,	O
and	O
then	O
the	O
corresponding	O
A	O
values	O
are	O
selected	O

I	O
fixed	O
this	O
bug	O
in	O
0.11-dev	O
in	O
any	O
event	O
,	O
see	O
here	O
:	O
#URL	O
thanks	O
!	O

To	O
split	O
`	O
my_data2	O
`	O
into	O
two	O
arrays	O
of	O
roughly	O
equal	O
size	O
:	O
#CODE	O

to	O
get	O
a	O
`	O
Series	O
`	O
of	O
`	O
list	O
`	O
s	O
of	O
strings	O
.	O

For	O
example	O
,	O
you	O
can't	O
sum	O
a	O
mix	O
of	O
strings	O
and	O
floats	O
in	O
pandas	O
but	O
Excel	O
would	O
silently	O
drop	O
the	O
string	O
value	O
and	O
sum	O
the	O
floats	O
.	O

Notice	O
how	O
the	O
values	O
in	O
the	O
second	O
column	O
are	O
no	O
longer	O
integers	O
,	O
as	O
they	O
were	O
originally	O
.	O

I	O
have	O
a	O
large	O
but	O
very	O
sparse	O
matrix	O
(	O
50,000	O
rows*	O
100,000	O
columns	O
,	O
only	O
10%	O
of	O
the	O
values	O
are	O
known	O
)	O
.	O

In	O
python	O
normally	O
you	O
don't	O
need	O
and	O
you	O
shouldn't	O
use	O
a	O
semicolon	O
at	O
the	O
end	O
of	O
the	O
line	O
.	O

That's	O
all	O
data	O
python	O
is	O
reading	O
in	O
,	O
apparently	O
:	O
the	O
16	O
first	O
lines	O
,	O
or	O
at	O
least	O
I	O
am	O
not	O
able	O
to	O
get	O
the	O
rest	O
of	O
data	O
in	O
.	O

The	O
problem	O
is	O
to	O
find	O
average	O
values	O
of	O
temp1	O
,	O
temp2	O
and	O
temp3	O
for	O
a	O
period	O
of	O
time	O
(	O
say	O
,	O
2	O
days	O
)	O
over	O
the	O
same	O
intervals	O
(	O
for	O
that	O
example	O
-	O
15	O
minutes	O
)	O
.	O

In	O
generally	O
I	O
wonder	O
if	O
pandas	O
should	O
not	O
at	O
least	O
throw	O
a	O
warning	O
,	O
afterall	O
broadcasting	O
the	O
result	O
to	O
both	O
columns	O
should	O
be	O
almost	O
never	O
what	O
is	O
wanted	O
.	O

I	O
get	O
pandas	O
error	O
when	O
I	O
try	O
to	O
read	O
HDF5	O
format	O
files	O
that	O
I	O
have	O
created	O
with	O
h5py	O
.	O

Additionally	O
you	O
can	O
use	O
numpys	O
matrix	O
#CODE	O

I	O
updated	O
pandas	O
'	O
sudo	O
pip	O
install	O
--	O
upgrade	O
pandas	O
'	O
,	O
between	O
both	O
of	O
these	O
fixes	O
,	O
everything	O
worked	O
.	O

Sorry	O
can't	O
reproduce	O
nor	O
understand	O
your	O
real	O
problem	O
,	O
please	O
post	O
what	O
you	O
see	O
in	O
your	O
question	O

When	O
I	O
used	O
'	O
ethnicity	O
'	O
or	O
'	O
veteran	O
'	O
as	O
a	O
value	O
my	O
results	O
came	O
out	O
really	O
strange	O
and	O
didn't	O
match	O
my	O
value	O
counts	O
numbers	O
.	O

`	O
post_start	O
`	O
is	O
the	O
date	O
that	O
the	O
employee	O
started	O
in	O
the	O
post	O
,	O
and	O
`	O
change_date	O
`	O
is	O
the	O
date	O
that	O
the	O
post	O
title	O
was	O
changed	O
.	O

How	O
do	O
I	O
replace	O
the	O
ints	O
with	O
the	O
float	O
values	O
from	O
another	O
column	O
(	O
by	O
same	O
row	O
)	O
,	O
but	O
leave	O
all	O
the	O
nulls	O
?	O

There	O
may	O
be	O
a	O
more	O
foolproof	O
,	O
cleaner	O
way	O
of	O
computing	O
date	O
time	O
differences	O
in	O
pandas	O
.	O

However	O
,	O
to	O
get	O
the	O
row	O
sum	O
,	O
one	O
needs	O
to	O
specify	O
axis=1	O
.	O

Using	O
the	O
second	O
method	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Filter	O
data	O
to	O
get	O
only	O
first	O
day	O
of	O
the	O
month	O
rows	O

(	O
FYI	O
if	O
i	O
insert	O
a	O
print	O
print	O
(	O
vals	O
)	O
in	O
the	O
middle	O
of	O
that	O
loop	O
,	O
it	O
prints	O
#CODE	O

For	O
days	O
in	O
a	O
month	O
(	O
'	O
2015-07	O
'	O
say	O
)	O
You	O
could	O
change	O
#CODE	O

Doesnt	O
the	O
frame	O
variable	O
get	O
overwritten	O
during	O
each	O
iteration	O
in	O
the	O
loop	O
?	O

Any	O
other	O
advice	O
I	O
can	O
leverage	O
in	O
the	O
meantime	O
?	O

If	O
`	O
Change	O
Closing	O
Date	O
`	O
is	O
True	O
,	O
I	O
would	O
like	O
to	O
add	O
`	O
Closing	O
Date2	O
`	O
column	O
into	O
my	O
new	O
column	O
with	O
adding	O
1	O
year	O
.	O

If	O
you	O
REALLY	O
want	O
to	O
get	O
by	O
a	O
group	O
individually	O
#CODE	O

but	O
I	O
get	O
the	O
error	O
:	O
#CODE	O

I	O
am	O
new	O
to	O
pandas	O
for	O
data	O
analysis	O
and	O
I	O
just	O
installed	O
pandas	O
with	O
required	O
dependencies	O
(	O
NumPy	O
,	O
python-dateutil	O
,	O
pytz	O
,	O
numexpr	O
,	O
bottleneck	O
and	O
matplotlib	O
)	O
.	O

What	O
do	O
you	O
get	O
if	O
you	O
print	O
that	O
?	O

Can't	O
you	O
use	O
sets	O
and	O
intersections	O
?	O

is	O
there	O
a	O
way	O
to	O
insert	O
`	O
s	O
`	O
into	O
`	O
df	O
`	O
without	O
creating	O
a	O
reindexed	O
copy	O
of	O
`	O
df	O
`	O
first	O
?	O

I'm	O
using	O
python	O
2.7.5	O
(	O
with	O
all	O
the	O
packages	O
in	O
the	O
python	O
(	O
x	O
,	O
y	O
)	O
bundle	O
)	O
,	O
and	O
running	O
files	O
from	O
the	O
command	O
prompt	O
.	O

Any	O
suggestions	O
??	O

This	O
will	O
never	O
get	O
the	O
similar	O
graph	O
as	O
the	O
kernel	O
estimate	O
base	O
of	O
the	O
original	O
data	O
,	O
result	O
:	O

The	O
working	O
version	O
I	O
have	O
is	O
this	O
one	O
,	O
but	O
I	O
feel	O
there	O
is	O
potential	O
for	O
improvement	O
,	O
as	O
I	O
find	O
my	O
solution	O
unreadable	O
and	O
I	O
am	O
unsure	O
about	O
how	O
it	O
would	O
generalize	O
to	O
multiindexes	O
#CODE	O

Also	O
,	O
once	O
you	O
get	O
to	O
15	O
points	O
,	O
you'll	O
be	O
able	O
to	O
upvote	O
as	O
well	O
.	O

You	O
can	O
then	O
get	O
the	O
last	O
first	O
value	O
by	O
forward	O
filling	O
`	O
first_values	O
`	O
,	O
reindexing	O
like	O
`	O
second_values	O
`	O
,	O
stacking	O
again	O
and	O
indexing	O
into	O
the	O
result	O
using	O
the	O
original	O
`'	O
time	O
'	O
,	O
'	O
second	O
'`	O
pairs	O
:	O
#CODE	O

how	O
do	O
i	O
avoid	O
creating	O
so	O
many	O
variables	O
as	O
I	O
add	O
columns	O
together	O
?	O

Any	O
suggestion	O
on	O
how	O
to	O
efficiently	O
achieve	O
this	O
?	O

I	O
get	O
:	O
#CODE	O

For	O
instance	O
,	O
I	O
can	O
compute	O
the	O
value	O
for	O
data	O
record	O
3	O
by	O
taking	O
`	O
len	O
(	O
set	O
([	O
4	O
,	O
4	O
,	O
6	O
,	O
12	O
]))`	O
which	O
gives	O
3	O
.	O

@USER	O
That's	O
a	O
great	O
suggestion	O
(	O
for	O
some	O
use-cases	O
)	O
it	O
should	O
be	O
its	O
own	O
answer	O
(	O
so	O
I	O
can	O
upvote	O
it	O
)	O
Though	O
it	O
does	O
need	O
tweak	O
to	O
multiply	O
by	O
100	O
.	O

python	O
how	O
to	O
sum	O
together	O
all	O
values	O
within	O
a	O
time	O
interval	O
in	O
datetime64	O
?	O

was	O
trying	O
to	O
do	O
a	O
"	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
results	O
))"	O
before	O
the	O
"	O
for	O
item	O
in	O
results	O
[	O
i	O
]"	O
that	O
you	O
did	O
but	O
not	O
working	O
for	O
me	O
...	O

But	O
,	O
on	O
the	O
other	O
hand	O
,	O
if	O
your	O
columns	O
aren't	O
in	O
the	O
same	O
order	O
,	O
then	O
my	O
suggestion	O
won't	O
work	O
.	O

When	O
I	O
execute	O
the	O
program	O
for	O
the	O
data	O
of	O
the	O
same	O
day	O
,	O
processor	O
time	O
becomes	O
long	O
from	O
the	O
same	O
point	O
.	O

I'm	O
new	O
to	O
pandas	O
,	O
python	O
,	O
and	O
scripting	O
in	O
general	O
,	O
so	O
am	O
still	O
getting	O
my	O
head	O
around	O
the	O
basics	O
.	O

You	O
can	O
,	O
for	O
example	O
,	O
use	O
interpolation	O
to	O
get	O
equally	O
spaced	O
datapoints	O
out	O
off	O
your	O
timeseries	O
.	O

What	O
I	O
was	O
hoping	O
for	O
was	O
to	O
add	O
up	O
all	O
of	O
the	O
frequencies	O
across	O
the	O
websites	O
and	O
to	O
create	O
two	O
columns	O
:	O
Column	O
A	O
with	O
the	O
word	O
,	O
and	O
Column	O
B	O
with	O
all	O
of	O
the	O
frequencies	O
added	O
together	O
.	O

It	O
does	O
not	O
work	O
without	O
dropping	O
index	O
.	O

Now	O
I	O
was	O
wondering	O
how	O
I	O
could	O
subtract	O
my	O
multi-year	O
timeseries	O
from	O
this	O
standard	O
year	O
,	O
in	O
order	O
to	O
get	O
a	O
timeseries	O
that	O
show	O
which	O
days	O
were	O
below	O
or	O
above	O
it's	O
standard	O
.	O

I	O
may	O
try	O
installing	O
an	O
older	O
version	O
to	O
find	O
out	O
what	O
was	O
actually	O
getting	O
calculated	O
.	O

Is	O
there	O
any	O
disadvantage	O
?	O

The	O
length	O
of	O
the	O
frame	O
is	O
over	O
2	O
million	O
rows	O
and	O
looping	O
to	O
extract	O
the	O
elements	O
I	O
need	O
is	O
a	O
poor	O
choice	O
.	O

edit	O
I	O
believe	O
'	O
endog	O
'	O
as	O
defined	O
is	O
incorrect-I	O
should	O
be	O
passing	O
the	O
values	O
for	O
which	O
I	O
want	O
to	O
predict	O
;	O
therefore	O
I've	O
created	O
a	O
date	O
range	O
of	O
12	O
periods	O
past	O
the	O
last	O
recorded	O
value	O
.	O

@USER	O
It	O
should	O
be	O
a	O
little	O
quicker	O
with	O
a	O
boolean	O
index	O
like	O
that	O
,	O
but	O
it	O
does	O
do	O
a	O
cast	O
(	O
timedelta	O
)	O
so	O
I'm	O
not	O
100%	O
sure	O
on	O
that	O
.	O

I	O
still	O
get	O
the	O
same	O
TypeError	O
message	O
using	O
the	O
line	O
you	O
suggest	O
.	O

Use	O
regex	O
with	O
`	O
python	O
`	O
engine	O
#CODE	O

(	O
it's	O
pretty	O
clear	O
that	O
`	O
id	O
`	O
maps	O
to	O
`	O
individual	O
`	O
,	O
but	O
I	O
would	O
clean	O
that	O
up	O
too	O
)	O
.	O

Being	O
able	O
to	O
quickly	O
determine	O
the	O
time	O
difference	O
between	O
Order	O
1	O
and	O
Order	O
2	O
(	O
per	O
PersonID	O
)	O
would	O
be	O
great	O
too	O
.	O

Thus	O
,	O
if	O
there	O
is	O
an	O
update	O
to	O
some	O
value	O
on	O
a	O
memory	O
page	O
,	O
that	O
page	O
is	O

and	O
make	O
this	O
a	O
Series	O
,	O
mapping	O
names	O
to	O
their	O
respective	O
numbers	O
:	O
#CODE	O

That	O
is	O
,	O
for	O
each	O
second	O
there	O
is	O
a	O
value	O
and	O
they	O
should	O
not	O
be	O
averaged	O
,	O
just	O
grouped	O
together	O
to	O
a	O
new	O
series	O
..	O

Specifically	O
,	O
in	O
this	O
case	O
,	O
I'd	O
only	O
like	O
to	O
drop	O
row	O
with	O
Indices	O
'	O
1991-12-31	O
'	O
and	O
'	O
1992-01-31	O
'	O
.	O

Or	O
read	O
it	O
in	O
directly	O
as	O
a	O
csv	O
,	O
by	O
appending	O
'	O
na	O
'	O
to	O
the	O
list	O
of	O
values	O
to	O
be	O
considered	O
NaN	O
:	O
#CODE	O

I	O
fail	O
to	O
see	O
the	O
corelation	O
between	O
"	O
John	O
"	O
and	O
the	O
dates	O
in	O
the	O
target	O
.	O

I	O
get	O
:	O

The	O
question	O
is	O
,	O
how	O
can	O
I	O
remove	O
or	O
filter	O
out	O
all	O
entries	O
that	O
have	O
frequency	O
1	O
?	O

For	O
all	O
the	O
other	O
names	O
that	O
are	O
not	O
in	O
the	O
top	O
ten	O
frequencies	O
I	O
want	O
to	O
combine	O
their	O
number	O
of	O
occurences	O
together	O
under	O
say	O
the	O
name	O
"	O
other	O
"	O
.	O

You	O
should	O
get	O
the	O
following	O
result	O
:	O

Which	O
indeed	O
is	O
longer	O
(	O
50	O
)	O
than	O
my	O
number	O
of	O
columns	O
/	O
indices	O
(	O
25	O
)	O
.	O

I	O
am	O
new	O
to	O
Python	O
(	O
and	O
programming	O
in	O
general	O
!	O
)	O
,	O
trying	O
to	O
conduct	O
some	O
data	O
analysis	O
using	O
Pandas	O
.	O

I	O
would	O
like	O
to	O
combine	O
these	O
columns	O
into	O
start	O
time	O
(	O
index	O
)	O
and	O
length	O
in	O
actual	O
seconds	O
.	O

I'm	O
looking	O
to	O
find	O
,	O
for	O
each	O
Census	O
Block	O
centroid	O
,	O
the	O
distance	O
to	O
it's	O
closest	O
restaurant	O
.	O

You	O
will	O
get	O
the	O
exception	O
"	O
appended	O
items	O
do	O
not	O
match	O
existing	O
items	O
in	O
table	O
!	O

Honestly	O
-	O
we	O
were	O
going	O
to	O
originally	O
do	O
visualizations	O
with	O
it	O
(	O
heatmaps	O
)	O
-	O
but	O
for	O
a	O
lot	O
of	O
reasons	O
we're	O
now	O
going	O
to	O
use	O
D3	O
...	O

For	O
example	O
,	O
if	O
I	O
say	O
year	O
,	O
the	O
entire	O
column	O
needs	O
to	O
be	O
appended	O
into	O
a	O
list	O
like	O
[	O
1	O
year	O
,	O
3	O
minutes	O
,	O
2	O
hours	O
]	O
.	O

Anyone	O
have	O
any	O
suggestions	O
for	O
how	O
to	O
accomplish	O
this	O
?	O

Yeah	O
I	O
know	O
it	O
gives	O
NaN	O
padding	O
,	O
but	O
only	O
on	O
the	O
indices	O
the	O
joining	O
is	O
done	O
over	O
.	O

The	O
paired	O
measurements	O
should	O
have	O
the	O
same	O
month	O
,	O
just	O
different	O
years	O
.	O

You	O
can	O
get	O
started	O
on	O
debugging	O
this	O
by	O
just	O
adding	O
a	O
line	O
to	O
your	O
code	O
and	O
running	O
again	O
:	O
#CODE	O

When	O
I	O
run	O
the	O
solution	O
I	O
get	O
the	O
error	O
.	O

Then	O
let's	O
add	O
a	O
helper	O
column	O
,	O
called	O
Safe	O
,	O
that	O
will	O
be	O
a	O
concatenation	O
of	O
all	O
the	O
Safex	O
columns	O
.	O

product	O
1111	O
non-null	O
object	O

In	O
R	O
,	O
using	O
the	O
car	O
package	O
,	O
there	O
is	O
a	O
useful	O
function	O
`	O
some	O
(	O
x	O
,	O
n	O
)`	O
which	O
is	O
similar	O
to	O
head	O
but	O
selects	O
,	O
in	O
this	O
example	O
,	O
10	O
rows	O
at	O
random	O
from	O
x	O
.	O

The	O
separator	O
(	O
between	O
cells	O
)	O
is	O
defined	O
by	O
the	O
operating	O
system	O
(	O
at	O
least	O
under	O
Windows	O
)	O
,	O
and	O
when	O
the	O
system	O
wide	O
list	O
separator	O
differs	O
from	O
comma	O
,	O
pandas	O
(	O
or	O
anything	O
else	O
I	O
tried	O
)	O
cannot	O
determine	O
what	O
separator	O
should	O
be	O
used	O
.	O

Setting	O
up	O
a	O
histogram	O
with	O
a	O
range	O
and	O
an	O
appropriate	O
bin	O
size	O
is	O
an	O
unknown	O
.	O

Thanks	O
TravisJ	O
,	O
I	O
guess	O
I	O
was	O
just	O
struggling	O
to	O
get	O
the	O
(	O
...	O
something	O
involving	O
group	O
...	O
)	O
in	O
when	O
i	O
was	O
using	O
the	O
ax=fig1	O
....	O
method	O
.	O

I	O
am	O
optimising	O
the	O
span	O
of	O
an	O
exponential	O
moving	O
average	O
and	O
the	O
number	O
of	O
lagged	O
variables	O
that	O
I	O
use	O
in	O
the	O
regression	O
.	O

The	O
error	O
message	O
that	O
I	O
get	O
is	O
:	O
#CODE	O

It	O
doesn't	O
however	O
take	O
advantage	O
of	O
the	O
psql	O
package	O
in	O
Pandas	O
.	O

On	O
a	O
much	O
larger	O
data	O
set	O
,	O
this	O
runs	O
in	O
790	O
ms	O
compared	O
to	O
1345	O
ms	O
for	O
ajcr's	O
and	O
Primer's	O
solutions	O
.	O

I've	O
put	O
together	O
one	O
approach	O
to	O
that	O
solution	O
that	O
should	O
scale	O
relatively	O
well	O
.	O

I	O
was	O
hoping	O
there	O
was	O
an	O
easy	O
way	O
to	O
get	O
the	O
set	O
of	O
B	O
values	O
per	O
each	O
A	O
value	O
like	O
`	O
{	O
'	O
one	O
'	O
:[	O
'	O
A	O
'	O
,	O
'	O
B	O
']	O
,	O
'	O
two	O
'	O
:[	O
'	O
A	O
']	O
,	O
'	O
three	O
'	O
:[	O
'	O
B	O
']	O
}	O
`	O
but	O
I	O
don't	O
see	O
anything	O
like	O
that	O
in	O
the	O
pandas	O
documentation	O

To	O
avoid	O
chained	O
indexing	O
,	O
you	O
need	O
to	O
get	O
all	O
your	O
conditions	O
into	O
a	O
single	O
set	O
of	O
brackets	O
.	O

But	O
trying	O
to	O
parse	O
the	O
column	O
name	O
and	O
hierarchy	O
and	O
auto-generate	O
the	O
insertable	O
thing	O
with	O
matching	O
index	O
is	O
unpleasant	O
.	O

The	O
seaborn	O
package	O
will	O
allow	O
you	O
to	O
plot	O
long	O
form	O
data	O
like	O
you	O
have	O
without	O
pivoting	O
but	O
pandas	O
requires	O
shared	O
index	O
and	O
one	O
column	O
per	O
plotted	O
line	O
by	O
default	O
so	O
your	O
solution	O
is	O
the	O
correct	O
one	O
.	O

Unable	O
to	O
filter	O
out	O
missing	O
(	O
NaN	O
)	O
location	O
data	O
while	O
using	O
Pandas	O
and	O
Geocoder	O
modules	O
in	O
Python	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

How	O
could	O
I	O
sum	O
consecutive	O
day	O
values	O
here	O
,	O
so	O
I	O
would	O
get	O
something	O
like	O
this	O
?	O

I	O
changed	O
this	O
to	O
use	O
\t	O
as	O
the	O
separator	O
.	O

It's	O
possible	O
,	O
but	O
if	O
your	O
data	O
is	O
organized	O
it's	O
very	O
quick	O
with	O
shifting	O
it	O

@USER	O
fixed	O
,	O
was	O
a	O
typo	O
;	O
this	O
take	O
a	O
full	O
uri	O

Just	O
get	O
rid	O
of	O
it	O
and	O
reindent	O
the	O
loop	O
body	O
to	O
the	O
left	O
one	O
level	O
.	O

However	O
,	O
I	O
still	O
get	O
the	O
warning	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

but	O
you	O
then	O
need	O
to	O
store	O
these	O
dfs	O
somewhere	O
which	O
means	O
either	O
in	O
a	O
list	O
or	O
tuple	O
or	O
some	O
other	O
container	O
or	O
use	O
a	O
generator	O

and	O
so	O
on	O
for	O
the	O
remaining	O
location	O
categories	O
.	O

@USER	O
do	O
**	O
all	O
**	O
the	O
columns	O
in	O
the	O
DF	O
require	O
that	O
same	O
replacement	O
?	O

did	O
you	O
get	O
any	O
warnings	O
while	O
installing	O
numpy	O
or	O
pandas	O
?	O

This	O
`	O
df	O
`	O
consist	O
of	O
volume	O
observations	O
at	O
every	O
10	O
second	O
for	O
22	O
non-consecutive	O
days	O
.	O

I've	O
also	O
included	O
a	O
section	O
to	O
immediately	O
identify	O
any	O
redundant	O
genes	O
that	O
don't	O
have	O
any	O
SNPs	O
that	O
fall	O
within	O
their	O
range	O
.	O

Using	O
some	O
string	O
formatting	O
to	O
get	O
the	O
index	O
,	O
but	O
works	O
for	O
any	O
combination	O
of	O
months	O
(	O
as	O
long	O
as	O
the	O
first	O
month	O
is	O
explicitly	O
included	O
)	O
.	O

Any	O
suggestions	O
please	O
?	O

I	O
get	O
:	O
#CODE	O

I	O
understand	O
that	O
we	O
can	O
line	O
them	O
all	O
together	O
side	O
by	O
side	O
so	O
their	O
dates	O
match	O
and	O
loop	O
row	O
by	O
row	O
,	O
but	O
then	O
when	O
i	O
have	O
100k	O
different	O
securities	O
,	O
this	O
is	O
slow	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
automate	O
this	O
table	O
so	O
If	O
I	O
change	O
my	O
parameters	O
in	O
my	O
code	O
,	O
I	O
get	O
a	O
new	O
table	O
with	O
that	O
new	O
data	O
.	O

But	O
I	O
get	O
,	O
which	O
I	O
cannot	O
understand	O
,	O
#CODE	O

Here's	O
the	O
product	O
:	O
#CODE	O

(	O
My	O
actual	O
problem	O
involves	O
parsing	O
strings	O
into	O
lists	O
,	O
then	O
checking	O
for	O
presents	O
of	O
a	O
1	O
or	O
0	O
in	O
one	O
list	O
and	O
if	O
so	O
marking	O
the	O
cosponsoring	O
element	O
in	O
the	O
other	O
list	O
with	O
a	O
asterix	O
,	O
but	O
I	O
didn't	O
want	O
to	O
put	O
that	O
in	O
my	O
example	O
and	O
it	O
is	O
long	O
and	O
harder	O
to	O
follow	O
.	O

What	O
is	O
the	O
error	O
you	O
get	O
?	O

So	O
first	O
chunk	O
is	O
stored	O
as	O
integer	O
and	O
in	O
second	O
chunk	O
gets	O
NaN	O
values	O
and	O
store	O
cannot	O
convert	O
NaN	O
to	O
integer	O

Just	O
play	O
around	O
with	O
it	O
to	O
get	O
it	O
right	O
.	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Edit	O
:	O
Here's	O
an	O
example	O
of	O
generating	O
a	O
close-enough	O
data	O
set	O
,	O
so	O
you	O
can	O
get	O
some	O
idea	O
of	O
what	O
I	O
mean	O
:	O
#CODE	O

@USER	O
that	O
means	O
you	O
have	O
non	O
string	O
values	O
mixed	O
in	O
;	O
you	O
need	O
to	O
specify	O
`	O
na=True	O
`	O
or	O
`	O
na=False	O
`	O
depending	O
on	O
what	O
those	O
values	O
are	O
.	O
see	O
my	O
edits	O
.	O

Preferably	O
,	O
use	O
Pandas	O
for	O
the	O
data	O
structure	O
and	O
Python	O
for	O
the	O
language	O
.	O

Then	O
as	O
in	O
the	O
first	O
point	O
,	O
I	O
would	O
like	O
to	O
calculate	O
the	O
number	O
of	O
continuous	O
up	O
and	O
down	O
sequences	O
from	O
the	O
previous	O
point	O
.	O

0	O
can	O
be	O
changed	O
to	O
1	O
or	O
other	O
values	O
later	O
in	O
the	O
code	O

and	O
in	O
the	O
instance	O
when	O
i	O
am	O
able	O
to	O
set	O
the	O
index	O
of	O
the	O
df	O
to	O
the	O
range	O
,	O
the	O
data	O
in	O
the	O
4	O
columns	O
change	O
to	O
NaN	O
since	O
they	O
have	O
no	O
data	O
that	O
matches	O
the	O
new	O
index	O
.	O

(	O
Note	O
that	O
this	O
produces	O
an	O
unrealistically	O
high	O
number	O
of	O
flooding	O
events	O
,	O
but	O
that's	O
just	O
because	O
of	O
how	O
the	O
sample	O
data	O
is	O
set	O
up	O
and	O
not	O
reflective	O
of	O
a	O
typical	O
pond	O
,	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
pond	O
flooding	O
!	O
)	O
#CODE	O

What	O
output	O
do	O
you	O
get	O
from	O
this	O
?	O

`	O
Ideally	O
,	O
for	O
the	O
pages	O
that	O
have	O
multiple	O
groups	O
of	O
34	O
,	O
i'd	O
like	O
to	O
add	O
a	O
suffix	O
of	O
_1	O
,	O
_2	O
,	O
_3	O
,	O
etc	O
.	O

That	O
means	O
duplicating	O
values	O
from	O
cols	O
`	O
product_id	O
`	O
and	O
tem_name	O
`	O
as	O
long	O
as	O
there	O
are	O
items	O
in	O
list	O
`	O
prices	O
`	O
.	O

cool	O
,	O
but	O
I	O
get	O
a	O
syntax	O
error	O
for	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

For	O
empty	O
date	O
cells	O
I	O
am	O
inserting	O
a	O
NaT	O
,	O
which	O
I	O
would	O
have	O
thought	O
would	O
be	O
fine	O
,	O
but	O
in	O
Oracle	O
that	O
is	O
becoming	O
some	O
weird	O
invalid	O
time	O
that	O
displays	O
as	O
"	O
0001-255-255	O
00:00	O
:	O
00	O
"	O
(	O
Something	O
like	O
MAXINT	O
or	O
0	O
being	O
converted	O
to	O
a	O
timestamp	O
I'm	O
guessing	O
?	O
)	O
#CODE	O

I	O
would	O
like	O
to	O
get	O
the	O
following	O
result	O
:	O
#CODE	O

so	O
in	O
all	O
both	O
suggestions	O
below	O
worked	O
for	O
me	O
:	O

I	O
get	O
the	O
following	O
error	O
message	O
:	O
#CODE	O

I	O
would	O
suggest	O
using	O
the	O
duplicated	O
method	O
on	O
the	O
Pandas	O
Index	O
itself	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
group	O
the	O
x	O
values	O
into	O
equal	O
size	O
bins	O
,	O
and	O
for	O
each	O
bin	O
take	O
the	O
average	O
value	O
of	O
both	O
x	O
and	O
y	O
.	O

For	O
this	O
data	O
set	O
the	O
two	O
numbers	O
are	O
always	O
equal	O
.	O

Do	O
you	O
want	O
to	O
check	O
if	O
the	O
value	O
is	O
in	O
the	O
provided	O
bounds	O
and	O
return	O
a	O
boolean	O
True	O
/	O
False	O
array	O
,	O
or	O
you	O
want	O
to	O
represent	O
your	O
values	O
in	O
categories	O
represented	O
by	O
those	O
bounds	O
?	O

The	O
series	O
I'd	O
like	O
to	O
get	O
would	O
contain	O
:	O
#CODE	O

The	O
error	O
I	O
get	O
:	O
#CODE	O

How	O
do	O
you	O
deal	O
with	O
apparently	O
overlapping	O
date	O
ranges	O
?	O

There	O
are	O
more	O
columns	O
in	O
the	O
data	O
that	O
are	O
not	O
shown	O
above	O
,	O
and	O
using	O
this	O
code	O
causes	O
the	O
non-numeric	O
columns	O
to	O
drop	O
off	O
.	O

Any	O
suggestions	O
?	O

python	O
-	O
trying	O
to	O
get	O
a	O
new	O
pandas	O
release	O

In	O
the	O
process	O
of	O
creating	O
an	O
example	O
with	O
code	O
,	O
I	O
managed	O
to	O
get	O
it	O
working	O
.	O

Can	O
you	O
post	O
raw	O
data	O
and	O
example	O
code	O
that	O
demonstrates	O
this	O
'	O
cutting	O
'	O
off	O

Running	O
your	O
code	O
on	O
the	O
sample	O
data	O
produces	O
the	O
same	O
result	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
sortedness	O
since	O
with	O
very	O
large	O
series	O
merging	O
when	O
we	O
know	O
it's	O
sorted	O
should	O
be	O
linear	O
in	O
total	O
length	O
of	O
the	O
arrays	O
,	O
whereas	O
any	O
sort	O
will	O
be	O
non-linear	O
.	O

What	O
if	O
you	O
just	O
changed	O
the	O
index	O
from	O
date	O
/	O
status	O
to	O
date	O
/	O
var1	O
/	O
status	O
?	O

which	O
when	O
imported	O
into	O
pandas	O
data	O
frames	O
and	O
each	O
joined	O
to	O
a	O
common	O
timestamp	O
,	O
with	O
a	O
day	O
of	O
year	O
field	O
added	O
,	O
so	O
looking	O
something	O
like	O
:	O
#CODE	O

This	O
works	O
only	O
if	O
your	O
object	O
is	O
table	O
format	O
(	O
rather	O
than	O
fixed	O
format	O
)	O
.	O

Drop	O
range	O
of	O
columns	O
by	O
labels	O

You	O
could	O
put	O
```	O
[	O
'	O
col1	O
']```	O
at	O
the	O
end	O
to	O
get	O
an	O
int	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

For	O
encoding	O
training	O
data	O
you	O
can	O
use	O
fit_transform	O
which	O
will	O
discover	O
the	O
category	O
labels	O
and	O
create	O
appropriate	O
dummy	O
variables	O
.	O

I	O
think	O
you're	O
confusing	O
how	O
to	O
filter	O
here	O
,	O
if	O
you're	O
looking	O
for	O
a	O
specific	O
value	O
then	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
VKO	O
']`	O
will	O
return	O
only	O
the	O
rows	O
where	O
that	O
condition	O
is	O
satisfied	O
,	O
for	O
your	O
last	O
part	O
the	O
reason	O
you	O
get	O
an	O
empty	O
row	O
is	O
that	O
you're	O
slicing	O
the	O
first	O
3	O
rows	O
and	O
then	O
comparing	O
the	O
first	O
value	O
with	O
your	O
string	O
but	O
the	O
first	O
string	O
value	O
is	O
'	O
VKO	O
'	O
and	O
not	O
'	O
ZZZ	O
'	O
,	O
you	O
should	O
do	O
this	O
:	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
ZZZ	O
']`	O
to	O
filter	O
the	O
resuls	O
first	O

I	O
had	O
changed	O
the	O
data	O
on	O
the	O
local	O
file	O
.	O

Note	O
that	O
this	O
is	O
slightly	O
different	O
as	O
we	O
are	O
returning	O
the	O
first	O
index	O
here	O
(	O
and	O
not	O
the	O
normally	O
returned	O
last	O
,	O
youy	O
could	O
do	O
either	O
)	O
.	O

I'm	O
not	O
sure	O
how	O
the	O
archive	O
block	O
reading	O
works	O
and	O
how	O
much	O
data	O
it	O
loads	O
into	O
memory	O
,	O
but	O
it's	O
clear	O
that	O
you	O
will	O
have	O
to	O
somehow	O
control	O
the	O
size	O
of	O
the	O
chunks	O
.	O

Regarding	O
nesting	O
of	O
functions	O
:	O
if	O
you	O
believe	O
a	O
function	O
has	O
a	O
general	O
purpose	O
or	O
is	O
reusable	O
,	O
then	O
it	O
should	O
be	O
defined	O
at	O
the	O
top	O
level	O
of	O
a	O
module	O
or	O
some	O
place	O
where	O
other	O
functions	O
can	O
call	O
it	O
.	O

I	O
get	O
an	O
error	O
saying	O
:	O
lambda	O
(	O
)	O
takes	O
exactly	O
1	O
argument	O
(	O
3	O
given	O
)	O

You	O
can	O
get	O
the	O
list	O
of	O
columns	O
with	O
:	O
#CODE	O

One	O
is	O
I	O
only	O
wanted	O
to	O
get	O
the	O
mean	O
of	O
the	O
next	O
rows	O
that	O
relate	O
to	O
the	O
same	O
group	O
.	O

By	O
default	O
this	O
will	O
add	O
a	O
column	O
of	O
integers	O
(	O
because	O
R	O
factors	O
are	O
encoded	O
as	O
integers	O
)	O
.	O

I	O
am	O
trying	O
to	O
calculate	O
the	O
percent	O
change	O
by	O
month	O
for	O
each	O
product	O
.	O

If	O
i	O
want	O
only	O
USA	O
Equities	O
vs	O
all	O
other	O
equity	O
and	O
not	O
the	O
enitre	O
89	O
columns	O
how	O
do	O
i	O
do	O
it	O
?	O

Pandas	O
:	O
Efficient	O
way	O
to	O
get	O
first	O
row	O
with	O
element	O
that	O
is	O
smaller	O
than	O
a	O
given	O
value	O

I	O
first	O
thought	O
this	O
was	O
a	O
spacing	O
issue	O
in	O
the	O
columns	O
values	O
,	O
so	O
I	O
replaced	O
them	O
with	O
underscores	O
,	O
but	O
it	O
also	O
doesn't	O
work	O
in	O
columns	O
which	O
only	O
contain	O
a	O
single	O
word	O
and	O
no	O
spaces	O
?	O

Are	O
you	O
getting	O
the	O
values	O
from	O
the	O
GUI	O
ok	O
,	O
but	O
your	O
calculations	O
are	O
returning	O
nothin	O
?	O

This	O
is	O
not	O
precisely	O
what	O
I'm	O
after	O
,	O
but	O
I	O
think	O
I'll	O
have	O
to	O
write	O
a	O
loop	O
top	O
get	O
that	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

I'm	O
having	O
a	O
problem	O
trying	O
to	O
get	O
a	O
character	O
count	O
column	O
of	O
the	O
string	O
values	O
in	O
another	O
column	O
,	O
and	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
it	O
efficiently	O
.	O

I	O
think	O
I	O
get	O
the	O
idea	O
.	O

How	O
can	O
I	O
get	O
the	O
position	O
of	O
index	O
`	O
18	O
`	O
?	O

When	O
using	O
`	O
groupy-apply	O
`	O
,	O
instead	O
of	O
dropping	O
the	O
group	O
key	O
index	O
using	O
:	O
#CODE	O

No	O
real	O
advantage	O
if	O
there	O
are	O
just	O
two	O
categories	O
:	O
#CODE	O

@USER	O
I	O
suspect	O
something	O
else	O
is	O
going	O
on	O
then	O
,	O
because	O
when	O
I	O
memory	O
profile	O
this	O
way	O
(	O
with	O
`	O
drop	O
`)	O
using	O
the	O
snippet	O
that	O
Michael	O
Laszlo	O
posted	O
,	O
I	O
do	O
not	O
see	O
memory	O
growth	O
.	O

Of	O
course	O
you	O
may	O
not	O
like	O
the	O
index	O
as	O
tuples	O
;	O
you	O
could	O
reset	O
the	O
index	O
within	O
the	O
list	O
comprehension	O
to	O
get	O
the	O
following	O
if	O
you	O
prefer	O
(	O
for	O
example	O
,	O
this	O
if	O
for	O
part	O
1	O
):	O
#CODE	O

Here	O
is	O
how	O
I	O
am	O
trying	O
to	O
get	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

memory	O
efficient	O
Python	O
(	O
pandas	O
)	O
aggregates	O
of	O
categories	O
from	O
one	O
csv	O
file	O
per	O
period	O

But	O
this	O
last	O
line	O
generate	O
the	O
error	O
message	O
:	O
`	O
no	O
item	O
named	O
timestamp	O
`	O
.	O

The	O
problem	O
here	O
,	O
well	O
the	O
biggest	O
one	O
,	O
is	O
that	O
your	O
`	O
data	O
`	O
is	O
string	O
,	O
not	O
valid	O
data	O
structure	O
,	O
same	O
thing	O
with	O
dictionary	O
inside	O
it	O
,	O
you	O
creating	O
strings	O
,	O
not	O
data	O
structures	O
.	O

I	O
also	O
want	O
to	O
create	O
a	O
new	O
column	O
that	O
shows	O
the	O
difference	O
in	O
days	O
between	O
the	O
end	O
and	O
begin	O
dates	O
.	O

In	O
the	O
next	O
column	O
(	O
B	O
)	O
,	O
I	O
want	O
to	O
create	O
an	O
indexed	O
series	O
that	O
begins	O
at	O
1000	O
based	O
on	O
the	O
percent	O
changes	O
.	O

I	O
can't	O
use	O
fixed	O
position	O
to	O
slice	O
it	O
.	O

This	O
gets	O
you	O
to	O
where	O
I	O
am	O
.	O

I	O
am	O
using	O
this	O
to	O
generate	O
nodes	O
in	O
a	O
graph	O
,	O
if	O
x1	O
,	O
x2	O
are	O
not	O
exactly	O
equal	O
,	O
networkx	O
recognizes	O
them	O
as	O
different	O
nodes	O
,	O
if	O
x1=x2	O
,	O
i	O
get	O
a	O
recombinant	O
tree	O
which	O
is	O
what	O
i	O
want	O
.	O

My	O
example	O
was	O
not	O
good	O
enough	O
,	O
as	O
your	O
script	O
smartly	O
took	O
the	O
size	O
from	O
the	O
length	O
of	O
the	O
df	O
.	O

iPython's	O
Rmagic	O
is	O
already	O
able	O
to	O
perform	O
an	O
automagic	O
conversion	O
between	O
the	O
two	O
in	O
a	O
number	O
of	O
situations	O
,	O
and	O
might	O
be	O
a	O
good	O
way	O
to	O
get	O
familiar	O
with	O
Python	O
.	O

So	O
if	O
your	O
dataset	O
is	O
really	O
large	O
,	O
perhaps	O
store	O
them	O
first	O
in	O
on-disk	O
database	O
/	O
HDF	O
rather	O
than	O
csv	O
file	O
and	O
sort	O
them	O
there	O
,	O
and	O
then	O
query	O
.	O

One	O
had	O
no	O
problem	O
at	O
all	O
(	O
the	O
xlsx	O
file	O
,	O
example	O
2	O
)	O
and	O
the	O
other	O
(	O
xls	O
,	O
example	O
1	O
)	O
had	O
a	O
difference	O
between	O
the	O
columns	O
.	O

then	O
with	O
the	O
`	O
sorted	O
`	O
function	O
and	O
`	O
datetime	O
`	O
module	O
(	O
remember	O
the	O
`	O
sorted	O
`	O
function	O
change	O
the	O
`	O
data	O
`	O
it	O
self	O
)	O
#CODE	O

BUT	O
,	O
I	O
get	O
the	O
"	O
SettingWithCopyWarning	O
"	O
:	O

This	O
is	O
machine	O
generated	O
data	O
.	O

You	O
can't	O
use	O
`	O
or	O
`	O
with	O
arrays	O
,	O
if	O
you	O
try	O
this	O
you	O
get	O
an	O
error	O
`	O
ValueError	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	O
is	O
ambiguous	O
.	O

Incidentally	O
,	O
this	O
is	O
the	O
same	O
result	O
that	O
you	O
would	O
get	O
with	O
the	O
Spearman	O
R	O
coefficient	O
as	O
well	O
.	O

How	O
could	O
I	O
do	O
to	O
have	O
exactly	O
one	O
calendar	O
year	O
between	O
dates	O
in	O
spite	O
of	O
leap	O
years	O
?	O

"	O
However	O
,	O
we	O
still	O
have	O
one	O
large	O
difference	O
.	O

All	O
that	O
remains	O
is	O
to	O
merged	O
the	O
contents	O
of	O
the	O
second-level	O
dictionaries	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
a	O
single	O
image	O
with	O
heatmaps	O
representing	O
the	O
correlation	O
of	O
features	O
of	O
data	O
points	O
for	O
each	O
label	O
separately	O
.	O

I	O
have	O
two	O
Series	O
which	O
have	O
a	O
format	O
equal	O
to	O
this	O
:	O
#CODE	O

However	O
,	O
when	O
I	O
do	O
the	O
following	O
,	O
the	O
error	O
does	O
not	O
show	O
up	O
and	O
I	O
get	O
the	O
expected	O
result	O
:	O
#CODE	O

As	O
brackets	O
are	O
part	O
of	O
the	O
regex	O
syntax	O
if	O
you're	O
trying	O
to	O
match	O
literal	O
brackets	O
you	O
need	O
to	O
escape	O
them	O
:	O
#CODE	O

Yeah	O
,	O
the	O
best	O
idea	O
I've	O
had	O
this	O
whole	O
time	O
was	O
to	O
actually	O
sign	O
up	O
to	O
SO	O
so	O
I	O
can	O
post	O
my	O
own	O
questions	O
instead	O
of	O
trying	O
to	O
funble	O
my	O
way	O
through	O
problems	O
by	O
patching	O
together	O
answers	O
to	O
other	O
peoples	O
questions	O
-	O
sometimes	O
what	O
I	O
need	O
just	O
isn't	O
covered	O
in	O
other	O
people's	O
questions	O
.	O

But	O
when	O
I	O
try	O
and	O
import	O
pandas	O
I	O
get	O
:	O
#CODE	O

However	O
,	O
I	O
can't	O
get	O
the	O
column	O
to	O
fill	O
up	O
with	O
the	O
appropriate	O
user	O
inputted	O
value	O
.	O

Also	O
I'm	O
not	O
getting	O
any	O
traceback	O
messages	O
but	O
I	O
think	O
I	O
have	O
an	O
idea	O
of	O
where	O
my	O
problem	O
may	O
be	O
.	O

Basically	O
I'm	O
trying	O
to	O
get	O
at	O
this	O
:	O
#CODE	O

So	O
this	O
is	O
not	O
a	O
fully	O
working	O
answer	O
,	O
but	O
maybe	O
it	O
can	O
be	O
extended	O
to	O
ultimatively	O
get	O
you	O
there	O
.	O

I	O
have	O
a	O
pandas	O
Series	O
holding	O
one	O
numpy	O
array	O
per	O
entry	O
(	O
same	O
length	O
for	O
all	O
entries	O
)	O
and	O
I	O
would	O
like	O
to	O
convert	O
this	O
to	O
a	O
2D	O
numpy	O
array	O
.	O

For	O
example	O
,	O
let's	O
say	O
I	O
want	O
to	O
select	O
50%	O
(	O
but	O
this	O
could	O
change	O
)	O
.	O

For	O
this	O
purpose	O
I	O
would	O
like	O
to	O
find	O
the	O
soonest	O
date	O
(	O
month	O
)	O
and	O
from	O
there	O
start	O
counting	O
months	O
and	O
their	O
averages	O
.	O

You've	O
changed	O
your	O
data	O
,	O
so	O
the	O
script	O
as	O
written	O
doesn't	O
work	O
.	O

Hopefully	O
you'll	O
get	O
an	O
answer	O
soon	O
.	O

This	O
should	O
get	O
you	O
to	O
the	O
point	O
in	O
your	O
code	O
where	O
you	O
start	O
dropping	O
columns	O
and	O
start	O
concatenating	O
.	O

So	O
,	O
if	O
k	O
were	O
equal	O
to	O
2	O
,	O
and	O
this	O
were	O
my	O
data	O
frame	O
:	O
#CODE	O

It	O
seems	O
as	O
though	O
the	O
second	O
approach	O
,	O
using	O
"	O
where	O
"	O
is	O
only	O
returning	O
data	O
from	O
the	O
last	O
few	O
appended	O
files	O
,	O
while	O
the	O
first	O
approach	O
is	O
returning	O
much	O
more	O
data	O
.	O

I	O
have	O
to	O
improve	O
this	O
to	O
get	O
rid	O
of	O
redundancy	O
,	O
and	O
I	O
am	O
not	O
sure	O
how	O
to	O
go	O
about	O
this	O
.	O

Are	O
you	O
able	O
to	O
post	O
raw	O
data	O
and	O
code	O
to	O
reproduce	O
this	O
issue	O
?	O

The	O
relative	O
size	O
between	O
consecutive	O
levels	O
.	O

This	O
will	O
potentially	O
cater	O
the	O
corner	O
cases	O
if	O
you	O
happen	O
to	O
have	O
conditions	O
like	O
:	O
"	O
value	O
"	O
360	O
then	O
+360	O
else	O
-360	O
but	O
the	O
sequence	O
of	O
the	O
update	O
will	O
cause	O
the	O
results	O
reapply	O
,	O
ie	O
.	O

I'll	O
put	O
an	O
example	O
of	O
what	O
I'm	O
suggesting	O
in	O
my	O
answer	O
.	O

But	O
you	O
said	O
you	O
want	O
only	O
the	O
time	O
points	O
from	O
the	O
longest	O
`	O
csv	O
`	O
.	O

replacing	O
this	O
in	O
code	O
just	O
drop	O
those	O
whole	O
rows	O
...	O

Thank	O
you	O
for	O
response	O
and	O
for	O
helping	O
me	O
get	O
to	O
next	O
level	O
of	O
pyhton	O
,	O
great	O
stuff	O
!	O

For	O
instance	O
,	O
you	O
can	O
insert	O
new	O
values	O
into	O
the	O
index	O
(	O
and	O
even	O
choose	O
what	O
value	O
it	O
should	O
have	O
):	O
#CODE	O

What	O
I	O
need	O
to	O
do	O
is	O
to	O
compute	O
the	O
average	O
temperature	O
for	O
every	O
run	O
,	O
averaging	O
all	O
the	O
temperature	O
measurements	O
belonging	O
to	O
a	O
run	O
.	O

@USER	O
I	O
get	O
`	O
Type	O
Error	O
:	O
'	O
bool	O
'	O
object	O
is	O
not	O
callable	O
`	O
when	O
I	O
do	O
that	O

For	O
your	O
specific	O
request	O
of	O
entries	O
between	O
12:00	O
to	O
13:00	O
for	O
every	O
single	O
day	O
,	O
you	O
can	O
fetch	O
the	O
rows	O
with	O
:	O
#CODE	O

Also	O
,	O
their	O
order	O
matters	O
(	O
they	O
are	O
sorted	O
by	O
decreasing	O
standard	O
deviation	O
across	O
rows	O
and	O
should	O
appear	O
in	O
this	O
order	O
in	O
the	O
heatmap	O
.	O
)	O

I'm	O
not	O
averse	O
the	O
reformatting	O
the	O
data	O
in	O
Pandas	O
-->	O
dumping	O
to	O
CSV	O
-->	O
importing	O
to	O
NetworkX	O
,	O
but	O
it	O
seems	O
as	O
if	O
I	O
should	O
be	O
able	O
to	O
generate	O
the	O
edges	O
from	O
the	O
index	O
and	O
the	O
nodes	O
from	O
the	O
values	O
.	O

What	O
is	O
the	O
simplest	O
way	O
to	O
get	O
a	O
sum	O
of	O
only	O
numbers	O
across	O
the	O
entire	O
frame	O
?	O

I	O
think	O
you	O
mean	O
a	O
Lorenz	O
plot	O
:	O
#URL	O
This	O
would	O
make	O
sense	O
then	O
as	O
it	O
requires	O
a	O
specific	O
preordering	O
of	O
the	O
data	O
.	O

Any	O
suggestions	O
?	O

Despite	O
the	O
title	O
,	O
similar	O
problems	O
can	O
occur	O
with	O
other	O
operating	O
systems	O
if	O
you	O
mix	O
32-bit	O
and	O
64-fit	O
versions	O
.	O

Use	O
`	O
select_as_coordinates	O
`	O
to	O
actually	O
execute	O
your	O
query	O
;	O
this	O
returns	O
an	O
`	O
Int64Index	O
`	O
of	O
the	O
row	O
number	O
(	O
the	O
coordinates	O
)	O
.	O

Somehow	O
create	O
a	O
mapping	O
so	O
that	O
instead	O
of	O
the	O
labels	O
being	O
29	O
,	O
30	O
etc	O
,	O
they	O
say	O
"	O
week	O
29	O
"	O
,	O
"	O
Week	O
30	O
"	O
etc	O
.	O

The	O
fix	O
you	O
describe	O
would	O
work	O
,	O
of	O
course	O
,	O
but	O
then	O
I	O
could	O
skip	O
pandas	O
all-together	O
and	O
directly	O
plot	O
the	O
results	O
of	O
my	O
individual	O
simulations	O
.	O

These	O
two	O
timezones	O
have	O
different	O
names	O
but	O
represent	O
the	O
same	O
thing	O
,	O
however	O

which	O
would	O
just	O
change	O
the	O
last	O
data	O
point	O
.	O

What	O
do	O
you	O
mean	O
by	O
reproducible	O
example	O
?	O

But	O
thought	O
i'd	O
make	O
it	O
clear	O
what	O
my	O
next	O
objective	O
was	O
,	O
in	O
case	O
someone	O
could	O
illuminate	O
a	O
better	O
method	O
to	O
get	O
there	O
.	O

What	O
output	O
do	O
you	O
get	O
when	O
you	O
just	O
enter	O
`	O
pd	O
`	O
in	O
the	O
console	O
?	O

When	O
I	O
train	O
on	O
each	O
label	O
I	O
get	O
et	O
better	O
than	O
73%	O
on	O
each	O
label	O
.	O

but	O
what	O
value	O
does	O
it	O
grab	O
when	O
indexing	O
?	O
in	O
other	O
words	O
,	O
if	O
i'm	O
just	O
testing	O
one	O
side	O
i'll	O
get	O
the	O
value	O
corresponding	O
to	O
that	O
row	O
if	O
true	O
.	O
since	O
both	O
sides	O
could	O
be	O
true	O
and	O
one	O
of	O
them	O
is	O
always	O
true	O
,	O
which	O
row's	O
values	O
will	O
be	O
selected	O
?	O

However	O
,	O
this	O
is	O
a	O
bug	O
as	O
you	O
should	O
get	O
an	O
error	O
.	O

guys	O
at	O
least	O
tell	O
me	O
why	O
i	O
am	O
getting	O
downvoted	O
?	O

In	O
some	O
cases	O
the	O
data	O
might	O
be	O
out	O
of	O
sync	O
which	O
makes	O
direct	O
comparisons	O
difficult	O
.	O

I	O
would	O
like	O
to	O
generate	O
a	O
matrix	O
which	O
contains	O
the	O
output	O
of	O
the	O
function	O
for	O
every	O
combination	O
of	O
X	O
and	O
Y	O
.	O

I'm	O
trying	O
to	O
get	O
the	O
data	O
to	O
in	O
the	O
column	O
"	O
Structure	O
"	O
to	O
repeat	O
the	O
row	O
labels	O
so	O
it	O
look	O
like	O
this	O
:	O
#CODE	O

decision	O
for	O
single	O
rows	O
to	O
get	O
converted	O
into	O
a	O
series	O
-	O
why	O
not	O
a	O

I	O
get	O
#CODE	O

More	O
info	O
as	O
requested	O
#CODE	O

[	O
Their	O
product	O
page	O
]	O
(	O
#URL	O
)	O
holds	O
the	O
answer	O
.	O

Next	O
,	O
you	O
wish	O
to	O
get	O
the	O
specific	O
groups	O
from	O
this	O
`	O
grouped	O
`	O
object	O
.	O

To	O
get	O
datetime64	O
that	O
uses	O
seconds	O
directly	O
:	O
#CODE	O

The	O
only	O
option	O
you	O
may	O
have	O
is	O
to	O
setup	O
your	O
data	O
structures	O
to	O
be	O
light	O
weight	O
so	O
each	O
worker	O
isn't	O
boated	O
by	O
redundant	O
copies	O
of	O
the	O
same	O
data	O
or	O
excessive	O
amounts	O
of	O
data	O
which	O
might	O
be	O
better	O
off	O
split	O
across	O
different	O
workers	O
.	O

Inplace	O
dropping	O
seems	O
more	O
like	O
idiomatic	O
pandas	O
to	O
me	O
than	O
making	O
a	O
copy	O
only	O
to	O
instantly	O
garbage	O
collect	O
the	O
now-defunct	O
original	O
.	O

And	O
fortunately	O
,	O
these	O
days	O
,	O
`	O
-pylab	O
`	O
has	O
been	O
deprecated	O
and	O
using	O
`	O
--	O
matplotlib	O
`	O
and	O
importing	O
`	O
pylab	O
`	O
manually	O
is	O
encouraged	O
.	O

Are	O
you	O
sure	O
you	O
don't	O
mean	O
`	O
range	O
(	O
1	O
,	O
len	O
(	O
DF	O
)):	O
`	O
?	O

Where	O
"	O
timeblock	O
"	O
#1	O
will	O
include	O
the	O
first	O
4:59	O
minutes	O
of	O
observation	O
period	O
#1	O
,	O
#2	O
will	O
include	O
5:00	O
to	O
9:59	O
minutes	O
...	O
through	O
to	O
25:00	O
and	O
over	O
,	O
for	O
each	O
observation	O
period	O
.	O

It	O
seems	O
like	O
I'm	O
maybe	O
getting	O
confused	O
between	O
the	O
underlying	O
data	O
and	O
views	O
on	O
it	O
.	O

When	O
I	O
try	O
specifying	O
index_col=0	O
,	O
as	O
some	O
examples	O
in	O
the	O
documentation	O
do	O
,	O
I	O
get	O
a	O
"	O
IndexError	O
:	O
list	O
index	O
out	O
of	O
range	O
"	O
error	O
,	O
which	O
was	O
a	O
solution	O
to	O
several	O
related	O
questions	O
but	O
for	O
some	O
reason	O
isn't	O
working	O
for	O
me	O
.	O

That's	O
what	O
I	O
thought	O
about	O
my	O
original	O
code	O
but	O
for	O
some	O
reason	O
when	O
I	O
check	O
len	O
(	O
Sframe	O
)	O
at	O
the	O
end	O
in	O
the	O
main	O
code	O
,	O
it	O
still	O
has	O
the	O
duplicate	O
values	O
even	O
if	O
the	O
conditional	O
statement	O
applies	O
option	O
2	O
to	O
remove	O
duplicates	O
.	O

The	O
main	O
thing	O
I	O
need	O
to	O
do	O
is	O
to	O
group	O
the	O
days	O
by	O
week	O
such	O
that	O
I	O
can	O
get	O
sum	O
of	O
the	O
data	O
to	O
be	O
by	O
week	O
.	O

To	O
move	O
the	O
third	O
row	O
to	O
the	O
first	O
,	O
you	O
can	O
create	O
an	O
index	O
moving	O
the	O
target	O
row	O
to	O
the	O
first	O
element	O
.	O

So	O
using	O
your	O
approach	O
,	O
how	O
can	O
I	O
:	O
1	O
)	O
plot	O
the	O
scores	O
as	O
a	O
histogram	O
in	O
a	O
memory-conscious	O
way	O
,	O
and	O
2	O
)	O
extract	O
the	O
scores	O
belonging	O
to	O
the	O
certain	O
cell	O
types	O
to	O
plot	O
those	O
as	O
well	O
?	O

I	O
have	O
a	O
CSV	O
file	O
,	O
I	O
wanted	O
to	O
filter	O
it	O
where	O
I	O
keep	O
just	O
rows	O
where	O
I	O
have	O
values	O
in	O
row	O
"	O
d	O
"	O
bigger	O
then	O
0	O
.	O

Simulations	O
can	O
be	O
repeated	O
for	O
different	O
scenarios	O
and	O
each	O
one	O
of	O
these	O
scenarios	O
will	O
produce	O
a	O
different	O
hourly	O
set	O
of	O
data	O
for	O
each	O
room	O
and	O
each	O
variable	O
.	O

To	O
get	O
around	O
this	O
,	O
I'm	O
passing	O
in	O
a	O
large	O
number	O
for	O
the	O
max_results	O
parameter	O
and	O
specifying	O
a	O
chunksize	O
.	O

You	O
get	O
back	O
a	O
float	O
because	O
each	O
row	O
contains	O
a	O
mix	O
of	O
`	O
float	O
`	O
and	O
`	O
int	O
`	O
types	O
.	O

Should	O
I	O
use	O
something	O
different	O
.	O

The	O
best	O
would	O
be	O
to	O
convert	O
that	O
one	O
file	O
to	O
a	O
an	O
actual	O
comma	O
(	O
semicolon	O
or	O
other	O
)	O
separated	O
file	O
or	O
make	O
sure	O
that	O
compound	O
values	O
are	O
quoted	O
(	O
"	O
Alabama	O
County	O
")	O
and	O
then	O
specify	O
the	O
quotechar	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
calculate	O
the	O
average	O
of	O
time	O
per	O
org	O
per	O
cluster	O
.	O

I	O
found	O
a	O
way	O
which	O
seems	O
very	O
inefficient	O
(	O
stacking	O
and	O
unstacking	O
which	O
will	O
create	O
many	O
many	O
columns	O
in	O
case	O
of	O
millions	O
of	O
categories	O
)	O
.	O

I	O
was	O
going	O
to	O
suggest	O
cumcount	O
and	O
tail	O
(	O
1	O
)	O
,	O
but	O
you're	O
after	O
something	O
else	O
(	O
these	O
would	O
be	O
much	O
faster	O
)	O
.	O

This	O
is	O
because	O
working	O
with	O
dictionaries	O
is	O
so	O
easy	O
and	O
thinking	O
of	O
them	O
like	O
simple	O
dicts	O
often	O
means	O
you	O
can	O
find	O
a	O
solution	O
to	O
an	O
issue	O
without	O
having	O
to	O
get	O
too	O
deep	O
into	O
pandas	O
.	O

If	O
you're	O
trying	O
to	O
slice	O
each	O
string	O
to	O
get	O
the	O
substring	O
from	O
5	O
to	O
7	O
,	O
you	O
need	O
a	O
`	O
:	O
`	O
,	O
not	O
a	O
`	O
,	O
`	O
:	O
#CODE	O

However	O
,	O
since	O
each	O
of	O
your	O
new	O
`	O
DataFrames	O
`	O
is	O
a	O
summary	O
of	O
a	O
single	O
customer	O
,	O
I	O
would	O
suggest	O
writing	O
one	O
function	O
that	O
can	O
return	O
all	O
of	O
your	O
desired	O
results	O
in	O
a	O
single	O
`	O
Series	O
`	O
.	O

I	O
want	O
to	O
get	O
statistics	O
of	O
debt_ratio	O
based	O
on	O
subgroups	O
of	O
market_capitalization	O
.	O

Any	O
ideas	O
why	O
this	O
error	O
might	O
be	O
showing	O
up	O
so	O
I	O
can	O
know	O
what	O
to	O
go	O
after	O
to	O
fix	O
?	O

I	O
believe	O
it	O
is	O
getting	O
at	O
what	O
I	O
want	O
.	O

I	O
haven't	O
done	O
any	O
stress	O
testing	O
but	O
I'd	O
imagine	O
this	O
could	O
get	O
slow	O
on	O
very	O
large	O
DataFrames	O
.	O

Is	O
there	O
a	O
quick	O
way	O
to	O
sort	O
my	O
data	O
by	O
a	O
given	O
column	O
that	O
only	O
takes	O
chunks	O
into	O
account	O
and	O
doesn't	O
require	O
loading	O
entire	O
datasets	O
in	O
memory	O
?	O

Take	O
a	O
look	O
at	O
the	O
regex	O
docs	O
.	O

Any	O
logic	O
requirements	O
(	O
like	O
comparing	O
elem+1	O
to	O
elem	O
)	O
should	O
be	O
in	O
your	O
question	O
so	O
there	O
is	O
no	O
confusion	O
.	O

I'd	O
like	O
to	O
get	O
a	O
list	O
as	O
`	O
[	O
'	O
abcd	O
'	O
,	O
'	O
ddse	O
'	O
,	O
'	O
123d	O
'	O
,	O
'	O
aaaaa*	O
']`	O
.	O

but	O
get	O
the	O
following	O
error	O
:	O

I	O
get	O
#CODE	O

Ideally	O
the	O
question	O
would	O
provide	O
a	O
self-contained	O
piece	O
of	O
code	O
generating	O
the	O
data	O
structure	O
,	O
or	O
even	O
just	O
something	O
like	O
`	O
df	O
=	O
[[	O
1	O
,	O
2	O
]	O
,	O
[	O
2	O
,	O
3	O
]	O
,	O
[	O
4	O
,	O
5	O
]]`	O
,	O
enough	O
to	O
try	O
to	O
get	O
an	O
answer	O
without	O
diving	O
into	O
panda	O
.	O

I'm	O
running	O
daily	O
simulations	O
in	O
a	O
batch	O
:	O
I	O
do	O
365	O
simluations	O
to	O
get	O
results	O
for	O
a	O
full	O
year	O
.	O

I	O
edited	O
my	O
answer	O
to	O
use	O
capwords	O
per	O
your	O
suggestion	O
,	O
that	O
fixed	O
the	O
problem	O
I	O
missed	O
where	O
it	O
capitalized	O
the	O
'	O
s	O
'	O
in	O
Guy's	O
Name	O
.	O

Does	O
not	O
work	O
exactly	O
right	O
since	O
categories	O
can	O
be	O
mixed	O
like	O
that	O
so	O
it	O
will	O
produce	O
more	O
duplications	O

Essentially	O
,	O
I	O
want	O
to	O
look	O
at	O
quintiles	O
(	O
since	O
there	O
are	O
5	O
days	O
in	O
a	O
business	O
week	O
)	O
rank	O
1	O
and	O
5	O
and	O
see	O
how	O
they	O
change	O
from	O
week	O
to	O
week	O
.	O

Doing	O
this	O
transformation	O
for	O
500,000	O
file	O
takes	O
time	O
.	O

The	O
requirement	O
isn't	O
easy	O
to	O
wrap	O
once	O
mind	O
around	O
,	O
so	O
sorry	O
If	O
I	O
am	O
not	O
being	O
clear	O
.	O

My	O
question	O
is	O
,	O
from	O
`	O
result	O
`	O
how	O
can	O
I	O
get	O
the	O
column	O
index	O
of	O
the	O
first	O
level	O
as	O
list	O
:	O
#CODE	O

This	O
function	O
will	O
then	O
work	O
out	O
the	O
maximum	O
,	O
minimum	O
,	O
and	O
return	O
rages	O
of	O
values	O
based	O
on	O
the	O
fact	O
I	O
want	O
5	O
categories	O
:	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
3	O
,	O
4	O
)	O
,	O
(	O
5	O
,	O
6	O
)	O
,	O
(	O
7	O
,	O
8)	O
,	O
(	O
9	O
,	O
10	O
)	O
.	O

Any	O
suggestions	O
of	O
a	O
better	O
way	O
?	O

In	O
addition	O
this	O
is	O
unlikely	O
to	O
be	O
only	O
time	O
I	O
have	O
to	O
do	O
this	O
so	O
being	O
able	O
to	O
change	O
the	O
numbers	O
and	O
columns	O
i'm	O
interested	O
in	O
would	O
be	O
good	O
.	O

And	O
at	O
that	O
time	O
i	O
think	O
the	O
state	O
becomes	O
bad	O
,	O
so	O
subsequent	O
calls	O
will	O
lead	O
to	O
exceptions	O
like	O
these	O
:	O
#CODE	O

Let	O
me	O
fix	O
that	O
and	O
return	O
to	O
this	O
issue	O
,	O
as	O
I	O
have	O
read	O
something	O
about	O
format	O
changes	O
between	O
10	O
and	O
12	O
.	O

All	O
the	O
data	O
,	O
columns	O
222	O
and	O
333	O
are	O
offset	O
as	O
required	O
,	O
but	O
it	O
isn't	O
even	O
the	O
same	O
size	O
as	O
the	O
output	O
in	O
the	O
first	O
order	O
.	O

But	O
I	O
can	O
only	O
get	O
this	O
:	O
#CODE	O

I	O
have	O
summary-level	O
data	O
of	O
the	O
count	O
of	O
people	O
by	O
age	O
group	O
,	O
city	O
,	O
income	O
and	O
the	O
industry	O
in	O
which	O
they	O
work	O
,	O
or	O
in	O
this	O
case	O
four	O
dimensions	O
.	O

@USER	O
:	O
Yes	O
,	O
it	O
can	O
as	O
I	O
said	O
,	O
"	O
They	O
all	O
suck	O
in	O
different	O
ways	O
"	O
.	O

Error	O
:	O
list	O
indices	O
must	O
be	O
integers	O
,	O
not	O
Series	O

Just	O
so	O
that	O
it	O
does	O
not	O
fail	O
silently	O
if	O
the	O
wrong	O
kind	O
of	O
data	O
is	O
passed	O
in	O
.	O

If	O
you	O
want	O
to	O
take	O
advantage	O
of	O
NumPy	O
/	O
Pandas	O
to	O
perform	O
fast	O
(	O
er	O
)	O
calculations	O
you	O
must	O
keep	O
the	O
data	O
in	O
a	O
NumPy	O
array	O
or	O
Pandas	O
NDFrame	O
.	O

Your	O
benchmark	O
is	O
actually	O
too	O
small	O
to	O
show	O
the	O
real	O
difference	O
.	O

I	O
was	O
able	O
to	O
resolve	O
this	O
by	O
opening	O
/	O
closing	O
a	O
connection	O
each	O
time	O
i	O
need	O
to	O
execute	O
a	O
query	O
.	O

I	O
used	O
a	O
chunksize	O
of	O
4	O
to	O
make	O
the	O
grouping	O
noticeable	O
on	O
the	O
small	O
dataset	O
;	O
you'll	O
want	O
to	O
change	O
it	O
to	O
90000	O
for	O
your	O
real	O
dataset	O
.	O

It's	O
mostly	O
trial	O
and	O
error	O
for	O
me	O
,	O
plus	O
knowledge	O
that	O
you	O
can	O
do	O
a	O
lot	O
just	O
with	O
```	O
rank	O
```	O
and	O
```	O
count	O
```	O
,	O
which	O
are	O
both	O
pretty	O
fast	O
.	O

And	O
those	O
columns	O
which	O
have	O
differing	O
values	O
:	O
#CODE	O

(	O
I	O
will	O
use	O
logistic	O
regression	O
and	O
random	O
forest	O
to	O
do	O
the	O
prediction	O
,	O
which	O
support	O
sparse	O
matrix	O
.	O
)	O
Is	O
there	O
anyway	O
to	O
efficiently	O
slicing	O
a	O
sparseDataFrame	O
or	O
for	O
the	O
whole	O
process	O
I	O
am	O
doing	O
,	O
it	O
should	O
be	O
improved	O
in	O
anyway	O
?	O

I	O
looked	O
here	O
,	O
but	O
when	O
I	O
ran	O
that	O
in	O
iPython	O
Notebook	O
,	O
I	O
don't	O
get	O
anything	O
.	O

does	O
not	O
produce	O
any	O
difference	O
in	O
terms	O
of	O
file	O
size	O
than	O
...	O

You	O
will	O
either	O
have	O
to	O
split	O
the	O
table	O
up	O
or	O
choose	O
another	O
storage	O
format	O
.	O

To	O
get	O
the	O
number	O
of	O
groups	O
you	O
can	O
use	O
the	O
ngroups	O
attribute	O
:	O
#CODE	O

this	O
does	O
not	O
seem	O
to	O
work	O
and	O
I	O
am	O
not	O
sure	O
if	O
it	O
just	O
is	O
not	O
possible	O
,	O
I	O
can	O
always	O
generate	O
separate	O
dictionaries	O
from	O
a	O
master	O
dictionary	O
to	O
get	O
around	O
having	O
to	O
update	O
data	O
in	O
multiple	O
locations	O

I	O
want	O
to	O
compare	O
these	O
two	O
data	O
frames	O
by	O
row	O
based	O
on	O
column	O
Value	O
and	O
keep	O
the	O
row	O
from	O
first	O
or	O
second	O
depending	O
where	O
the	O
Value	O
is	O
bigger	O
.	O

Then	O
,	O
you	O
can	O
run	O
`	O
sudo	O
port	O
install	O
py27-pandas	O
`	O
to	O
get	O
Python	O
and	O
all	O
of	O
the	O
dependencies	O
installed	O
.	O

See	O
the	O
shape	O
method	O
of	O
the	O
input	O
array	O
,	O
and	O
you	O
should	O
get	O
something	O
like	O
`	O
(	O
N	O
,	O
)`	O
and	O
not	O
`	O
(	O
N	O
,	O
1	O
)`	O
.	O

So	O
,	O
will	O
the	O
lower	O
value	O
always	O
come	O
first	O
,	O
or	O
could	O
that	O
change	O
?	O

and	O
get	O
the	O
following	O
error	O
:	O

In	O
order	O
to	O
have	O
the	O
index	O
the	O
exact	O
same	O
as	O
the	O
first	O
example	O
you'd	O
need	O
to	O
change	O
to	O
int	O
from	O
float	O
.	O

The	O
graph	O
bit	O
is	O
sorted	O
but	O
the	O
part	O
i'm	O
finding	O
hard	O
is	O
the	O
fact	O
the	O
column	O
headers	O
can	O
change	O
so	O
picking	O
up	O
their	O
data	O
without	O
manually	O
calling	O
them	O
is	O
something	O
I'm	O
unable	O
to	O
do	O
.	O

If	O
pytables	O
used	O
msgpack	O
it	O
would	O
be	O
easier	O
for	O
other	O
languages	O
to	O
read	O
the	O
data	O
but	O
obviously	O
their	O
target	O
is	O
python	O
.	O

The	O
company	O
name	O
may	O
be	O
variable	O
length	O
,	O
it	O
will	O
however	O
always	O
be	O
after	O
the	O
first	O
`	O
\s	O
`	O

To	O
get	O
the	O
same	O
form	O
of	O
broadcasting	O
to	O
occur	O
like	O
the	O
diagram	O
above	O
shows	O
we	O
have	O
to	O
decompose	O
to	O
numpy	O
arrays	O
which	O
then	O
become	O
anonymous	O
data	O
:	O
#CODE	O

I	O
want	O
to	O
generate	O
a	O
plot	O
showing	O
these	O
dates	O
graphically	O
.	O

How	O
is	O
it	O
possible	O
to	O
get	O
the	O
label	O
of	O
value	O
'	O
12	O
'	O
?	O

You	O
shall	O
note	O
that	O
`	O
and	O
`	O
and	O
`	O
or	O
`	O
are	O
not	O
appropriate	O
for	O
a	O
vector	O
of	O
booleans	O
,	O
use	O
`	O
`	O
and	O
`	O
|	O
`	O
instead	O
.	O

No	O
,	O
this	O
table	O
is	O
used	O
by	O
a	O
lot	O
of	O
other	O
code	O
(	O
mostly	O
C#	O
)	O
,	O
I	O
am	O
just	O
doing	O
some	O
data	O
analysis	O
on	O
it	O
from	O
Python	O
,	O
so	O
I'm	O
not	O
in	O
a	O
position	O
to	O
change	O
the	O
semantics	O
/	O
data	O
structure	O
.	O

#	O
Valid	O
positions	O
in	O
output	O
array	O
to	O
be	O
changed	O

Is	O
there	O
a	O
bug	O
in	O
my	O
code	O
or	O
is	O
there	O
another	O
reason	O
for	O
the	O
huge	O
computation	O
speed	O
difference	O
between	O
those	O
two	O
lines	O
of	O
code	O
?	O

I	O
also	O
used	O
a	O
longer	O
window	O
because	O
there	O
were	O
only	O
15	O
values	O
per	O
array	O
but	O
you	O
seemed	O
to	O
be	O
planning	O
on	O
using	O
the	O
last	O
50	O
days	O
.	O

I	O
am	O
trying	O
to	O
get	O
the	O
data	O
into	O
the	O
following	O
shape	O
:	O
#CODE	O

I	O
then	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
:	O
plot	O
a	O
heatmap	O
of	O
x	O
,	O
y	O
and	O
the	O
colour	O
is	O
the	O
z	O
value	O
..	O

I'm	O
also	O
running	O
python	O
3.4	O
and	O
I	O
didn't	O
get	O
any	O
warning	O
when	O
I	O
ran	O
your	O
code	O
exactly	O
as	O
is	O
.	O

This	O
does	O
it	O
in	O
a	O
one	O
liner	O
but	O
is	O
not	O
so	O
readable	O
,	O
basically	O
it	O
tests	O
where	O
the	O
value	O
counts	O
for	O
each	O
column	O
is	O
equal	O
to	O
1	O
,	O
filters	O
the	O
resultant	O
list	O
out	O
and	O
uses	O
the	O
index	O
as	O
a	O
boolean	O
indec	O
:	O
#CODE	O

I	O
still	O
think	O
pandas	O
is	O
not	O
correctly	O
handling	O
your	O
empty	O
column	O
and	O
you	O
end	O
up	O
with	O
either	O
with	O
5	O
columns	O
,	O
or	O
with	O
6	O
columns	O
,	O
but	O
shifted	O
one	O
to	O
the	O
left	O
.	O

However	O
,	O
I	O
am	O
receiving	O
this	O
warning	O
`	O
/	O
usr	O
/	O
local	O
/	O
lib	O
/	O
python2.7	O
/	O
dist-packages	O
/	O
pandas	O
/	O
core	O
/	O
index	O
.	O

It	O
looks	O
like	O
this	O
changed	O
at	O
some	O
point	O
;	O
maybe	O
he	O
has	O
an	O
old	O
version	O
of	O
pandas	O
where	O
S	O
and	O
Sec	O
are	O
no	O
good	O
.	O

I	O
have	O
also	O
heard	O
of	O
Orange	O
library	O
for	O
imputation	O
,	O
but	O
haven't	O
had	O
a	O
chance	O
to	O
use	O
it	O
yet	O
.	O

I	O
managed	O
to	O
find	O
how	O
this	O
is	O
almost	O
done	O
:	O
#CODE	O

For	O
this	O
purpose	O
you	O
need	O
pandas	O
-	O
most	O
popular	O
python	O
package	O
for	O
working	O
with	O
timeseries	O
and	O
another	O
analytic	O
data	O
.	O

well	O
,	O
what	O
I	O
am	O
trying	O
to	O
do	O
is	O
to	O
have	O
all	O
my	O
results	O
ready	O
out	O
of	O
mysql	O
,	O
and	O
then	O
do	O
different	O
types	O
of	O
merging	O
to	O
get	O
my	O
plots	O
.	O

I	O
agree	O
with	O
Chang	O
that	O
it	O
would	O
help	O
to	O
have	O
a	O
very	O
clear	O
example	O
of	O
how	O
the	O
exact	O
alignment	O
should	O
be	O
.	O

You	O
mention	O
in	O
update	O
2	O
above	O
that	O
you	O
want	O
to	O
get	O
the	O
columns	O
and	O
the	O
only	O
way	O
is	O
opening	O
the	O
hdf	O
.	O

So	O
where	O
those	O
indices	O
don't	O
match	O
up	O
(	O
50	O
,	O
and	O
51	O
)	O
,	O
you	O
get	O
`	O
NaN	O
`	O
as	O
I	O
would	O
hope	O
.	O

However	O
,	O
if	O
I	O
save	O
it	O
as	O
a	O
csv	O
file	O
and	O
reload	O
it	O
again	O
,	O
I	O
got	O
error	O
message	O
and	O
the	O
plot	O
is	O
not	O
quite	O
right	O
either	O
,	O
#CODE	O

My	O
main	O
goal	O
is	O
to	O
match	O
the	O
index	O
value	O
from	O
`	O
ds2	O
`	O
into	O
`	O
ds1	O
`	O
and	O
replace	O
it	O
with	O
corresponding	O
value	O
,	O
so	O
the	O
output	O
would	O
look	O
like	O
#CODE	O

That	O
would	O
be	O
a	O
possibility	O
but	O
the	O
problem	O
is	O
that	O
each	O
frame5	O
has	O
a	O
different	O
index	O
.	O

First	O
,	O
you	O
need	O
some	O
kind	O
of	O
mapping	O
of	O
what	O
makes	O
up	O
each	O
level	O
.	O

PyTables	O
3.1	O
was	O
just	O
released	O
that	O
changes	O
the	O
file	O
caching	O
mechanism	O
at	O
least	O
on	O
a	O
lower	O
HDF5	O
version	O
,	O
do	O
to	O
see	O
your	O
version	O
:	O
#CODE	O

backfilling	O
data	O
from	O
one	O
column	O
into	O
another	O

Also	O
,	O
I	O
know	O
I	O
am	O
missing	O
patterns	O
that	O
may	O
be	O
useful	O
because	O
if	O
a	O
pattern	O
exists	O
between	O
Variable_1	O
and	O
Variable_2	O
and	O
Variable_3	O
and	O
Variable_4	O
are	O
missing	O
completely	O
at	O
random	O
,	O
then	O
concatenating	O
them	O
as	O
strings	O
will	O
not	O
capture	O
the	O
pattern	O
between	O
Variable_1	O
and	O
Variable_2	O
.	O

Do	O
you	O
know	O
what	O
is	O
the	O
difference	O
in	O
this	O
case	O
between	O
both	O
?	O

The	O
goal	O
is	O
to	O
take	O
the	O
2x2	O
piece	O
of	O
df	O
with	O
index	O
(	O
4	O
,	O
5	O
)	O
and	O
columns	O
(	O
'	O
date	O
'	O
,	O
'	O
val	O
')	O
and	O
replace	O
it	O
with	O
a	O
same-shaped	O
,	O
same-typed	O
2x2	O
block	O
.	O

Convert	O
Matrix	O
format	O
to	O
Column	O
in	O
Pandas	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

I	O
want	O
to	O
drop	O
all	O
values	O
after	O
index	O
`	O
5	O
`	O
because	O
it	O
has	O
no	O
values	O
,	O
but	O
not	O
index	O
`	O
2	O
`	O
,	O
`	O
3	O
`	O
.	O

You	O
should	O
be	O
able	O
to	O
uninstall	O
Anaconda	O
(	O
it	O
is	O
only	O
a	O
directory	O
)	O
to	O
reverse	O
any	O
changes	O
.	O

Since	O
it	O
is	O
a	O
very	O
large	O
data	O
frame	O
,	O
I	O
think	O
it	O
might	O
be	O
inefficient	O
to	O
do	O
a	O
loop	O
and	O
row	O
by	O
row	O
extraction	O
.	O

Since	O
the	O
NumPy	O
array	O
has	O
no	O
index	O
,	O
there	O
should	O
be	O
no	O
"	O
Unalignable	O
boolean	O
Series	O
"	O
problem	O
.	O

My	O
guess	O
is	O
that	O
I	O
am	O
either	O
not	O
applying	O
the	O
functions	O
correctly	O
for	O
a	O
column	O
or	O
the	O
values	O
I	O
am	O
getting	O
arent	O
integers	O
.	O

which	O
gives	O
you	O
your	O
date	O
as	O
a	O
list	O
arranged	O
in	O
the	O
order	O
of	O
importance	O
...	O

I'm	O
not	O
sure	O
what	O
the	O
difference	O
was	O
.	O

The	O
mongodb	O
collection	O
contains	O
sensor	O
values	O
tagged	O
with	O
date	O
and	O
time	O
.	O

This	O
should	O
not	O
make	O
any	O
difference	O
.	O

I	O
would	O
like	O
to	O
split	O
that	O
file	O
into	O
files	O
of	O
len	O
(	O
index	O
)	O
=	O
2	O
,	O
using	O
linux	O
:	O
#CODE	O

You	O
can	O
coerce	O
the	O
response	O
into	O
a	O
data	O
frame	O
after	O
you	O
get	O
it	O
.	O

Oh	O
wait	O
,	O
your	O
matrix	O
must	O
already	O
be	O
in	O
the	O
form	O
of	O
differences	O
from	O
the	O
mean	O
(	O
by	O
column	O
)	O
?	O

Any	O
suggestions	O
?	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
am	O
curious	O
why	O
doing	O
`	O
unique_df	O
[	O
i	O
]	O
=	O
"	O
AAA	O
"`	O
no	O
longer	O
modifies	O
the	O
data	O
frame	O
values	O
.	O

Unfortunately	O
I	O
get	O
an	O
error	O
,	O
and	O
the	O
shading	O
doesn't	O
work	O
.	O

The	O
range	O
of	O
the	O
values	O
in	O
`	O
megaball	O
`	O
are	O
from	O
1	O
to	O
25	O
,	O
and	O
this	O
line	O
of	O
code	O
:	O
#CODE	O

yes	O
it	O
works	O
fine	O
but	O
I	O
need	O
to	O
drop	O
'	O
2014-07-16	O
14:24	O
'	O
thnx	O

If	O
you	O
have	O
more	O
pressing	O
things	O
to	O
do	O
,	O
you	O
could	O
temporarily	O
rename	O
it	O
out	O
of	O
the	O
way	O
to	O
get	O
through	O
your	O
installs	O
,	O
then	O
rename	O
it	O
back	O
.	O

As	O
you	O
can	O
see	O
,	O
the	O
lines	O
overlap	O
perfectly	O
for	O
the	O
days	O
where	O
there	O
is	O
data	O
:	O
no	O
original	O
data	O
is	O
'	O
changed	O
'	O
.	O

I	O
have	O
a	O
massive	O
file	O
with	O
per	O
timestamp	O
survey	O
data	O
from	O
about	O
thousands	O
of	O
different	O
people	O
and	O
over	O
20	O
different	O
locations	O
.	O

So	O
if	O
you	O
have	O
a	O
million	O
items	O
that	O
,	O
on	O
average	O
,	O
belong	O
to	O
three	O
categories	O
each	O
,	O
then	O
you	O
need	O
storage	O
for	O
the	O
million	O
items	O
plus	O
three	O
million	O
references	O
.	O

Given	O
how	O
the	O
sample	O
was	O
built	O
,	O
there	O
was	O
a	O
need	O
to	O
weight	O
adjust	O
the	O
respondent	O
data	O
so	O
that	O
not	O
every	O
one	O
is	O
deemed	O
as	O
"	O
equal	O
"	O
when	O
performing	O
the	O
analysis	O
.	O

What	O
I'm	O
hoping	O
to	O
achieve	O
is	O
knowing	O
where	O
the	O
first	O
/	O
last	O
row	O
of	O
trimmed	O
data	O
is	O
located	O
so	O
I	O
can	O
set	O
up	O
a	O
for-loop	O
to	O
go	O
through	O
the	O
data	O
and	O
perform	O
mathematical	O
calculations	O
with	O
the	O
values	O
and	O
then	O
send	O
those	O
results	O
back	O
into	O
new	O
columns	O
attached	O
directly	O
to	O
the	O
same	O
date	O
as	O
the	O
date	O
in	O
question	O
.	O

Ultimately	O
I	O
want	O
to	O
be	O
able	O
to	O
loop	O
through	O
the	O
json	O
to	O
display	O
the	O
dates	O
and	O
corresponding	O
values	O
,	O
but	O
I	O
cant	O
do	O
that	O
until	O
this	O
error	O
is	O
no	O
longer	O
happening	O
.	O

They	O
are	O
however	O
extremely	O
useful	O
once	O
you	O
get	O
to	O
grips	O
with	O
them	O
.	O

I	O
know	O
the	O
values	O
within	O
the	O
CSV	O
are	O
not	O
all	O
"	O
NaN	O
"	O
so	O
why	O
does	O
the	O
output	O
looks	O
like	O
this	O
and	O
how	O
can	O
I	O
get	O
the	O
correct	O
output	O
with	O
the	O
numbers	O
in	O
reach	O
of	O
the	O
rows	O
?	O

I've	O
filtered	O
my	O
data	O
as	O
suggested	O
here	O
:	O
With	O
Pandas	O
in	O
Python	O
,	O
select	O
the	O
highest	O
value	O
row	O
for	O
each	O
group	O
#CODE	O

I	O
was	O
able	O
to	O
get	O
the	O
more	O
precise	O
value	O
in	O
my	O
previous	O
environment	O
by	O
doing	O
the	O
incremental	O
update	O
to	O
cumulative	O
mean	O
instead	O
of	O
taking	O
a	O
batch	O
sum	O
and	O
divide	O
.	O

Then	O
use	O
the	O
`	O
~	O
`	O
to	O
flip	O
the	O
bools	O
.	O

