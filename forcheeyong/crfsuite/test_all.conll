i	O
am	O
facing	O
this	O
strange	O
behaviour	O
,	O
i	O
got	O
a	O
HDFStore	B-API
containing	O
DataFrames	O
.	O

What	O
happens	O
if	O
you	O
do	O
`	O
ax.legend()	B-API
`	O
and	O
`	O
plt.draw()	B-API
`	O
?	O

E.g.	O
,	O
you	O
can	O
call	O
`	O
pandas.isnull	B-API
(	O
the_frame	O
)`	O
.	O

do	O
this	O
:	O
``	O
df.ix	B-API
[	O
0	O
,	O
'	O
a	O
']	O
+=	O
1	O
``	O
.	O

Using	O
unstack()	B-API
is	O
one	O
way	O
.	O

Clearly	O
I'm	O
missing	O
something	O
as	O
to	O
why	O
df.loc	B-API
[:	O
,	O
tuple	O
]	O
is	O
different	O
than	O
df	O
[	O
tuple	O
]	O
.	O

datetime	O
dtypes	B-API
in	O
pandas	O
read_csv	B-API

If	O
you're	O
using	O
version	O
`	O
0.17.0	O
`	O
or	O
higher	O
then	O
you	O
can	O
call	O
this	O
using	O
`	O
.dt	B-API
.strftime	B-API
`	O
which	O
is	O
vectorised	O
:	O
#CODE	O

Looks	O
like	O
you	O
need	O
histogram()	B-API
of	O
months	O
.	O

You	O
want	O
`	O
.dt	B-API
.time	B-API
`	O
see	O
the	O
docs	O
for	O
some	O
more	O
examples	O
of	O
things	O
under	O
the	O
`	O
.dt	B-API
`	O
accessor	O
.	O

pd.rolling_apply	B-API
(	O
df.exma	O
,	O
2	O
,	O
(	O
df.alpha	O
*	O
df.exma.shift	O
(	O
1	O
))	O
+	O
((	O
1	O
-	O
df.alpha	O
)	O
*	O
df.outperf	O
))	O

I	O
suggest	O
not	O
using	O
`	O
file	O
`	O
for	O
your	O
`	O
open()	B-API
`	O
object	O
.	O

If	O
you	O
want	O
every	O
value	O
from	O
row	O
2	O
you	O
can	O
just	O
do	O
`	O
df.iloc	B-API
[	O
2	O
]`	O
or	O
`	O
df.iloc	B-API
[	O
2	O
]	O
.values	B-API
`	O
respectively	O
.	O

dtype	B-API
:	O
object	O

Check	O
out	O
the	O
glob	O
module	O
and	O
pandas	O
read_csv()	B-API
and	O
concat()	B-API

`	O
np.dtype	B-API
(	O
'	O
datetime64	O
[	O
ns	O
]')`	O
and	O
`	O
np.dtype	B-API
(	O
'	O
M	O
8[	O
ns	O
]')`	O
:	O
#CODE	O

Doing	O
`	O
pd.crosstab	B-API
(	O
rows	O
=[	O
df	O
[	O
'	O
A	O
']	O
,	O
df	O
[	O
'	O
B	O
']]	O
,	O
cols	O
=[	O
df	O
[	O
'	O
C	O
']]	O
,	O
margins=True	O
,	O
aggfu	O

DataFrame.drop_duplicates	B-API
and	O
DataFrame.drop	B-API
not	O
removing	O
rows	O

`	O
pd.rolling_mean	B-API
(	O
df	O
,	O
num	O
)	O
.dropna()	B-API
.plot()	B-API
`	O

This	O
is	O
a	O
replacement	O
for	O
`	O
np.array_equal	B-API
`	O
which	O
is	O
broken	O
for	O
nan	O
positional	O
detections	O
(	O
and	O
object	O
dtypes	B-API
)	O
.	O

It	O
looks	O
like	O
`	O
pd.unique	O
`	O
does	O
not	O
respect	O
the	O
`	O
datetime64	O
`	O
dtype	B-API
:	O
#CODE	O

df	O
[	O
'	O
NumActivity	O
']	O
=	O
pd.factorize	B-API
(	O
df	O
[	O
'	O
Activity	O
'])	O
[	O
0	O
]	O
+1	O
works	O
.	O

calling	O
`	O
as_matrix()	B-API
`	O
on	O
`	O
df	O
`	O
returns	O
a	O
`	O
numpy.ndarray	B-API
`	O
object	O
#CODE	O

And	O
using	O
pandas.Series.map	B-API
:	O
#CODE	O

works	O
,	O
but	O
I	O
think	O
`	O
df.select_dtypes	B-API
`	O
should	O
be	O
preferred	O
since	O
it	O
uses	O
the	O

Then	O
you	O
can	O
access	O
the	O
relevant	O
rows	O
using	O
groupby's	O
`	O
get_group	B-API
`	O
:	O
#CODE	O

It's	O
not	O
in	O
the	O
online	O
docs	O
,	O
you	O
have	O
to	O
check	O
the	O
`	O
help	O
(	O
pd.bdate_range	B-API
)`	O
:)	O

dtype	B-API
:	O
int64	O
`	O

dtype	B-API
:	O
object	O

df.drop	B-API
([	O
'	O
one	O
'	O
,	O
'	O
two	O
'	O
,	O
'	O
three	O
']	O
,	O
axis=1	O
,	O
inplace=True	O
)	O

I	O
tried	O
make	O
these	O
'	O
filenames	O
'	O
dataFrames	O
using	O
`	O
pd.DataFrame	B-API
`	O
but	O
wasn't	O
able	O
to	O
do	O
so	O
.	O

Try	O
using	O
.loc	B-API
[	O
row_index	O
,	O
col_indexer	O
]	O
=	O
value	O
instead	O
`	O
.	O

Good	O
question	O
,	O
`	O
plt.hist	B-API
(	O
hour_list	O
,	O
bins=	O
np.arange	B-API
(	O
24	O
)	O
-	O
0.5	O
)`	O
will	O
work	O
.	O

what	O
is	O
the	O
-	O
0.00343	O
(	O
ah	O
I	O
see	O
,	O
a	O
consequence	O
of	O
not	O
using	O
right=False	O
)	O
,	O
pd.cut	B-API
very	O
neat	O

2	O
)	O
Alternatively	O
,	O
don't	O
create	O
levels	O
for	O
`	O
aa	O
`	O
and	O
`	O
bb	O
`	O
using	O
`	O
as_index=False	O
`	O
and	O
`	O
pd.merge	B-API
`	O
#CODE	O

`	O
df.groupby	B-API
(	O
level	O
=[	O
'	O
major	O
'	O
,	O
'	O
minor	O
'])	O
.mean()	B-API
.dropna()	B-API
`	O

Argh	O
,	O
that	O
still	O
gives	O
me	O
`	O
dtype	B-API
(	O
'	O
datetime64	O
[	O
us	O
]')`	O
.	O

or	O
better	O
yet	O
,	O
just	O
don't	O
specify	O
a	O
dtype	B-API
:	O
#CODE	O

I	O
was	O
trying	O
to	O
do	O
this	O
with	O
`	O
df	O
[	O
df.shift()	B-API
!	O
=	O
df	O
]	O
.dropna()	B-API
.reset_index	B-API
(	O
drop=True	O
)`	O
but	O
`	O
shift()	B-API
`	O
is	O
not	O
behaving	O
in	O
the	O
way	O
I	O
meant	O
.	O

`	O
median_search_query	O
=	O
np.median	B-API
(	O
df.srch_query_affinity_score	O
)`	O

Example	O
using	O
df.asfreq	B-API
(	O
'	O
Q	O
')	O
:	O
#CODE	O

```	O
In	O
[	O
15	O
]:	O
pd.to_timedelta	B-API
(	O
s.str.replace	O
(	O
'	O
hrs	O
'	O
,	O
'	O
h	O
'))	O

Python	O
Dict	O
,	O
Lambda	O
x	O
,	O
map()	B-API
doesn't	O
work	O

I	O
have	O
tried	O
to	O
modify	O
your	O
answer	O
to	O
make	O
it	O
an	O
example	O
about	O
modifying	O
two	O
columns	O
like	O
I	O
asked	O
(	O
for	O
one	O
the	O
`	O
df.loc	B-API
[:	O
,	O
(	O
'	O
A	O
'	O
,	O
'	O
a	O
')]`	O
works	O
like	O
a	O
charm	O
)	O
.	O

I	O
thought	O
np.ma.average	B-API
is	O
just	O
what	O
I	O
need	O
,	O
but	O
that	O
also	O
gives	O
me	O
NaN	O
as	O
a	O
result	O
.	O

Although	O
```	O
pandas.cut()	B-API
```	O
is	O
the	O
better	O
and	O
more	O
general	O
answer	O
,	O
it	O
looks	O
like	O
in	O
this	O
case	O
you	O
could	O
do	O
```	O
df	O
[	O
'	O
B	O
']	O
=	O
(	O
df.A	O
/	O
500	O
)	O
.astype	B-API
(	O
int	O
)	O
+	O
1	O
```	O
.	O

outcome	O
:	O
displays	O
good	O
in	O
`	O
df.head()	B-API
`	O
,	O
but	O
reverts	O
to	O
scientific	O
notation	O
upon	O
coercion	O
to	O
string	O
concatenation	O
using	O
+	O
operator	O

The	O
`	O
ewma	B-API
`	O
case	O
can	O
be	O
solved	O
similarly	O
:	O
`	O
df.groupby	B-API
(	O
level=0	O
)	O
.apply	B-API
(	O
lambda	O
x	O
:	O
pd.ewma	B-API
(	O
x	O
,	O
com=2	O
))`	O

I	O
didn't	O
realise	O
that	O
`	O
.resample()	B-API
`	O
wasn't	O
inplace	O
!	O

I	O
can't	O
think	O
of	O
a	O
way	O
to	O
overload	O
the	O
`	O
.loc	B-API
`	O
method	O
properly	O
!	O

Do	O
I	O
have	O
to	O
specific	O
the	O
dtypes	B-API
to	O
make	O
this	O
work	O
?	O

It	O
appears	O
that	O
the	O
`	O
applymap	B-API
`	O
is	O
the	O
culprit	O
here	O
:-)	O

"	O
"	O
.join	B-API
(	O
header.split()	O
)	O
.split	B-API
(	O
'	O
')	O

I	O
want	O
df.append	B-API
(	O
df2	O
)	O
in	O
this	O
case	O
to	O
be	O
:	O
#CODE	O

I	O
completely	O
forgot	O
about	O
.loc	B-API

It	O
sounds	O
like	O
maybe	O
you	O
want	O
`	O
pandas.concat	B-API
`	O
?	O

Is	O
there	O
anything	O
in	O
pandas	O
that	O
is	O
the	O
opposite	O
to	O
`	O
.dropna()	B-API
`	O
?	O

Datetimes	O
are	O
handled	O
if	O
they	O
can	O
properly	O
be	O
converted	O
(	O
e.g.	O
they	O
have	O
a	O
dtype	B-API
of	O
'	O
datetime64	O
[	O
ns	O
]'	O
,	O
notably	O
datetimes.date	O
are	O
NOT	O
handled	O
(	O
NaN	O
are	O
a	O
different	O
story	O
and	O
depending	O
on	O
usage	O
can	O
cause	O
the	O
entire	O
column	O
type	O
to	O
be	O
mishandled	O
)	O

The	O
`	O
args	O
`	O
and	O
`	O
kwargs	O
`	O
parameters	O
were	O
added	O
to	O
`	O
rolling_apply	B-API
`	O
in	O
Pandas	O
version	O
0.14.0	O
.	O

try	O
``	O
df	O
[	O
'	O
LastName	O
']	O
=	O
df.apply	B-API
(	O
updateDataframe	O
)``	O
,	O
but	O
even	O
better	O
is	O
:	O
``	O
df.ix	B-API
[	O
df	O
[	O
'	O
LastName	O
']	O
==	O
'	O
Joe	O
'	O
,	O
'	O
LastName	O
']	O
=	O
'	O
Black	O
'``	O

There	O
should	O
be	O
no	O
problem	O
;	O
`	O
plt.plot	B-API
`	O
and	O
`	O
plt.fill_between	B-API
`	O
accept	O
arrays	O
of	O
dtype	B-API
`	O
datetime64	O
[	O
ns	O
]`	O
just	O
fine	O
.	O

You	O
could	O
try	O
pandas	O
pivot()	B-API
.	O

What	O
happens	O
if	O
you	O
do	O
`	O
ax.legend()	B-API
`	O
and	O
`	O
plt.draw()	B-API
`	O
?	O

Pandas	O
Python	O
read_csv	B-API
error_bad_lines	O
producing	O
shell	O
feedback	O

Though	O
in	O
some	O
respects	O
,	O
the	O
np.clip	B-API
or	O
np.max	O
solutions	O
are	O
more	O
easily	O
read	O
,	O
I	O
think	O
this	O
is	O
the	O
most	O
precise	O
answer	O
to	O
my	O
original	O
question	O
.	O

Did	O
you	O
try	O
setting	O
the	O
delimiter	O
to	O
semicolon	O
in	O
your	O
`	O
read_csv	B-API
`	O
call	O
?	O

In	O
[	O
8]	O
:	O
%timeit	O
df	O
[	O
'	O
r	O
']	O
=	O
df	O
[[	O
'	O
minor	O
'	O
,	O
'	O
major	O
']]	O
.abs()	B-API
.max	B-API
(	O
axis=1	O
)	O

I	O
guess	O
my	O
confusion	O
stems	O
from	O
the	O
fact	O
that	O
the	O
Series.value_counts	B-API
doesn't	O
seem	O
to	O
fit	O
into	O
the	O
arguments	O
required	O
by	O
the	O
df.apply	B-API
method	O
.	O

The	O
problem	O
is	O
that	O
`	O
a	O
`	O
is	O
dtype	B-API
`	O
object	O
`	O
.	O

try	O
`	O
video_base	O
=	O
pd.merge	B-API
(	O
df_one	O
,	O
df_two	O
[[	O
'	O
count_watched_yeterday	O
']]	O
,	O
how=	O
'	O
left	O
'	O
,	O
left_index=True	O
,	O
right_index=True	O
)`	O

ax.xaxis.set_major_locator	O
(	O
MultipleLocator	B-API
(	O
10	O
))	O

Actually	O
I	O
think	O
you	O
want	O
:	O
`	O
pd.concat	B-API
([	O
df_may	O
,	O
df_jun	O
]	O
,	O
axis=0	O
,	O
ignore_index=True	O
)`	O

Maybe	O
there	O
is	O
a	O
better	O
approach	O
thats	O
takes	O
advantage	O
of	O
features	O
of	O
the	O
Pandas.DataFrame	B-API
class	O
?	O

Try	O
`	O
df.loc	B-API
[	O
'	O
a	O
']`	O
instead	O
.	O

Then	O
this	O
should	O
work	O
:	O
`	O
df	O
[	O
'	O
Gene.Symbol	O
']	O
=	O
df	O
[	O
'	O
Gene.Symbol	O
']	O
.str	B-API
.strip()	B-API
.str	B-API
.upper()	B-API
`	O

then	O
[	O
`	O
df.groupy	O
(	O
'	O
key	O
')	O
.agg	B-API
(	O
...	O
)`]	O
(	O
#URL	O
)	O
might	O
be	O
what	O
you	O
are	O
looking	O
for	O
.	O

DataFrame.apply	B-API
in	O
python	O
pandas	O
alters	O
both	O
original	O
and	O
duplicate	O
DataFrames	O

you	O
might	O
want	O
to	O
try	O
`	O
df.iloc	B-API
[	O
0	O
]`	O
rather	O
than	O
`	O
df.iloc	B-API
(	O
0	O
)`	O
.	O

and	O
,	O
`	O
ts.asfreq	O
(	O
'	O
H	O
'	O
,	O
method=	O
'	O
ffill	B-API
')`	O
to	O
have	O
hourly	O
frequency	O
.	O

I	O
tried	O
something	O
like	O
`	O
set_index	B-API
`	O
,	O
`	O
pd.factorize()	B-API
`	O
and	O
`	O
index_col	O
`	O
but	O
they	O
do	O
not	O
work	O
.	O

So	O
df	O
=	O
df.reindex()	B-API
results	O
in	O
the	O
same	O
indexing	O
...	O

I	O
think	O
this	O
solution	O
will	O
execute	O
faster	O
than	O
using	O
iterrows()	B-API
,	O
but	O
I'm	O
not	O
sure	O
.	O

One	O
option	O
using	O
`	O
df.reindex	B-API
`	O
:	O
#CODE	O

I	O
am	O
using	O
dtype	B-API
as	O
suggested	O
in	O
the	O
answer	O
there	O
.	O

You	O
might	O
be	O
interested	O
in	O
`	O
pd.cut	B-API
`	O
:	O
#CODE	O

df.groupby	B-API
(	O
'	O
A	O
')	O
.size()	B-API
.apply	B-API
(	O
lambda	O
x	O
:	O
float	O
(	O
x	O
)	O
/	O
df.groupby	B-API
(	O
'	O
A	O
')	O
.size()	B-API
.sum()	B-API
*100	O
)	O

`	O
df.apply()	B-API
`	O
works	O
off	O
a	O
single	O
row	O
,	O
`	O
shift()	B-API
`	O
doesnt	O
seem	O
to	O
work	O
.	O

Could	O
you	O
try	O
this	O
:	O
`	O
df	O
[	O
'	O
GC	O
content	O
']	O
=	O
df	O
[[	O
'	O
oligo_sequence	O
']]	O
.apply	B-API
(	O
lambda	O
row	O
:	O
GC	O
(	O
row	O
)	O
,	O
axis=1	O
)`	O

And	O
,	O
`	O
pd.eval()	B-API
`	O
works	O
well	O
with	O
expressions	O
containing	O
large	O
arrays	O
#CODE	O

The	O
eventual	O
goal	O
being	O
to	O
arrange	O
hierarchically	O
to	O
weekday	B-API
hour-range	O
,	O
something	O
like	O
:	O
#CODE	O

Reading	O
about	O
`	O
applymap	B-API
`	O
I	O
wondered	O
if	O
there	O
is	O
a	O
similar	O
way	O
of	O
defining	O
and	O
applying	O
operators	O
that	O
work	O
on	O
pairs	O
of	O
dataframes	O
.	O

Executing	O
your	O
code	O
with	O
pandas	O
0.16.2	O
yielded	O
the	O
two	O
columns	O
with	O
dtype	B-API
datetime64	O
[	O
ns	O
]	O
.	O

`	O
df	O
[	O
'	O
price_trend	O
']	O
.apply	B-API
(	O
lambda	O
x	O
:[	O
i.split	O
(	O
'	O
:	O
')	O
for	O
i	O
in	O
x	O
])`	O
#CODE	O

Here	O
the	O
new	O
value	O
AND	O
the	O
existing	O
dtype	B-API
of	O
the	O
column	O
matters	O
.	O

how	O
about	O
using	O
the	O
`	O
pd.DataFrame.drop_duplicates()	B-API
`	O
method	O
?	O

dtype	B-API
:	O
timedelta64	O
[	O
ns	O
]```	O

`	O
gg	O
[	O
'	O
cumt	O
']	O
=	O
gg.apply	O
(	O
lambda	O
x	O
:	O
x	O
[	O
'	O
tavg	O
']	O
+	O
x	O
[	O
'	O
tavg	O
']	O
.shift	B-API
(	O
1	O
)	O
[	O
1	O
:]	O
)`	O

"	O
"	O
.join	B-API
(	O
header.split()	O
)	O
.split	B-API
(	O
'	O
')	O

print	O
'	O
\nAfter	O
replace\n	O
'	O
,	O
df.replace	B-API
(	O
{	O
'	O
c1	O
'	O
:	O
c1_fromto	O
,	O
'	O
c2	O
'	O
:	O
c2_fromto	O
}	O
)	O

Profiling	O
shows	O
the	O
culprit	O
is	O
obviously	O
`	O
B.ix	O
[	O
row	O
[	O
0	O
]]	O
.irow	B-API
(	O
np.searchsorted	B-API
(	O
B.ts	O
[	O
row	O
[	O
0	O
]]	O
,	O
row	O
[	O
2	O
])))`	O
.	O

At	O
the	O
first	O
step	O
I	O
used	O
`	O
df.T	B-API
`	O
to	O
transpose	O
the	O
dataframe	O
,	O
and	O
tried	O
something	O
like	O
`	O
df.value_counts()	O
`	O
,	O
however	O
I'd	O

Replace	O
NaN	O
in	O
a	O
dataframe	O
with	O
random	O
values	O

I	O
want	O
to	O
replace	O
all	O
the	O
NaN	O
with	O
some	O
random	O
values	O
like	O
.	O

#CODE	O

Resample	B-API
function	O
throwing	O
error	O
with	O
Twitter	O
Data	O

I	O
then	O
try	O
to	O
resample	O
for	O
analysis	O
#CODE	O

I'm	O
writing	O
several	O
pivot	O
tables	O
using	O
pandas	O
.	O

For	O
many	O
of	O
them	O
,	O
I	O
need	O
to	O
return	O
unique	O
values	O
.	O

In	O
a	O
two-dimensional	O
pivot	O
table	O
,	O
the	O
below	O
code	O
works	O
as	O
it	O
should	O
.	O

When	O
I	O
add	O
a	O
third	O
dimension	O
,	O
the	O
code	O
returns	O
the	O
count	O
rather	O
than	O
the	O
unique	O
count	O
.	O

I	O
suspect	O
this	O
has	O
something	O
to	O
do	O
with	O
the	O
aggfunc	B-API
,	O
but	O
can't	O
determine	O
to	O
what	O
it	O
should	O
be	O
changed	O
.	O

Use	O
a	O
groupby	B-API
to	O
get	O
at	O
each	O
combination	O
of	O
`	O
col_1	O
`	O
and	O
`	O
col_3	O
`	O
,	O
then	O
unstack	B-API
to	O
get	O
the	O
`	O
col_3	O
`	O
values	O
as	O
columns	O
:	O
#CODE	O

Python	O
pandas	O
merge	O
or	O
concat	O
dataframes	O

The	O
data	O
is	O
for	O
2	O
products	O
(	O
BBG.XAMS.UL.S_pnl_pos_cost	O
and	O
BBG.XAMS.UNA.S_pnl_pos_cost	O
)	O
by	O
date	O
,	O
in	O
the	O
future	O
there	O
will	O
be	O
more	O
products	O
.	O

I	O
want	O
to	O
concat	O
or	O
merge	O
(	O
not	O
sure	O
which	O
)	O
the	O
list	O
of	O
dataframes	O
into	O
one	O
data	O
frame	O
(	O
called	O
result	O
)	O
so	O
they	O
look	O
like	O
:	O
#CODE	O

where	O
axis	O
is	O
the	O
date	O
.	O

It	O
looks	O
like	O
the	O
data	O
is	O
merged	O
by	O
date	O
,	O
but	O
I	O
am	O
missing	O
the	O
data	O
for	O
the	O
week	O
beginning	O
2015-03-23	O
.	O

My	O
current	O
concat	O
result	O
dataframe	O
looks	O
like	O
:	O
#CODE	O

Try	O
using	O
axis=0	O
.	O

This	O
should	O
concat	O
column-wise	O
,	O
assuming	O
each	O
dataframe	O
has	O
the	O
same	O
column	O
names	O
.	O

possible	O
duplicate	O
of	O
[	O
Pandas	O
join	B-API
/	O
merge	B-API
/	O
concat	B-API
two	O
dataframes	O
]	O
(	O
#URL	O
)	O

Also	O
,	O
how	O
do	O
you	O
join	O
this	O
back	O
to	O
original	O
dataframe	O
?	O

We	O
can	O
resample	O
this	O
to	O
days	O
;	O
it'll	O
be	O
a	O
much	O
longer	O
timeseries	O
,	O
of	O
course	O
,	O
but	O
memory	O
is	O
cheap	O
and	O
I'm	O
lazy	O
:	O
#CODE	O

How	O
do	O
I	O
merge	O
the	O
birth	O
rate	O
back	O
to	O
the	O
original	O
table	O
?	O

Indexes	O
aren't	O
compatible	O
...	O

Turns	O
out	O
size	O
isn't	O
such	O
an	O
issue	O
.	O

Python	O
&	O
Pandas	O
:	O
Unable	O
to	O
drop	O
columns	O

I	O
try	O
to	O
drop	O
the	O
data	O
,	O
but	O
it	O
reports	O
some	O
column	O
does	O
not	O
exist	O
.	O

#CODE	O

@USER	O
,	O
that's	O
possible	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
deal	O
with	O
it	O
.	O

In	O
my	O
previous	O
experience	O
with	O
pandas	O
,	O
it	O
will	O
automatically	O
turn	O
the	O
second	O
`	O
Q	O
`	O
into	O
`	O
Q.1	O
`	O
when	O
reading	O
the	O
data	O
.	O

However	O
,	O
in	O
my	O
case	O
,	O
it	O
failed	O
to	O
do	O
it	O
,	O
and	O
I	O
don't	O
know	O
why	O
.	O

However	O
,	O
This	O
it	O
cannot	O
`	O
drop	B-API
`	O
`	O
NCDC	O
`	O
either	O
.	O

You	O
can	O
then	O
drop	O
your	O
columns	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
take	O
a	O
given	O
row	O
from	O
a	O
DataFrame	O
and	O
prepend	O
or	O
append	O
to	O
the	O
same	O
DataFrame	O
.	O

Rather	O
than	O
concat	O
I	O
would	O
just	O
assign	O
directly	O
to	O
the	O
df	O
after	O
`	O
shift	O
`	O
ing	O
,	O
then	O
use	O
`	O
iloc	B-API
`	O
to	O
reference	O
the	O
position	O
you	O
want	O
to	O
assign	O
the	O
row	O
,	O
you	O
have	O
to	O
call	O
`	O
squeeze	B-API
`	O
so	O
that	O
you	O
assign	O
just	O
the	O
values	O
and	O
lose	O
the	O
original	O
index	O
value	O
otherwise	O
it'll	O
raise	O
a	O
`	O
ValueError	O
`	O
:	O
#CODE	O

To	O
insert	O
at	O
the	O
end	O
:	O
#CODE	O

I'm	O
not	O
sure	O
exactly	O
what	O
you're	O
expecting	O
,	O
but	O
you	O
could	O
replace	O
your	O
lists	O
with	O
numpy	O
arrays	O
(	O
I	O
don't	O
think	O
it'll	O
improve	O
your	O
specific	O
code	O
):	O
#CODE	O

What	O
is	O
the	O
best	O
way	O
for	O
me	O
to	O
get	O
this	O
data	O
into	O
Pandas	O
?	O

Is	O
there	O
a	O
standard	O
way	O
I	O
could	O
use	O
`	O
read_table	B-API
`	O
or	O
some	O
similar	O
function	O
to	O
read	O
this	O
file	O
directly	O
?	O

Should	O
I	O
write	O
a	O
script	O
to	O
insert	O
commas	O
where	O
all	O
the	O
column	O
breaks	O
are	O
and	O
then	O
read	O
it	O
in	O
as	O
CSV	O
?	O

(	O
I'd	O
just	O
do	O
the	O
latter	O
,	O
but	O
I'm	O
also	O
interested	O
in	O
becoming	O
better	O
with	O
Pandas	O
so	O
if	O
there's	O
an	O
out-of-the-box	O
way	O
I'd	O
like	O
to	O
know	O
it	O
.	O
)	O

Any	O
ideas	O
on	O
how	O
to	O
get	O
this	O
file	O
to	O
load	O
?	O

Unfortunately	O
I	O
can't	O
just	O
strip	O
out	O
accents	O
,	O
as	O
I	O
have	O
to	O
interface	O
with	O
software	O
that	O
requires	O
the	O
proper	O
name	O
,	O
and	O
I	O
have	O
a	O
ton	O
of	O
files	O
to	O
format	O
(	O
not	O
just	O
the	O
one	O
)	O
.	O

Thanks	O
!	O

pls	O
show	O
your	O
input	O
and	O
what	O
is	O
the	O
expected	O
output	O
,	O
in	O
a	O
copy-pastable	O
form	O
.	O

What	O
you	O
are	O
doing	O
is	O
very	O
inefficient	O
.	O

A	O
groupby	B-API
should	O
try	O
to	O
use	O
vectorized	O
functions	O
when	O
possible	O
.	O

Then	O
join	O
them	O
up	O
at	O
the	O
end	O
.	O

Hi	O
Tom	O
,	O
it	O
doesn't	O
look	O
like	O
this	O
works	O
.	O

It	O
outputs	O
just	O
one	O
array	O
and	O
is	O
equivalent	O
to	O
df2	O
[	O
'	O
array	O
']	O
.sum()	B-API
.	O

But	O
you	O
have	O
given	O
me	O
an	O
idea	O
with	O
apply	B-API
.	O

Let	O
me	O
see	O
if	O
I	O
can	O
figure	O
something	O
out	O
.	O

You	O
need	O
`	O
apply	B-API
(	O
your_func	O
,	O
axis=1	O
)`	O
to	O
work	O
on	O
a	O
row-by-row	O
basis	O
.	O

#CODE	O

Another	O
way	O
would	O
be	O
to	O
call	O
`	O
unique	B-API
`	O
on	O
the	O
transpose	O
of	O
your	O
df	O
:	O
#CODE	O

Drop	O
values	O
satisfying	O
condition	O
plus	O
arbitrary	O
number	O
of	O
next	O
values	O
in	O
a	O
pandas	O
DataFrame	O

So	O
my	O
final	O
goal	O
is	O
to	O
drop	B-API
values	O
in	O
one	O
column	O
of	O
a	O
`	O
pandas	O
`	O
`	O
DataFrame	O
`	O
according	O
to	O
some	O
condition	O
on	O
another	O
column	O
of	O
the	O
same	O
`	O
DataFrame	O
`	O
,	O
plus	O
several	O
next	O
values	O
e.g.	O
:	O
#CODE	O

So	O
this	O
will	O
drop	O
the	O
records	O
where	O
the	O
condition	O
is	O
satisfied	O
,	O
but	O
how	O
do	O
I	O
drop	O
the	O
next	O
3	O
records	O
after	O
the	O
condition	O
was	O
satisfied	O
too	O
?	O

My	O
desired	O
output	O
would	O
look	O
something	O
like	O
this	O
:	O
#CODE	O

We	O
can	O
use	O
the	O
boolean	O
condition	O
index	O
to	O
slice	O
the	O
df	O
using	O
`	O
loc	B-API
`	O
and	O
set	O
the	O
following	O
values	O
:	O
#CODE	O

Panda's	O
boxplot	O
but	O
not	O
showing	O
the	O
box	O

Note	O
,	O
`	O
showbox	O
`	O
and	O
`	O
whiskerprops	O
`	O
are	O
the	O
`	O
kwds	O
`	O
of	O
boxplot	O
,	O
which	O
are	O
in	O
turn	O
passed	O
to	O
`	O
matplotlib.boxplot	B-API
`	O
.	O

Applying	O
aggregate	B-API
function	O
on	O
columns	O
of	O
Pandas	O
pivot	O
table	O

I	O
generated	O
the	O
following	O
pivot	O
table	O
via	O
taking	O
maximum	O
of	O
values	O
in	O
`	O
Z	O
`	O
column	O
:	O
#CODE	O

Here's	O
a	O
fairly	O
general	O
solution	O
you	O
can	O
apply	O
to	O
multiple	O
columns	O
.	O

The	O
'	O
To	O
'	O
column	O
doesn't	O
need	O
to	O
be	O
rounded	O
,	O
I	O
just	O
included	O
it	O
for	O
the	O
generality	O
of	O
two	O
columns	O
rather	O
than	O
one	O
:	O
#CODE	O

428	O
base	O
,	O
mult	O
=	O
_gfc	O
(	O
freq	O
)	O

-->	O
429	O
return	O
tslib.dt64arr_to_periodarr	O
(	O
data.view	O
(	O
'	O
i8	O
')	O
,	O
base	O
,	O
tz	O
)	O

I	O
could	O
do	O
a	O
left	O
merge	B-API
,	O
but	O
I	O
would	O
end	O
up	O
with	O
a	O
huge	O
file	O
.	O

Is	O
there	O
any	O
way	O
to	O
add	O
specific	O
rows	O
from	O
df2	O
to	O
df1	O
using	O
merge	B-API
?	O

Unclear	O
why	O
you	O
think	O
a	O
left	O
merge	B-API
would	O
produce	O
a	O
huge	O
file	O
,	O
by	O
performing	O
a	O
left	O
merge	B-API
on	O
the	O
product	O
id	O
you	O
are	O
stating	O
that	O
you	O
are	O
only	O
interested	O
in	O
matches	O
in	O
the	O
product_id	O
column	O
only	O

Just	O
perform	O
a	O
left	O
`	O
merge	B-API
`	O
on	O
'	O
product_id	O
'	O
column	O
:	O
#CODE	O

What	O
would	O
be	O
the	O
Python	O
equivalent	O
?	O

I	O
cannot	O
think	O
of	O
a	O
way	O
to	O
translate	O
this	O
where	O
statement	O
into	O
pandas	O
syntax	O
.	O

The	O
only	O
way	O
I	O
can	O
think	O
of	O
is	O
to	O
add	O
an	O
arbitrary	O
field	O
to	O
people_usa	O
(	O
e.g.	O
`	O
people_usa	O
[	O
'	O
dummy	O
']	O
=1	O
`)	O
,	O
do	O
a	O
left	O
join	B-API
,	O
then	O
take	O
only	O
the	O
records	O
where	O
'	O
dummy	O
'	O
is	O
nan	O
,	O
then	O
delete	O
the	O
dummy	O
field	O
-	O
which	O
seems	O
a	O
bit	O
convoluted	O
.	O

Does	O
this	O
work	O
only	O
on	O
the	O
index	O
of	O
the	O
dataframe	O
?	O

I'd	O
like	O
the	O
option	O
to	O
specify	O
the	O
field	O
(	O
s	O
)	O
to	O
apply	O
this	O
to	O

Is	O
there	O
any	O
easy	O
way	O
to	O
do	O
this	O
if	O
you	O
have	O
multiple	O
columns	O
to	O
check	O
/	O
join	O
?	O

You	O
could	O
do	O
a	O
`	O
merge	B-API
`	O
and	O
then	O
eliminate	O
the	O
rows	O
that	O
exist	O
in	O
the	O
merged	O
df	O
otherwise	O
you'd	O
have	O
to	O
build	O
a	O
boolean	O
condition	O
for	O
all	O
the	O
columns	O
you	O
want	O
to	O
compare	O
but	O
presumably	O
when	O
checking	O
the	O
multiple	O
columns	O
you're	O
stating	O
that	O
it's	O
unique	O
for	O
those	O
columns	O
,	O
correct	O
?	O

For	O
instance	O
it's	O
not	O
a	O
match	O
if	O
say	O
col1	O
and	O
col2	O
match	O
but	O
col3	O
does	O
not	O

Yes	O
merge	B-API
is	O
what	O
I	O
have	O
been	O
doing	O
but	O
it	O
feels	O
like	O
a	O
hassle	O
.	O

I've	O
come	O
up	O
with	O
this	O
,	O
using	O
itertools	B-API
,	O
to	O
find	O
mid-day	O
timestamps	O
and	O
group	O
them	O
by	O
date	O
,	O
and	O
now	O
I'm	O
coming	O
up	O
short	O
trying	O
to	O
apply	O
imap	O
to	O
find	O
the	O
means	O
.	O

#CODE	O

Since	O
not	O
sure	O
what	O
your	O
end	O
output	O
should	O
look	O
like	O
,	O
just	O
create	O
a	O
time-based	O
grouper	B-API
manually	O
(	O
this	O
is	O
essentially	O
a	O
resample	B-API
)	O
,	O
but	O
doesn't	O
do	O
anything	O
with	O
the	O
final	O
results	O
(	O
its	O
just	O
a	O
list	O
of	O
the	O
aggregated	O
values	O
)	O
#CODE	O

You	O
can	O
get	O
reasonable	O
fancy	O
here	O
and	O
say	O
return	O
a	O
pandas	O
object	O
(	O
and	O
potentially	O
`	O
concat	O
`	O
them	O
)	O
.	O

and	O
I	O
want	O
to	O
pivot	O
it	O
like	O
this	O
:	O
#CODE	O

I	O
am	O
calling	O
a	O
function	O
from	O
within	O
a	O
'	O
for	O
each	O
loop	O
'	O
which	O
attempts	O
to	O
insert	O
values	O
into	O
a	O
Pandas	O
DataFrame	O
based	O
on	O
a	O
specified	O
column	O
start	O
and	O
end	O
location	O
.	O

The	O
function	O
is	O
this	O
:	O
#CODE	O

My	O
issue	O
is	O
that	O
despite	O
the	O
same	O
starting	O
conditions	O
when	O
I	O
call	O
this	O
function	O
it	O
seems	O
to	O
generate	O
a	O
list	O
of	O
inconsistent	O
length	O
.	O

e.g.	O
with	O
values	O
of	O
srowb	O
=	O
1	O
and	O
erowb	O
=	O
18	O
it	O
will	O
generate	O
a	O
list	O
(	O
tmp_brollb	O
)	O
which	O
has	O
either	O
len	B-API
(	O
tmp_brollb	O
)	O
=	O
17	O
or	O
len	B-API
(	O
tmp_brollb	O
)	O
=	O
18	O

Use	O
`	O
max	B-API
`	O
and	O
check	O
for	O
equality	O
using	O
`	O
eq	B-API
`	O
and	O
cast	O
the	O
boolean	O
df	O
to	O
int	O
using	O
`	O
astype	B-API
`	O
,	O
this	O
will	O
convert	O
`	O
True	O
`	O
and	O
`	O
False	O
`	O
to	O
`	O
1	O
`	O
and	O
`	O
0	O
`	O
:	O
#CODE	O

Thanks	O
@USER	O
.	O

Did	O
you	O
try	O
my	O
original	O
post	O
?	O

I	O
would	O
be	O
interested	O
to	O
know	O
how	O
much	O
time	O
this	O
one	O
is	O
taking	O
compared	O
to	O
yours	O
?	O

`	O
for	O
i	O
in	O
range	O
(	O
len	B-API
(	O
df	O
)):	O
...	O

df.loc	B-API
[	O
i	O
]	O
[	O
df.loc	B-API
[	O
i	O
]	O
.idxmax	B-API
(	O
axis=1	O
)]	O
=	O
1	O
...	O

df.loc	B-API
[	O
i	O
]	O
[	O
df.loc	B-API
[	O
i	O
]	O
!	O
=	O
1	O
]	O
=	O
0	O
`	O

I	O
am	O
trying	O
to	O
normalize	O
the	O
missing	O
values	O
in	O
matrix	O
.	O

Here	O
is	O
the	O
code	O
.	O

#CODE	O

Last	O
line	O
should	O
replace	O
the	O
values	O
in	O
dataset1	O
by	O
mean	O
values	O
from	O
`	O
ds2_mean	O
[	O
1	O
]`	O
.	O

But	O
it	O
does	O
not	O
do	O
.	O

Anything	O
wrong	O
here	O
?	O

And	O
after	O
that	O
can	O
I	O
replace	O
NaN	O
with	O
the	O
average	O
value	O
of	O
it's	O
neighbours	O
in	O
dataset1	O
?	O

it	O
does	O
wrong	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

It	O
should	O
replace	O
all	O
values	O
of	O
x	O
in	O
ds1	O
by	O
mapped	O
value	O
.	O

But	O
this	O
also	O
does	O
not	O
do	O
it	O

Sorry	O
can	O
you	O
explain	O
clearer	O
,	O
what	O
are	O
you	O
mapping	O
from	O
what	O
to	O
what	O
exactly	O
?	O

By	O
default	O
fillna	B-API
will	O
use	O
the	O
index	O
so	O
how	O
do	O
you	O
want	O
the	O
mapping	O
from	O
`	O
ds2	O
`	O
to	O
map	O
to	O
the	O
missing	O
values	O
in	O
`	O
ds1	O
`	O
?	O

Are	O
you	O
wanting	O
to	O
map	O
using	O
the	O
values	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
as	O
the	O
index	O
lookup	O
?	O

So	O
use	O
the	O
index	O
from	O
`	O
ds1	O
`	O
find	O
value	O
in	O
`	O
ds2	O
[	O
0	O
]`	O
and	O
return	O
`	O
ds2	O
[	O
1	O
]`	O
?	O

yes	O
,	O
I	O
want	O
to	O
use	O
the	O
index	O
from	O
ds1	O
find	O
value	O
in	O
ds2	O
[	O
0	O
]	O
and	O
replace	O
it	O
with	O
ds2	O
[	O
1	O
]"	O
sorry	O
for	O
inconvenience	O

I	O
want	O
to	O
add	O
a	O
new	O
column	O
which	O
contains	O
values	O
based	O
on	O
df	O
[	O
'	O
diff	O
']	O

When	O
using	O
`	O
DataFrame.apply	B-API
`	O
if	O
you	O
use	O
`	O
axis=0	O
`	O
it	O
applies	O
the	O
condition	O
through	O
columns	O
,	O
to	O
use	O
`	O
apply	B-API
`	O
to	O
go	O
through	O
each	O
row	O
,	O
you	O
need	O
`	O
axis=1	O
`	O
.	O

But	O
given	O
that	O
,	O
you	O
can	O
use	O
`	O
Series.apply	B-API
`	O
instead	O
of	O
`	O
DataFrame.apply	B-API
`	O
on	O
the	O
`'	O
diff	B-API
'`	O
series	O
.	O

Example	O
-	O
#CODE	O

You	O
can	O
just	O
set	O
all	O
the	O
values	O
that	O
meet	O
your	O
criteria	O
rather	O
than	O
looping	O
over	O
the	O
df	O
by	O
calling	O
`	O
apply	B-API
`	O
so	O
the	O
following	O
should	O
work	O
and	O
as	O
it's	O
vectorised	O
will	O
scale	O
better	O
for	O
larger	O
datasets	O
:	O
#CODE	O

this	O
will	O
set	O
all	O
rows	O
that	O
meet	O
the	O
criteria	O
,	O
the	O
problem	O
using	O
`	O
apply	B-API
`	O
is	O
that	O
it's	O
just	O
syntactic	O
sugar	O
for	O
a	O
`	O
for	O
`	O
loop	O
and	O
where	O
possible	O
this	O
should	O
be	O
avoided	O
where	O
a	O
vectorised	O
solution	O
exists	O
.	O

Then	O
you	O
can	O
`	O
stack	B-API
`	O
(	O
first	O
by	O
`'	O
Marker	O
'`	O
then	O
by	O
`'	O
mrk	O
'`)	O
:	O
#CODE	O

Python	O
DataFrame	O
-	O
apply	O
different	O
calculations	O
due	O
to	O
a	O
column's	O
value	O

You	O
could	O
do	O
this	O
using	O
2	O
`	O
loc	B-API
`	O
calls	O
:	O
#CODE	O

There	O
are	O
two	O
reasons	O
whiskers	O
length	O
vary	O
from	O
one	O
boxplot	O
to	O
any	O
other	O
boxplot	O

Are	O
you	O
asking	O
why	O
the	O
top	O
whisker	O
isn't	O
the	O
same	O
length	O
as	O
the	O
bottom	O
?	O

I	O
think	O
the	O
whiskers	O
are	O
actually	O
the	O
lowest	O
or	O
highest	O
data	O
point	O
within	O
1.5	O
IQR	O
.	O

So	O
if	O
there	O
are	O
no	O
data	O
points	O
between	O
Q3	O
and	O
Q3	O
+	O
1.5	O
IQR	O
,	O
then	O
the	O
top	O
whisker	O
won't	O
show	O
up	O
.	O

For	O
the	O
one	O
boxplot	O
where	O
the	O
are	O
outliers	O
beyond	O
the	O
whiskers	O
on	O
both	O
the	O
top	O
and	O
the	O
bottom	O
,	O
the	O
whiskers	O
do	O
look	O
about	O
the	O
same	O
size	O
.	O

``	O
hist	B-API
``	O
->	O
``	O
histogram	O
``	O
(	O
``	O
hist	B-API
``	O
is	O
pyplot	B-API
or	O
something	O
)	O
.	O

There	O
is	O
a	O
pandas	O
equivalent	O
to	O
this	O
`	O
cut	B-API
`	O
there	O
is	O
a	O
section	O
describing	O
this	O
here	O
.	O

`	O
cut	B-API
`	O
returns	O
the	O
open	O
closed	O
intervals	O
for	O
each	O
value	O
:	O
#CODE	O

Pandas	O
Dataframe	O
,	O
Apply	B-API
Function	O
,	O
Return	O
Index	O

Then	O
I	O
can	O
apply	O
the	O
function	O
to	O
my	O
dataframe	O
,	O
grouped	O
by	O
I	O
D:	O
#CODE	O

If	O
I	O
resample	O
this	O
DataField	O
by	O
any	O
frequency	O
,	O
the	O
timezone	O
is	O
kept	O
:	O
#CODE	O

their	O
are	O
a	O
couple	O
of	O
outstanding	O
bugs	O
w.r.t	O
to	O
resample	O
and	O
extra	O
binning	O
:	O
#URL	O
if	O
you	O
would	O
like	O
to	O
investigate	O
and	O
try	O
to	O
pinpoint	O
(	O
or	O
better	O
yet	O
fix	O
)	O
would	O
be	O
appreciated	O
!	O

you	O
can	O
comment	O
on	O
that	O
issue	O
directly	O

@USER	O
;	O
You	O
mean	O
to	O
the	O
stack	O
exchange	O
answer	O
?	O

I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

After	O
each	O
operation	O
,	O
drop	O
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	B-API
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	B-API
`	O

I	O
have	O
tried	O
a	O
some	O
join	B-API
/	O
merge	B-API
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	B-API
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	B-API
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	O
with	O
outer	O
product	O

How	O
to	O
join	O
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	B-API
`	O
needs	O
to	O
return	O
a	O
`	O
Series	B-API
`	O
,	O
not	O
a	O
`	O
DataFrame	O
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	O
`	O
into	O
a	O
`	O
Series	B-API
`	O
is	O
to	O
use	O
`	O
stack	B-API
`	O
.	O

`	O
stack	B-API
`	O
this	O
DataFrame	O
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	B-API
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	B-API
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	B-API
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

`	O
apply	B-API
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	B-API
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	B-API
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	B-API
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	B-API
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	B-API
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	O
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	B-API
method	O
?	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	B-API
`	O
or	O
`	O
iloc	B-API
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	O
string	O
to	O
DateOffset	B-API
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	B-API
"	O
when	O
speaking	O
of	O
freq	O
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	O
/	O
unstack	O
/	O
pivot	O
dataframe	O
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	B-API
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	B-API
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	O
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	O
each	O
chunk	O
into	O
HDFStore	B-API
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

If	O
you	O
replace	O
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	O
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	B-API
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	B-API
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	B-API
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	O
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	O
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	O
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	O
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	O
insert	O
rows	O

I	O
want	O
to	O
insert	O
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	O
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	O
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	O
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

After	O
reading	O
one	O
line	O
I	O
append	O
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	B-API
/	O
merge	B-API
/	O
join	B-API
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	B-API
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	O
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

You	O
then	O
want	O
to	O
apply	O
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	O
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	B-API
`	O
.	O

`	O
loop_iter	O
=	O
len	B-API
(	O
A	O
)	O
/	O
max	B-API
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	O
transformation	O
from	O
`	O
pandas.get_dummies	B-API
`	O
to	O
new	O
data	O
?	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	B-API
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	B-API
.	O

Call	O
`	O
transform	B-API
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	B-API
`	O
,	O
transform	B-API
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	O
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	B-API
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	B-API
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	O
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	O
maybe	O
.	O

the	O
documentation	O
to	O
concat	B-API
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	B-API
`	O
on	O
the	O
`	O
groupby	B-API
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	B-API
`	O
:	O
#CODE	O

pandas	O
groupby	B-API
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	O
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	B-API
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	B-API
`	O
instead	O
of	O
`	O
iloc	B-API
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	O
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	O
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	O
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	B-API
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	O
files	O
.	O

Use	O
`	O
shift	B-API
`	O
and	O
`	O
np.log	B-API
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	O
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	O
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	O
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	B-API
method	O
.	O

I	O
tried	O
to	O
map	O
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	O
it	O
to	O
a	O
correct	O
row	O
.	O

Do	O
you	O
know	O
if	O
append	B-API
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	O
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	O
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	B-API
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	B-API
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	O
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	B-API
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	B-API
so	O
much	O
slower	O
than	O
put	O
?	O

pandas	O
merge	B-API
with	O
MultiIndex	B-API
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	O
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	O
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	B-API
levels	O
.	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	B-API
?	O

loc	B-API
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	O
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	O
?	O

it'll	O
be	O
difficult	O
to	O
translate	O
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	B-API
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	O
to	O
python	O

If	O
you	O
drop	O
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

Append	O
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	O
/	O
join	O
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	O
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	B-API
and	O
join	B-API
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	B-API
or	O
reset	B-API
the	O
index	O
so	O
they	O
align	O

Use	O
`	O
concat	B-API
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	B-API
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	B-API
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	O
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	O
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	O
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	B-API
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

And	O
you	O
could	O
always	O
drop	O
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	B-API
(	O
0	O
,	O
len	B-API
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	B-API
(	O
int	O
)`	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	B-API
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	O
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	B-API
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	B-API
I	O
think	O

After	O
they	O
are	O
done	O
,	O
merge	O
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	O
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	B-API
`	O
on	O
`	O
merged_id	O
`	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	O
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	O
the	O
dataframe	O
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	O
trace	O
?	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	O
table	O

With	O
pivot	O
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	O
question	O

For	O
each	O
group	O
in	O
the	O
groupby	B-API
object	O
,	O
we	O
will	O
want	O
to	O
apply	O
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	O
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	B-API
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
join	O
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	O
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	O
on	O
.	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	O
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	O
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	O
of	O
about	O
10378	O
rows	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	B-API
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	B-API
`	O
function	O
,	O
as	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	B-API
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	B-API
(	O
len	B-API
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	O
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	O
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	B-API
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	O
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	B-API
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	B-API
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	B-API
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	O
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	O
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	O
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	B-API
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	B-API
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	B-API
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	B-API
as	O
desired	O

I	O
can	O
strip	O
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	O
two	O
DataFrame	O
columns	O
and	O
apply	O
pandas.to_datetime	B-API
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	O
two	O
columns	O
,	O
and	O
apply	O
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	O
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	B-API
`	O
and	O
`	O
diff	B-API
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	B-API
`	O
is	O
used	O
to	O
apply	B-API
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	B-API
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	B-API
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	B-API
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	B-API
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	O
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	B-API
in	O
the	O
dataframe	O
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	O
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	B-API
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	B-API
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	B-API
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	B-API
to	O
do	O
that	O
with	O
`	O
polyfit	B-API
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	B-API
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	B-API
function	O
.	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	B-API
does	O
?	O

No	O
,	O
apply	B-API
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	B-API
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

*	O
apply	B-API
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	O
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	O
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	B-API
`	O
methods	O
to	O
replace	O
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	B-API
`	O
creates	O
a	O
new	O
dataframe	O
,	O
`	O
ix	B-API
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	B-API
`	O
and	O
`	O
iloc	B-API
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	B-API
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	B-API
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	B-API
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	B-API
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	O
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	B-API
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	O
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	O
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O

I	O
think	O
that	O
I	O
understand	O
what's	O
going	O
on	O
:	O
create	O
a	O
frequency	O
table	O
of	O
ALL	O
words	O
.	O

After	O
each	O
operation	O
,	O
drop	O
all	O
relevant	O
columns	O
,	O
then	O
finally	O
count	O
all	O
remaining	O
columns	O
.	O

Also	O
,	O
I	O
quickly	O
tried	O
this	O
in	O
Python	O
3.4.3	O
and	O
I	O
got	O
the	O
error	O
that	O
freqDf	O
isn't	O
defined	O
.	O

Should	O
I	O
first	O
create	O
a	O
new	O
table	O
named	O
freqDf	O
?	O

`	O
df.precedingWord.isin	O
(	O
neuter	O
)`	O
is	O
just	O
a	O
Series	O
of	O
True	O
or	O
False	O
(	O
results	O
of	O
the	O
previous	O
test	O
`	O
isin	B-API
`)	O
,	O
and	O
pandas	O
will	O
just	O
access	O
True	O
indexes	O
with	O
`	O
loc	B-API
`	O

I	O
have	O
tried	O
a	O
some	O
join	B-API
/	O
merge	B-API
ideas	O
but	O
can't	O
seem	O
to	O
get	O
it	O
to	O
work	O
.	O

Just	O
`	O
concat	B-API
`	O
them	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

Or	O
`	O
merge	B-API
`	O
on	O
'	O
Symbol	O
'	O
column	O
:	O
#CODE	O

Pandas	O
:	O
join	O
with	O
outer	O
product	O

How	O
to	O
join	O
/	O
multiply	O
the	O
DataFrames	O
`	O
areas	O
`	O
and	O
`	O
demand	O
`	O
together	O
in	O
a	O
decent	O
way	O
?	O

Now	O
`	O
apply	B-API
`	O
needs	O
to	O
return	O
a	O
`	O
Series	B-API
`	O
,	O
not	O
a	O
`	O
DataFrame	O
`	O
.	O

One	O
way	O
to	O
turn	O
a	O
`	O
DataFrame	O
`	O
into	O
a	O
`	O
Series	B-API
`	O
is	O
to	O
use	O
`	O
stack	B-API
`	O
.	O

`	O
stack	B-API
`	O
this	O
DataFrame	O
.	O

This	O
can	O
be	O
done	O
with	O
`	O
unstack	B-API
`	O
:	O
#CODE	O

`	O
del	O
`	O
+	O
`	O
pivot	B-API
`	O
turns	O
out	O
to	O
be	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
in	O
this	O
case	O
.	O

Maybe	O
the	O
reason	O
`	O
pivot	B-API
`	O
exists	O
is	O
because	O
it	O
is	O
faster	O
than	O
`	O
pivot_table	B-API
`	O
for	O
those	O
cases	O
where	O
it	O
is	O
applicable	O
(	O
such	O
as	O
when	O
you	O
don't	O
need	O
aggregation	O
)	O
.	O

`	O
apply	B-API
`	O
is	O
now	O
among	O
my	O
top	O
5	O
functions	O
to	O
always	O
remember	O
.	O

Concerning	O
the	O
`	O
pivot_table	B-API
`	O
solution	O
:	O
At	O
which	O
point	O
am	O
I	O
supposed	O
to	O
enter	O
the	O
line	O
?	O

No	O
matter	O
when	O
in	O
my	O
attempt	O
above	O
,	O
I	O
always	O
get	O
`	O
no	O
item	O
named	O
Edge	O
`	O
.	O

Or	O
pass	O
`	O
axis=0	O
`	O
to	O
`	O
loc	B-API
`	O
:	O
#CODE	O

I've	O
got	O
2	O
pandas	O
dataframes	O
,	O
each	O
of	O
them	O
has	O
an	O
index	O
with	O
dtype	B-API
`	O
object	O
`	O
,	O
and	O
in	O
both	O
of	O
them	O
I	O
can	O
see	O
the	O
value	O
`	O
533	O
`	O
.	O

However	O
,	O
when	O
I	O
join	B-API
them	O
the	O
result	O
is	O
empty	O
,	O
as	O
one	O
of	O
them	O
is	O
the	O
number	O
`	O
533	O
`	O
and	O
the	O
other	O
is	O
a	O
string	O
`"	O
533	O
"`	O
.	O

Ideally	O
I	O
would	O
like	O
something	O
like	O
`	O
apply_chunk()	O
`	O
which	O
is	O
the	O
same	O
as	O
apply	B-API
but	O
only	O
works	O
on	O
a	O
piece	O
of	O
the	O
dataframe	O
.	O

This	O
has	O
to	O
be	O
a	O
common	O
problem	O
though	O
,	O
is	O
there	O
a	O
design	O
pattern	O
I	O
should	O
be	O
using	O
for	O
adding	O
columns	O
to	O
large	O
pandas	O
dataframes	O
?	O

whats	O
about	O
using	O
the	O
apply	B-API
method	O
?	O

Anytime	O
you	O
find	O
yourself	O
using	O
`	O
apply	B-API
`	O
or	O
`	O
iloc	B-API
`	O
in	O
a	O
loop	O
it's	O
likely	O
that	O
Pandas	O
is	O
operating	O
much	O
slower	O
than	O
is	O
optimal	O
.	O

Convert	O
freq	O
string	O
to	O
DateOffset	B-API
in	O
pandas	O

In	O
pandas	O
documentation	O
one	O
can	O
read	O
"	O
Under	O
the	O
hood	O
,	O
these	O
frequency	O
strings	O
are	O
being	O
translated	O
into	O
an	O
instance	O
of	O
pandas	O
DateOffset	B-API
"	O
when	O
speaking	O
of	O
freq	O
string	O
such	O
as	O
"	O
W	O
"	O
or	O
"	O
W-SUN	O
"	O
.	O

stack	O
/	O
unstack	O
/	O
pivot	O
dataframe	O
on	O
python	O
/	O
pandas	O

yes	O
,	O
`	O
isnull	B-API
`	O
will	O
create	O
a	O
boolean	O
series	O
,	O
`	O
all	B-API
`	O
returns	O
`	O
True	O
`	O
if	O
all	O
are	O
`	O
True	O
`	O

Then	O
merge	O
the	O
sub-tables	O
back	O
together	O
in	O
a	O
way	O
that	O
replaces	O
NaN	O
values	O
when	O
there	O
is	O
data	O
in	O
one	O
of	O
the	O
tables	O
.	O

I	O
regularly	O
work	O
with	O
very	O
large	O
data	O
sets	O
that	O
are	O
too	O
big	O
to	O
manipulate	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
read	O
in	O
a	O
csv	O
file	O
iteratively	O
,	O
append	O
each	O
chunk	O
into	O
HDFStore	B-API
object	O
,	O
and	O
then	O
work	O
with	O
subsets	O
of	O
the	O
data	O
.	O

If	O
you	O
replace	O
that	O
line	O
with	O
:	O

I	O
wanted	O
to	O
merge	O
these	O
files	O
so	O
that	O
i	O
have	O
something	O
like	O
this	O
#CODE	O

If	O
it's	O
six	O
,	O
then	O
you	O
can	O
use	O
join	B-API
method	O
by	O
@USER	O
Hayden	O
.	O

Then	O
you	O
can	O
simply	O
`	O
join	B-API
`	O
them	O
:	O
#CODE	O

@USER	O
when	O
you	O
do	O
a	O
join	B-API
with	O
2x2	O
duplicates	O
you	O
get	O
4	O
in	O
the	O
joined	O
DataFrame	O
.	O

It's	O
unclear	O
how	O
pandas	O
should	O
join	O
in	O
this	O
case	O
,	O
so	O
you	O
need	O
to	O
be	O
more	O
explicit	O
to	O
it	O
(	O
and	O
tell	O
it	O
what	O
do	O
you	O
want	O
)	O
.	O

On	O
the	O
similar	O
note	O
,	O
is	O
there	O
a	O
way	O
to	O
merge	O
values	O
based	O
on	O
index	O
.	O

For	O
example	O
,	O
instead	O
of	O
listing	O
Bact5	O
in	O
two	O
rows	O
,	O
can	O
we	O
merge	O
its	O
value	O
corresponding	O
to	O
file2	O
in	O
one	O
row	O
separated	O
by	O
a	O
delimeter	O
?	O

Pandas	O
dataframe	O
insert	O
rows	O

I	O
want	O
to	O
insert	O
rows	O
in	O
DF	O
and	O
modify	O
its	O
related	O
values	O
:	O

The	O
code	O
can	O
only	O
append	O
rows	O
but	O
how	O
to	O
modify	O
its	O
values	O
in	O
a	O
faster	O
way	O
?	O

I	O
want	O
to	O
use	O
a	O
function	O
from	O
an	O
add-in	O
in	O
excel	O
and	O
apply	O
it	O
to	O
some	O
data	O
i	O
have	O
simulated	O
in	O
python	O
.	O

I	O
need	O
to	O
be	O
able	O
to	O
call	O
the	O
add-in	O
and	O
apply	O
my	O
data	O
indexes	O
there	O
...	O
something	O
along	O
these	O
lines	O
:	O
=	O
add-in_name	O
(	O
data_range1	O
,	O
data_range2	O
,	O
"	O
GGCV	O
")	O

After	O
reading	O
one	O
line	O
I	O
append	O
the	O
dictionary	O
to	O
a	O
list	O
(	O
so	O
,	O
the	O
number	O
of	O
dictionaries	O
in	O
the	O
list	O
is	O
equal	O
to	O
the	O
number	O
of	O
lines	O
in	O
the	O
file	O
)	O
.	O

I	O
can	O
easily	O
do	O
this	O
iteratively	O
with	O
loops	O
,	O
but	O
I've	O
read	O
that	O
you're	O
supposed	O
to	O
slice	B-API
/	O
merge	B-API
/	O
join	B-API
data	O
frames	O
holistically	O
,	O
so	O
I'm	O
trying	O
to	O
see	O
if	O
I	O
can	O
find	O
a	O
better	O
way	O
of	O
doing	O
this	O
.	O

A	O
join	B-API
will	O
give	O
me	O
all	O
the	O
stuff	O
that	O
matches	O
,	O
but	O
that's	O
not	O
exactly	O
what	O
I'm	O
looking	O
for	O
,	O
since	O
I	O
need	O
a	O
resulting	O
dataframe	O
for	O
each	O
key	O
(	O
i.e.	O
for	O
every	O
row	O
)	O
in	O
A	O
.	O

You	O
then	O
want	O
to	O
apply	O
some	O
function	O
to	O
each	O
group	O
of	O
rows	O
in	O
`	O
b	O
`	O
where	O
the	O
`	O
b	O
[	O
"	O
key	O
"]`	O
is	O
one	O
of	O
the	O
values	O
in	O
`	O
keys	O
`	O
.	O

Under	O
the	O
covers	O
,	O
these	O
are	O
really	O
similar	O
uses	O
of	O
`	O
apply	B-API
`	O
.	O

`	O
loop_iter	O
=	O
len	B-API
(	O
A	O
)	O
/	O
max	B-API
(	O
A	O
[	O
'	O
SEQ_NUM	O
'])	O

Easy	O
way	O
to	O
apply	O
transformation	O
from	O
`	O
pandas.get_dummies	B-API
`	O
to	O
new	O
data	O
?	O

As	O
an	O
aside	O
that	O
may	O
help	O
you	O
in	O
the	O
meantime	O
,	O
with	O
datetime-indexed	O
data	O
,	O
[	O
resample	B-API
]	O
(	O
#URL	O
)	O
is	O
usually	O
a	O
better	O
choice	O
than	O
reindex	B-API
.	O

Call	O
`	O
transform	B-API
`	O
on	O
the	O
'	O
measurement	O
'	O
column	O
and	O
pass	O
the	O
method	O
`	O
diff	B-API
`	O
,	O
transform	B-API
returns	O
a	O
series	O
with	O
an	O
index	O
aligned	O
to	O
the	O
original	O
df	O
:	O
#CODE	O

If	O
you	O
are	O
intending	O
to	O
apply	O
some	O
sorting	O
on	O
the	O
result	O
of	O
`	O
transform	B-API
`	O
then	O
sort	O
the	O
df	O
first	O
:	O
#CODE	O

Or	O
you	O
can	O
slice	O
the	O
columns	O
and	O
pass	O
this	O
to	O
`	O
drop	B-API
`	O
:	O
#CODE	O

These	O
values	O
are	O
median	O
values	O
I	O
calculated	O
from	O
elsewhere	O
,	O
and	O
I	O
have	O
also	O
their	O
variance	O
and	O
standard	O
deviation	O
(	O
and	O
standard	O
error	O
,	O
too	O
)	O
.	O

=	O
Hash	O
[	O
0	O
]	O
was	O
my	O
point	O
,	O
but	O
even	O
without	O
arithmetic	O
,	O
there	O
will	O
be	O
a	O
huge	O
range	O
values	O
for	O
the	O
keys	O
that	O
will	O
give	O
potentially	O
unfortunate	O
results	O
.	O

if	O
precision	O
is	O
to	O
decimal	O
place	O
,	O
I'd	O
multiply	O
it	O
by	O
10	O
and	O
truncate	O
maybe	O
.	O

the	O
documentation	O
to	O
concat	B-API
is	O
impenetrable	O
and	O
its	O
hard	O
to	O
find	O
examples	O
of	O
this	O
relatively	O
simple	O
task	O
in	O
the	O
docs	O

If	O
you	O
had	O
not	O
called	O
`	O
apply	B-API
`	O
on	O
the	O
`	O
groupby	B-API
`	O
object	O
then	O
you	O
could	O
access	O
the	O
`	O
groups	B-API
`	O
:	O
#CODE	O

pandas	O
groupby	B-API
X	O
,	O
Y	O
and	O
select	O
last	O
week	O
of	O
X1	O
and	O
X2	O
(	O
which	O
have	O
diff	O
frequency	O
)	O

Then	O
you	O
can	O
select	O
the	O
rows	O
you	O
want	O
in	O
an	O
apply	B-API
call	O
on	O
the	O
grouped	O
object	O
:	O
#CODE	O

If	O
you	O
can't	O
upgrade	O
or	O
don't	O
solve	O
the	O
issue	O
you	O
have	O
with	O
0.14	O
,	O
you	O
can	O
try	O
to	O
use	O
`	O
ix	B-API
`	O
instead	O
of	O
`	O
iloc	B-API
`	O

How	O
do	O
I	O
export	O
multiple	O
pivot	O
tables	O
from	O
python	O
using	O
pandas	O
to	O
a	O
single	O
csv	O
document	O
?	O

Say	O
I	O
have	O
a	O
function	O
pivots()	O
which	O
aggregates	O
pivot	O
tables	O
#CODE	O

I	O
know	O
how	O
to	O
export	O
a	O
single	O
pivot	O
table	O
#CODE	O

You	O
can	O
use	O
`	O
to_csv	B-API
(	O
path	O
,	O
mode=	O
'	O
a	O
')`	O
to	O
append	O
files	O
.	O

Use	O
`	O
shift	B-API
`	O
and	O
`	O
np.log	B-API
`	O
:	O
#CODE	O

I'd	O
look	O
at	O
seeing	O
if	O
you	O
can	O
export	O
it	O
in	O
it's	O
raw	O
form	O
,	O
otherwise	O
this	O
must	O
be	O
a	O
common	O
problem	O
and	O
someone	O
somewhere	O
has	O
probably	O
coded	O
a	O
method	O
to	O
strip	O
the	O
emojis	O
out	O
of	O
the	O
text	O

Python	O
pandas	O
map	O
dict	O
keys	O
to	O
values	O

I	O
have	O
a	O
csv	O
for	O
input	O
,	O
whose	O
row	O
values	O
I'd	O
like	O
to	O
join	O
into	O
a	O
new	O
field	O
.	O

This	O
new	O
field	O
is	O
a	O
constructed	O
url	O
,	O
which	O
will	O
then	O
be	O
processed	O
by	O
the	O
requests.post()	B-API
method	O
.	O

I	O
tried	O
to	O
map	O
values	O
to	O
keys	O
with	O
a	O
dict	O
comprehension	O
,	O
but	O
the	O
assignment	O
of	O
a	O
key	O
like	O
'	O
FIRST_NAME	O
'	O
could	O
end	O
up	O
mapping	O
to	O
values	O
from	O
an	O
arbitrary	O
field	O
like	O
test_df	O
[	O
'	O
CITY	O
']	O
.	O

which	O
will	O
give	O
you	O
output	O
as	O
follows	O
:	O
`	O
[	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
,	O
{	O
'	O
FIRST_NAME	O
'	O
:	O
...,	O
'	O
LAST_NAME	O
'	O
:	O
...	O
}	O
]`	O
(	O
which	O
will	O
give	O
you	O
a	O
list	O
that	O
has	O
equal	O
length	O
as	O
`	O
test_df	O
`)	O
.	O

This	O
might	O
be	O
one	O
possibility	O
to	O
easily	O
map	O
it	O
to	O
a	O
correct	O
row	O
.	O

Do	O
you	O
know	O
if	O
append	B-API
returns	O
a	O
copy	O
/	O
view	O
/	O
reference	O
of	O
the	O
original	O
dataframe	O
?	O

Right	O
now	O
,	O
I	O
am	O
trying	O
to	O
replace	O
a	O
stored	O
procedure	O
with	O
a	O
Python	O
service	O
,	O
and	O
the	O
temp	O
tables	O
with	O
Pandas	O
dataframes	O
.	O

You	O
could	O
pass	O
an	O
argument	O
to	O
`	O
apply	B-API
`	O
:	O
#CODE	O

Originally	O
,	O
I	O
used	O
append	B-API
api	O
to	O
create	O
a	O
single	O
table	O
'	O
impression	O
'	O
,	O
however	O
that	O
was	O
taking	O
80sec	O
per	O
dataframe	O
and	O
given	O
that	O
I	O
have	O
almost	O
200	O
of	O
files	O
to	O
be	O
processed	O
,	O
the	O
'	O
append	B-API
'	O
appeared	O
to	O
be	O
too	O
slow	O
.	O

Also	O
,	O
why	O
is	O
append	B-API
so	O
much	O
slower	O
than	O
put	O
?	O

pandas	O
merge	B-API
with	O
MultiIndex	B-API
,	O
when	O
only	O
one	O
level	O
of	O
index	O
is	O
to	O
be	O
used	O
as	O
key	O

I	O
want	O
to	O
recover	O
the	O
values	O
in	O
the	O
column	O
'	O
_Cat	O
'	O
from	O
df2	O
and	O
merge	O
them	O
into	O
df1	O
for	O
the	O
appropriate	O
values	O
of	O
'	O
_ItemId	O
'	O
.	O

This	O
is	O
almost	O
(	O
I	O
think	O
?	O
)	O
a	O
standard	O
many-to-one	O
merge	O
,	O
except	O
that	O
the	O
appropriate	O
key	O
for	O
the	O
left	O
df	O
is	O
one	O
of	O
MultiIndex	B-API
levels	O
.	O

Or	O
is	O
there	O
a	O
better	O
approach	O
to	O
this	O
merge	B-API
?	O

loc	B-API
will	O
not	O
attempt	O
to	O
use	O
a	O
number	O
(	O
eg	O
1	O
)	O
as	O
a	O
positional	O
argument	O
at	O
all	O
(	O
and	O
will	O
raise	O
instead	O
);	O
see	O
main	O
pandas	O
docs	O
/	O
selecting	O
data	O

I	O
have	O
the	O
following	O
boxplot	O
:	O
#CODE	O

My	O
question	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
whiskers	O
/	O
quantiles	O
being	O
plotted	O
in	O
the	O
boxplot	O
?	O

it'll	O
be	O
difficult	O
to	O
translate	O
those	O
`	O
ddply	O
`	O
calls	O
to	O
pandas	O
.	O

I	O
guess	O
`	O
groupby	B-API
`	O
should	O
be	O
used	O
but	O
I	O
find	O
this	O
format	O
very	O
cryptic	O
so	O
it's	O
hard	O
to	O
translate	O
to	O
python	O

If	O
you	O
drop	O
the	O
"	O
%	O
"	O
sign	O
,	O
you	O
can	O
make	O
the	O
plot	O
without	O
ticks	O
.	O

Append	O
Two	O
Dataframes	O
Together	O
(	O
Pandas	O
,	O
Python3	O
)	O

I	O
am	O
trying	O
to	O
append	O
/	O
join	O
(	O
?	O
)	O
two	O
different	O
dataframes	O
together	O
that	O
don't	O
share	O
any	O
overlapping	O
data	O
.	O

I	O
am	O
trying	O
to	O
append	O
these	O
together	O
using	O
#CODE	O

EDIT	O
:	O
in	O
regards	O
to	O
Edchum's	O
answers	O
,	O
I	O
have	O
tried	O
merge	B-API
and	O
join	B-API
but	O
each	O
create	O
somewhat	O
strange	O
tables	O
.	O

OK	O
,	O
what	O
you	O
have	O
to	O
do	O
is	O
reindex	B-API
or	O
reset	B-API
the	O
index	O
so	O
they	O
align	O

Use	O
`	O
concat	B-API
`	O
and	O
pass	O
param	O
`	O
axis=1	O
`	O
:	O
#CODE	O

`	O
join	B-API
`	O
also	O
works	O
:	O
#CODE	O

As	O
does	O
`	O
merge	B-API
`	O
:	O
#CODE	O

In	O
the	O
case	O
where	O
the	O
indices	O
do	O
not	O
align	O
where	O
for	O
example	O
your	O
first	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
]`	O
and	O
your	O
second	O
df	O
has	O
index	O
`	O
[	O
0	O
,	O
2	O
]`	O
this	O
will	O
mean	O
that	O
the	O
above	O
operations	O
will	O
naturally	O
align	O
against	O
the	O
first	O
df's	O
index	O
resulting	O
in	O
a	O
`	O
NaN	O
`	O
row	O
for	O
index	O
row	O
`	O
1	O
`	O
.	O

To	O
fix	O
this	O
you	O
can	O
reindex	O
the	O
second	O
df	O
either	O
by	O
calling	O
`	O
reset_index()	B-API
`	O
or	O
assign	O
directly	O
like	O
so	O
:	O
`	O
df2.index	O
=[	O
0	O
,	O
1	O
]`	O
.	O

And	O
you	O
could	O
always	O
drop	O
back	O
to	O
numpy	O
operations	O
on	O
the	O
numpy	O
array	O
`	O
pan.values	O
`	O
if	O
need	O
be	O
,	O
though	O
,	O
hopefully	O
,	O
that	O
would	O
be	O
unnecessary	O
.	O

This	O
argument	O
is	O
new	O
in	O
1.9	O
...	O
but	O
there	O
is	O
a	O
workaround	O
,	O
try	O
`	O
np.linspace	B-API
(	O
0	O
,	O
len	B-API
(	O
pep_list	O
)	O
,	O
n+1	O
,	O
endpoint=True	O
)	O
.astype	B-API
(	O
int	O
)`	O

Take	O
the	O
time	O
difference	O
(	O
using	O
`	O
shift	B-API
`	O
)	O
til	O
the	O
next	O
value	O
,	O
and	O
multiply	O
(	O
value	O
*	O
seconds	O
):	O
#CODE	O

Then	O
do	O
the	O
resample	O
to	O
seconds	O
(	O
sum	O
the	O
value*seconds	O
):	O
#CODE	O

you	O
can	O
isnull	B-API
(	O
df	O
[	O
'	O
difference	O
'])	O
will	O
give	O
True	O
on	O
NaT	O
,	O
so	O
you	O
could	O
subtract	O
then	O
use	O
mask	B-API
I	O
think	O

After	O
they	O
are	O
done	O
,	O
merge	O
the	O
two	O
frames	O
together	O
:	O
#CODE	O

Another	O
solution	O
(	O
slightly	O
harder	O
):	O
Merge	O
the	O
columns	O
`	O
transcript_id	O
`	O
,	O
`	O
gene_id	O
`	O
and	O
`	O
gene_name	O
`	O
in	O
another	O
column	O
,	O
say	O
`	O
merged_id	O
`	O
and	O
`	O
groupby	B-API
`	O
on	O
`	O
merged_id	O
`	O
.	O

Geo	O
Pandas	O
Data	O
Frame	O
/	O
Matrix	O
-	O
filter	O
/	O
drop	O
NaN	O
/	O
False	O
values	O

Then	O
I	O
stack	O
the	O
dataframe	O
,	O
give	O
the	O
index	O
levels	O
the	O
desired	O
names	O
,	O
and	O
select	O
only	O
the	O
rows	O
where	O
we	O
have	O
'	O
True	O
'	O
values	O
:	O
#CODE	O

Can	O
you	O
enable	O
the	O
debugger	O
to	O
get	O
a	O
stack	O
trace	O
?	O

reshape	O
data	O
frame	O
in	O
pandas	O
with	O
pivot	O
table	O

With	O
pivot	O
table	O
you	O
can	O
get	O
a	O
matrix	O
showing	O
which	O
`	O
baz	O
`	O
corresponds	O
to	O
which	O
`	O
qux	O
`	O
:	O
#CODE	O

Rolling	O
apply	O
question	O

For	O
each	O
group	O
in	O
the	O
groupby	B-API
object	O
,	O
we	O
will	O
want	O
to	O
apply	O
a	O
function	O
:	O
#CODE	O

We	O
want	O
to	O
take	O
the	O
Times	O
column	O
,	O
and	O
for	O
each	O
time	O
,	O
apply	O
a	O
function	O
.	O

That's	O
done	O
with	O
`	O
applymap	B-API
`	O
:	O
#CODE	O

Given	O
a	O
time	O
`	O
t	O
`	O
,	O
we	O
can	O
select	O
the	O
`	O
Value	O
`	O
s	O
from	O
`	O
subf	O
`	O
whose	O
times	O
are	O
in	O
the	O
half-open	O
interval	O
`	O
(	O
t-60	O
,	O
t	O
]`	O
using	O
the	O
`	O
ix	B-API
`	O
method	O
:	O
#CODE	O

pandas	O
join	O
data	O
frames	O
on	O
similar	O
but	O
not	O
identical	O
string	O
using	O
lower	O
case	O
only	O

I	O
need	O
to	O
join	O
data	O
frames	O
on	O
columns	O
that	O
are	O
similar	O
but	O
not	O
identical	O
.	O

So	O
I	O
am	O
trying	O
to	O
isolate	O
the	O
lowercase	O
letters	O
from	O
each	O
column	O
,	O
create	O
new	O
columns	O
to	O
join	O
on	O
.	O

Note	O
that	O
this	O
assumes	O
collecting	O
all	O
ASCII	O
characters	O
from	O
`	O
a	O
`	O
to	O
`	O
z	O
`	O
suffices	O
to	O
produce	O
values	O
on	O
which	O
to	O
join	O
.	O

You	O
can	O
of	O
course	O
extend	O
this	O
with	O
several	O
joins	O
,	O
the	O
join	O
solution	O
detects	O
common	O
indices	O
automatically	O
.	O

My	O
data	O
is	O
in	O
a	O
DataFrame	O
of	O
about	O
10378	O
rows	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
'])`	O
is	O
10378	O
,	O
as	O
expected	O
.	O

But	O
`	O
len	B-API
(	O
choices	O
)`	O
is	O
only	O
1695	O
.	O

I'm	O
fairly	O
certain	O
that	O
the	O
issue	O
is	O
in	O
the	O
first	O
line	O
,	O
with	O
the	O
`	O
to_dict()	B-API
`	O
function	O
,	O
as	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)`	O
results	O
in	O
10378	O
and	O
`	O
len	B-API
(	O
df	O
[	O
'	O
Full	O
name	O
']	O
.to_dict()	B-API
)`	O
results	O
in	O
1695	O
.	O

what	O
is	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
?	O

@USER	O
using	O
`	O
choices	O
=	O
dict	O
(	O
zip	O
(	O
df	O
[	O
'	O
n	O
']	O
,	O
df	O
[	O
'	O
Full	O
name	O
']	O
.astype	B-API
(	O
str	O
)))`	O
,	O
where	O
df	O
[	O
'	O
n	O
']	O
is	O
np.arange	B-API
(	O
len	B-API
(	O
df	O
))	O
,	O
worked	O
fine	O
and	O
got	O
what	O
I	O
needed	O
.	O

Had	O
some	O
indexing	O
issues	O
because	O
I	O
was	O
importing	O
the	O
data	O
from	O
different	O
Excel	O
spreadsheets	O
.	O

This	O
is	O
what	O
is	O
happening	O
in	O
your	O
case	O
,	O
and	O
noted	O
from	O
the	O
comments	O
,	O
since	O
the	O
amount	O
of	O
`	O
unique	O
`	O
values	O
for	O
the	O
index	O
are	O
only	O
`	O
1695	O
`	O
,	O
we	O
can	O
confirm	O
this	O
by	O
testing	O
the	O
value	O
of	O
`	O
len	B-API
(	O
df.index.unique()	O
)`	O
.	O

what	O
do	O
you	O
mean	O
by	O
normalize	O
?	O

The	O
other	O
way	O
is	O
much	O
easier	O
and	O
involves	O
using	O
`	O
resample	B-API
`	O
to	O
convert	O
to	O
daily	O
observations	O
and	O
backfill	O
daily	O
consumption	O
.	O

(	O
Note	O
that	O
the	O
first	O
and	O
last	O
months	O
are	O
based	O
on	O
partial	O
data	O
,	O
you	O
may	O
want	O
to	O
either	O
drop	O
them	O
or	O
pro-rate	O
the	O
daily	O
consumption	O
.	O
)	O
#CODE	O

Basically	O
,	O
after	O
calculating	O
the	O
daily	O
consumption	O
,	O
do	O
a	O
partial	O
resample	B-API
by	O
adding	O
the	O
first	O
and	O
last	O
day	O
of	O
each	O
month	O
.	O

I	O
will	O
implement	O
it	O
and	O
see	O
how	O
it	O
goes	O
,	O
but	O
can	O
you	O
also	O
explain	O
what	O
'	O
1d	O
'	O
means	O
in	O
the	O
resample	B-API
method	O
?	O

@USER	O
'	O
1d	O
'	O
just	O
means	O
1	O
day	O
for	O
the	O
frequency	O
of	O
the	O
resample	B-API
.	O

So	O
I	O
want	O
something	O
that	O
will	O
drop	O
the	O
`	O
lob	O
`	O
group	O
,	O
but	O
keep	O
every	O
record	O
of	O
both	O
the	O
`	O
mol	O
`	O
and	O
`	O
thg	O
`	O
group	O
.	O

Pandas	O
Merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
each	O

In	O
each	O
data	O
frame	O
i	O
have	O
column	O
with	O
the	O
same	O
name	O
and	O
values	O
(	O
Key_Merge1	O
)	O
and	O
in	O
each	O
data	O
frame	O
i	O
have	O
2	O
different	O
column	O
names	O
with	O
same	O
values	O
(	O
Key_Merge2	O
)	O
.	O

How	O
can	O
i	O
merge	O
2	O
data	O
frames	O
by	O
2	O
columns	O
:	O

Can	O
you	O
post	O
an	O
example	O
data	O
and	O
df	O
,	O
your	O
text	O
description	O
is	O
not	O
clear	O
enough	O
but	O
generally	O
you	O
want	O
to	O
merge	O
and	O
pass	O
the	O
list	O
of	O
cols	O
to	O
merge	O
the	O
;	O
hs	O
and	O
rhs	O
on	O
:	O
`	O
pd.merge	B-API
(	O
df1	O
,	O
df2	O
,	O
left_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_Merge21	O
']	O
,	O
right_on	O
=[	O
'	O
Key_Merge1	O
'	O
,	O
'	O
Key_merge22	O
'])`	O

OK	O
,	O
you	O
have	O
to	O
rename	O
'	O
PRODUCT_GROUP	O
'	O
in	O
DF2	O
in	O
order	O
for	O
the	O
`	O
merge	B-API
`	O
to	O
work	O
:	O
#CODE	O

the	O
merge	B-API
will	O
naturally	O
find	O
the	O
2	O
columns	O
that	O
match	O
and	O
perform	O
an	O
inner	O
merge	B-API
as	O
desired	O

I	O
can	O
strip	O
out	O
the	O
rightmost	O
'	O
.csv	O
'	O
part	O
like	O
this	O
:	O
#CODE	O

How	O
to	O
merge	O
two	O
DataFrame	O
columns	O
and	O
apply	O
pandas.to_datetime	B-API
to	O
it	O
?	O

What	O
would	O
be	O
a	O
more	O
pythonic	O
way	O
to	O
merge	O
two	O
columns	O
,	O
and	O
apply	O
a	O
function	O
into	O
the	O
result	O
?	O

once	O
sorted	O
I	O
replace	O
the	O
df.index	O
with	O
a	O
numerical	O
index	O
#CODE	O

This	O
can	O
be	O
accomplished	O
with	O
a	O
one	O
line	O
solution	O
using	O
Pandas	O
'	O
boolean	O
indexing	O
.	O

The	O
one-liner	O
also	O
employs	O
some	O
other	O
tricks	O
:	O
Pandas	O
'	O
`	O
map	B-API
`	O
and	O
`	O
diff	B-API
`	O
methods	O
and	O
a	O
`	O
lambda	O
`	O
function	O
.	O

`	O
map	B-API
`	O
is	O
used	O
to	O
apply	B-API
the	O
`	O
lambda	O
`	O
function	O
to	O
all	O
rows	O
.	O

The	O
`	O
lambda	O
`	O
function	O
is	O
needed	O
to	O
create	O
a	O
custom	O
less-then	O
comparison	O
that	O
will	O
evaluate	O
NaN	O
values	O
to	O
True	O
.	O

There	O
is	O
a	O
built	O
in	O
method	O
for	O
this	O
`	O
diff	B-API
`	O
:	O
#CODE	O

as	O
pointed	O
out	O
calling	O
`	O
diff	B-API
`	O
here	O
will	O
lose	O
the	O
first	O
row	O
so	O
I'm	O
using	O
a	O
ugly	O
hack	O
where	O
I	O
concatenate	O
the	O
first	O
row	O
with	O
the	O
result	O
of	O
the	O
`	O
diff	B-API
`	O
so	O
I	O
don't	O
lose	O
the	O
first	O
row	O

Using	O
`	O
diff	B-API
`	O
like	O
this	O
drops	O
the	O
first	O
row	O
.	O

(	O
I	O
can	O
also	O
use	O
the	O
chunksize	O
option	O
and	O
concat	O
myself	O
,	O
but	O
that	O
seems	O
to	O
be	O
a	O
bit	O
of	O
a	O
hack	O
.	O
)	O

Jeff	O
,	O
I	O
updated	O
sec_id	O
and	O
dt	B-API
in	O
the	O
dataframe	O
.	O

Sorry	O
,	O
I	O
had	O
to	O
update	O
"	O
sec_id	O
"	O
and	O
"	O
dt	O
"	O
to	O
"	O
id	O
"	O
and	O
"	O
date	O
"	O
.	O

0.12	O
is	O
fine	O
;	O
FYI	O
the	O
format	O
keyword	O
doesn't	O
do	O
anything	O
with	O
append	B-API
(	O
and	O
it's	O
for	O
0.13	O
anyhow	O
);	O
append	B-API
always	O
is	O
a	O
table	O

I	O
would	O
like	O
to	O
get	O
every	O
,	O
let's	O
say	O
,	O
6	O
hours	O
of	O
data	O
and	O
independently	O
fit	O
a	O
curve	O
to	O
that	O
data	O
.	O

Since	O
pandas	O
'	O
`	O
resample	B-API
`	O
function	O
has	O
a	O
`	O
how	O
`	O
keyword	O
that	O
is	O
supposed	O
to	O
be	O
any	O
numpy	O
array	O
function	O
,	O
I	O
thought	O
that	O
I	O
could	O
maybe	O
try	O
to	O
use	O
resample	B-API
to	O
do	O
that	O
with	O
`	O
polyfit	B-API
`	O
,	O
but	O
apparently	O
there	O
is	O
no	O
way	O
(	O
right	O
?	O
)	O
.	O

Why	O
does	O
the	O
second	O
block	O
of	O
code	O
not	O
work	O
?	O

Doesn't	O
DataFrame.apply()	B-API
default	O
to	O
inplace	O
?	O

There	O
is	O
no	O
inplace	O
parameter	O
to	O
the	O
apply	B-API
function	O
.	O

Even	O
if	O
it	O
doesn't	O
default	O
to	O
inplace	O
,	O
shouldn't	O
it	O
provide	O
an	O
inplace	O
parameter	O
the	O
way	O
replace()	B-API
does	O
?	O

No	O
,	O
apply	B-API
does	O
not	O
work	O
inplace*	O
.	O

In	O
general	O
apply	B-API
is	O
slow	O
(	O
since	O
you	O
are	O
basically	O
iterating	O
through	O
each	O
row	O
in	O
python	O
)	O
,	O
and	O
the	O
"	O
game	O
"	O
is	O
to	O
rewrite	O
that	O
function	O
in	O
terms	O
of	O
pandas	O
/	O
numpy	O
native	O
functions	O
and	O
indexing	O
.	O

If	O
you	O
want	O
to	O
delve	O
into	O
more	O
details	O
about	O
the	O
internals	O
,	O
check	O
out	O
the	O
BlockManager	O
in	O
core	O
/	O
internals.py	O
,	O
this	O
is	O
the	O
object	O
which	O
holds	O
the	O
underlying	O
numpy	O
arrays	O
.	O

*	O
apply	B-API
is	O
not	O
usually	O
going	O
to	O
make	O
sense	O
inplace	O
(	O
and	O
IMO	O
this	O
behaviour	O
would	O
rarely	O
be	O
desired	O
)	O
.	O

I	O
use	O
this	O
function	O
with	O
pandas	O
to	O
apply	O
it	O
to	O
each	O
month	O
of	O
a	O
historical	O
record	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
merge	O
tsv	O
files	O
using	O
pandas	O
but	O
cannot	O
get	O
pandas	O
to	O
return	O
the	O
file	O
contents	O
correctly	O
.	O

You	O
can	O
use	O
the	O
vectorised	O
`	O
str	B-API
`	O
methods	O
to	O
replace	O
the	O
unwanted	O
characters	O
and	O
then	O
cast	O
the	O
type	O
to	O
int	O
:	O
#CODE	O

perhaps	O
`	O
reindex	B-API
`	O
creates	O
a	O
new	O
dataframe	O
,	O
`	O
ix	B-API
`	O
returns	O
a	O
view	O

@USER	O
you	O
are	O
,	O
of	O
course	O
,	O
absolutely	O
right	O
.	O
what	O
do	O
`	O
loc	B-API
`	O
and	O
`	O
iloc	B-API
`	O
do	O
?	O

The	O
reason	O
for	O
the	O
seeming	O
redundancy	O
is	O
that	O
,	O
while	O
using	O
`	O
ix	B-API
`	O
is	O
syntacticly	O
limiting	O
(	O
you	O
can	O
only	O
pass	O
a	O
single	O
argument	O
to	O
`	O
__getitem__	O
`)	O
,	O
`	O
reindex	B-API
`	O
is	O
a	O
method	O
,	O
which	O
supports	O
taking	O
various	O
optional	O
parameters	O
.	O

I	O
am	O
getting	O
different	O
results	O
when	O
using	O
`	O
reindex	B-API
`	O
with	O
`	O
inplace=True	O
`	O
vs	O
using	O
`	O
ix	B-API
`	O
(	O
I	O
updated	O
the	O
OP	O
)	O

What	O
if	O
you	O
have	O
many	O
conditions	O
,	O
e.g.	O
you	O
want	O
to	O
split	O
up	O
the	O
scatters	O
into	O
4	O
types	O
of	O
points	O
or	O
even	O
more	O
,	O
plotting	O
each	O
in	O
different	O
shape	O
/	O
color	O
.	O

How	O
can	O
you	O
elegantly	O
apply	O
condition	O
a	O
,	O
b	O
,	O
c	O
,	O
etc	O
.	O
and	O
make	O
sure	O
you	O
then	O
plot	O
"	O
the	O
rest	O
"	O
(	O
things	O
not	O
in	O
any	O
of	O
these	O
conditions	O
)	O
as	O
the	O
last	O
step	O
?	O

To	O
find	O
points	O
skipped	O
due	O
to	O
NA	O
,	O
try	O
the	O
`	O
isnull	B-API
`	O
method	O
:	O
`	O
df	O
[	O
df.col3.isnull()	O
]`	O

How	O
do	O
I	O
create	O
a	O
pivot	O
table	O
in	O
Pandas	O
where	O
one	O
column	O
is	O
the	O
mean	O
of	O
some	O
values	O
,	O
and	O
the	O
other	O
column	O
is	O
the	O
sum	O
of	O
others	O
?	O

Basically	O
,	O
how	O
would	O
I	O
create	O
a	O
pivot	O
table	O
that	O
consolidates	O
data	O
,	O
where	O
one	O
of	O
the	O
columns	O
of	O
data	O
it	O
represents	O
is	O
calculated	O
,	O
say	O
,	O
by	O
`	O
likelihood	O
percentage	O
`	O
(	O
0.0	O
-	O
1.0	O
)	O
by	O
taking	O
the	O
mean	O
,	O
and	O
another	O
is	O
calculated	O
by	O
`	O
number	O
ordered	O
`	O
which	O
sums	O
all	O
of	O
them	O
?	O

Why	O
do	O
you	O
say	O
that	O
reusing	O
plt.figure	B-API
will	O
lead	O
to	O
memory	O
issues	O
?	O

I	O
am	O
using	O
matplotlib.pyplot	B-API
to	O
create	O
histograms	O
.	O

I	O
corrected	O
this	O
using	O
the	O
`	O
set_position	B-API
`	O
command	O
.	O

`	O
np.asarray	B-API
(	O
Image.fromarray	O
(	O
image.astype	O
(	O
'	O
uint8	O
')	O
*255	O
)	O
.convert	B-API
(	O
'	O
L	O
'))	O
.astype	B-API
(	O
float	O
)	O
/	O
255	O
`	O

python	O
:	O
ImportError	O
:	O
No	O
module	O
named	O
patheffects	B-API

import	O
matplotlib.patheffects	B-API

`	O
plt.bar	B-API
(	O
left	O
,	O
height	O
,	O
width=	O
0.8	O
,	O
bottom=None	O
,	O
hold=None	O
,	O
**	O
kwargs	O
)`	O

I	O
typically	O
have	O
them	O
in	O
the	O
ax.plot	B-API
line	O
.	O

my	O
attempt	O
to	O
define	O
the	O
aspect	O
ratio	O
for	O
both	O
`	O
y	O
`	O
-axis	O
while	O
using	O
`	O
twinx	B-API
`	O
(	O
which	O
does	O
not	O
work	O
)	O

For	O
each	O
of	O
this	O
point	O
,	O
I	O
can	O
calculate	O
the	O
value	O
of	O
my	O
unordered	O
categorical	B-API
variable	O
X	O
.	O

You	O
can	O
achieve	O
it	O
using	O
the	O
`	O
ScalarFormatter	B-API
`	O
:	O
#CODE	O

@USER	O
how	O
is	O
hexbin	B-API
useful	O
here	O
?	O

Hmm	O
,	O
doesn't	O
look	O
like	O
there	O
is	O
anything	O
obvious	O
is	O
rcParams	B-API
or	O
the	O
matplotlibrc	O
file	O
.	O

What	O
you're	O
doing	O
is	O
aliasing	O
(	O
"	O
renaming	O
")	O
matplotlib.font_manager	B-API
to	O
FontProperties	B-API
so	O
calling	O
fontP	O
=	O
FontProperties()	B-API
is	O
actually	O
calling	O
matplotlib.font_manager	B-API
which	O
is	O
not	O
callable	O
.	O

I	O
think	O
changing	O
the	O
arrowstyle	B-API
will	O
help	O
here	O
.	O

plt.plot	B-API
(	O
x	O
,	O
z	O
)	O

`	O
plt.tight_layout	B-API
(	O
pad=	O
2.0	O
,	O
h_pad=	O
6.5	O
,	O
w_pad=	O
4.5	O
)`	O
gives	O
the	O
desired	O
result	O
(	O
Fig.3	O
)	O
.	O

Should	O
work	O
without	O
the	O
line	O
`	O
X	O
,	O
Y	O
=	O
np.meshgrid	B-API
(	O
X	O
,	O
Y	O
)`	O
.	O

O1	O
=	O
np.array	B-API
(	O
new_OI	O
)	O

There's	O
no	O
need	O
for	O
a	O
vector	O
representation	O
of	O
each	O
pixel	O
,	O
and	O
`	O
imshow	B-API
`	O
will	O
be	O
much	O
faster	O
.	O

So	O
definitely	O
not	O
in	O
an	O
equidistant	O
meshgrid	B-API
.	O

Inserted	O
after	O
pyplot.savefig()	B-API
and	O
the	O
problem	O
seems	O
to	O
be	O
solved	O
.	O

`	O
numpy.polyfit	B-API
(	O
x	O
,	O
y	O
,	O
deg	O
,	O
rcond=None	O
,	O
full=False	O
,	O
w=None	O
,	O
cov=False	O
)`	O

Simply	O
putting	O
in	O
:	O
`	O
bbox	O
=d	O
ict	O
(	O
facecolor=	O
'	O
blue	O
'	O
,	O
alpha=	O
0.5	O
)`	O
in	O
the	O
ax.text	B-API
statement	O
changes	O
the	O
color	O
.	O

ax.set_xticks	B-API
([	O
"	O
1975-01-01	O
"	O
,	O
"	O
1980-01-01	O
"	O
,	O
"	O
1985-01-01	O
"	O
,	O
"	O
1990-01-01	O
"	O
,	O
"	O
1995-01-01	O
"	O
,	O

Also	O
,	O
don't	O
mix	O
`	O
pyplot	B-API
`	O
with	O
guis	O
(	O
unless	O
you	O
are	O
explicitly	O
using	O
a	O
non-interactive	O
backend	O
)	O
.	O

FuncAnimation	B-API

`	O
x2	O
'	O
=	O
-mu	O
/	O
np.sqrt	B-API
(	O
x	O
**	O
2	O
+	O
y	O
**	O
2	O
+	O
z	O
**	O
2	O
)	O
*	O
x	O
`	O
,	O

matplotlib.pyplot.stem	B-API
(	O
*args	O
,	O
**	O
kwargs	O
)	O

I've	O
confirmed	O
that	O
this	O
works	O
for	O
`	O
plot()	B-API
`	O
as	O
well	O
as	O
`	O
imshow()	B-API
`	O
.	O

PCA	B-API
(	O
X	O
)	O
is	O
not	O
the	O
same	O
thing	O
as	O
PCA	B-API
(	O
X.T	O
)	O
.T	B-API

I	O
should	O
have	O
used	O
OffsetImage	B-API
and	O
AnnotationBbox	B-API
.	O

Using	O
GridSpec	B-API

Try	O
removing	O
that	O
argument	O
from	O
your	O
call	O
to	O
`	O
streamplot	B-API
`	O
.	O

Try	O
``	O
ax.text	B-API
(	O
0	O
,	O
0	O
,	O
'	O
whatever	O
')``	O
.	O

matplotlib	O
tripcolor	B-API
-	O
removing	O
edges	O

`	O
ax.xaxis.get_children()	O
[	O
1	O
]	O
.set_size	B-API
(	O
15	O
)`	O

I	O
have	O
tried	O
lots	O
of	O
different	O
combinations	O
of	O
the	O
ax.plot()	B-API
part	O
,	O
but	O
if	O
I	O
change	O
it	O
to	O
:	O
#CODE	O

but	O
i	O
want	O
to	O
keep	O
the	O
ytick	B-API
marks	O
only	O
on	O
the	O
left	O
hand	O
side	O
.	O
thank	O
you	O
.	O

fig	O
is	O
defined	O
as	O
:	O
`	O
fig	O
=	O
plt.figure()	B-API
`	O

We	O
used	O
the	O
same	O
method	O
,	O
but	O
I	O
somehow	O
missed	O
`	O
set_markeredgewidth	B-API
`	O
.	O

--->	O
29	O
import	O
matplotlib.colorbar	B-API

ax.plot	B-API
(	O
r.date	O
,	O
r.close	O
)	O

plt.plot()	B-API

The	O
`	O
.format	B-API
`	O
form	O
is	O
the	O
preferred	O
way	O
to	O
do	O
string	O
formatting	O
now	O
as	O
opposed	O
to	O
the	O
`	O
%	O
`	O
operator	O
.	O

`	O
GridSpec	B-API
`	O
works	O
for	O
me	O
(	O
`	O
matplotlib	O
`	O
v1.5.0	O
in	O
case	O
it	O
matters	O
):	O
#CODE	O

A	O
search	O
of	O
the	O
`	O
matplotlib.pyplot	B-API
`	O
API	O
documentation	O
does	O
not	O
reveal	O
anything	O
,	O
so	O
I'm	O
afraid	O
you'll	O
have	O
to	O
live	O
with	O
using	O
`	O
matplotlib.patches.Ellipse	B-API
`	O

for	O
why	O
`	O
LinearSegmentedColormap	B-API
`	O
shows	O
strange	O
color	O
,	O
I	O
think	O
this	O
link	O
would	O
be	O
helpful	O
.	O

I	O
did	O
so	O
both	O
in	O
the	O
Windows	O
shell	O
and	O
emacs	O
shell	O
and	O
the	O
same	O
PYTHONPATH	O
is	O
used	O
.	O
doing	O
the	O
rcParams	B-API
both	O
return	O
TkAgg	O
.	O

However	O
,	O
the	O
savefig()	B-API
output	O
is	O
too	O
zoomed	O
out	O
,	O
too	O
general	O
.	O

plt.imshow	B-API
(	O
Xt	O
[	O
0	O
,	O
:]	O
.reshape	B-API
(	O
105,105	O
)	O
.T	B-API
,	O
interpolation=	O
'	O
nearest	O
'	O
,	O
cmap=	O
cm.Greys_r	O
)	O
and	O
it	O
gives	O
me	O
the	O
correct	O
flipped	O
output	O
.	O

See	O
[	O
`	O
matplotlib.pyplot.contourf	B-API
`]	O
(	O
#URL	O
)	O
for	O
example	O
.	O

Why	O
pyplot	B-API
(	O
matplotlib	O
)	O
is	O
joining	O
points	O
randomly	O
?	O

x	O
=	O
np.array	B-API
([	O
1	O
,	O
3	O
,	O
4	O
])	O
,	O
([	O
1	O
,	O
2	O
])	O

`	O
np.arange	B-API
`	O
doc	O

plt.colorbar	B-API
(	O
cs	O
)	O

`	O
tick_params	B-API
`	O
should	O
do	O
it	O
.	O

I	O
have	O
experiemented	O
with	O
turning	O
interactivity	O
on	O
and	O
off	O
via	O
`	O
plt.ioff()	B-API
`	O
,	O
but	O
to	O
no	O
avail	O
.	O

don't	O
do	O
`	O
plt.xticks	B-API
...	O

thx	O
!	O
but	O
in	O
the	O
end	O
this	O
worked	O
:	O
`	O
ax	O
[	O
1	O
]	O
.bar	B-API
(	O
bins	O
[:	O
-1	O
]	O
,	O
1	O
.	O
*hist	O
/	O
N	O
,	O
np.diff	B-API
(	O
bins	O
)	O
,	O
log=True	O
)`	O

For	O
example	O
`	O
griddata	B-API
`	O
,	O
`	O
Rbf	O
`	O
,	O
...	O

It	O
sounds	O
like	O
you	O
would	O
rather	O
have	O
an	O
`	O
axhspan	B-API
`	O
or	O
`	O
axvspan	B-API
`	O
.	O

matplotlib	O
:	O
plt.isinteractive()	B-API
returns	O
0	O
instead	O
of	O
True	O
or	O
False	O
after	O
plt.ion()	B-API
is	O
called	O
,	O
when	O
the	O
script	O
is	O
being	O
run	O
in	O
a	O
terminal	O
?	O

matplotlib	O
FuncAnimation	B-API
input	O

@USER	O
Do	O
I	O
need	O
to	O
pass	O
anything	O
additional	O
to	O
`	O
ColorbarBase()	B-API
`	O
?	O

In	O
some	O
other	O
code	O
,	O
I	O
used	O
the	O
OpenCV	O
`	O
SaveImage	O
`	O
on	O
a	O
single	O
frame	O
to	O
provide	O
a	O
reference	O
for	O
what	O
I	O
would	O
expect	O
from	O
`	O
imshow	B-API
`	O
.	O

AxesSubplot	O
has	O
no	O
attribute	O
hist2d	B-API
-	O
Matplotlib	O

fig	O
=	O
plt.figure()	B-API

customize	O
ticks	O
for	O
AxesImage	B-API
?	O

In	O
addition	O
,	O
"	O
changing	O
plt.figure	B-API
(	O
i	O
)"	O
is	O
not	O
correct	O
.	O

I	O
tried	O
it	O
out	O
.	O
linecollection	B-API
does	O
not	O
appear	O
to	O
work	O
in	O
3D	O
.	O

You	O
can	O
verify	O
it	O
by	O
calling	O
`	O
plt.xlim()	B-API
`	O
both	O
before	O
and	O
after	O
the	O
`	O
pandas.plot()	O
`	O
call	O
.	O

Did	O
you	O
try	O
calling	O
`	O
plt.show()	B-API
`	O
?	O

Edit	O
:	O
My	O
original	O
answer	O
used	O
`	O
ax.scatter	B-API
`	O
.	O

The	O
helper	O
functions	O
date2num()	B-API
,	O
num2date()	B-API
and	O
drange()	B-API
are	O
used	O
to	O
facilitate	O
easy	O
conversion	O
to	O
and	O
from	O
datetime	O
and	O
numeric	O
ranges	O
.	O

Here's	O
an	O
example	O
of	O
how	O
you	O
can	O
display	O
multiple	O
plots	O
side-by-side	O
below	O
a	O
larger	O
one	O
using	O
Gridspec	B-API
:	O
#CODE	O

I	O
only	O
just	O
discovered	O
the	O
awesome	O
that	O
is	O
`	O
matplotlib.mlab.psd()	B-API
`	O
,	O
but	O
I	O
am	O
having	O
one	O
issue	O
,	O
that	O
is	O
:	O
how	O
can	O
I	O
change	O
the	O
frequency	O
range	O
used	O
by	O
the	O
method	O
?	O

Python	O
Matlplotlib	O
errorbar	B-API
:	O
some	O
errors	O
disappear	O

@USER	O
It	O
says	O
:	O
font_manager	B-API
attribute	O
not	O
found	O
o_O	O
.	O

Documentation	O
of	O
legacy	O
`	O
subplot()	B-API
`	O
is	O
[	O
here	O
]	O
(	O
#URL	O
)	O
and	O
`	O
subplots()	B-API
`	O
is	O
[	O
here	O
]	O
(	O
#URL	O
)	O
.	O

I	O
am	O
not	O
completely	O
sure	O
if	O
this	O
is	O
the	O
kind	O
of	O
thing	O
you're	O
looking	O
for	O
,	O
but	O
`	O
legend()	B-API
`	O
inside	O
`	O
matplotlib.pyplot	B-API
`	O
may	O
be	O
able	O
to	O
help	O
you	O
out	O
.	O

Is	O
the	O
PCA	B-API
stuff	O
really	O
relevant	O
?	O

Seaborns	O
clustermap	O
doesn't	O
work	O
with	O
plt.tight_layout()	B-API
.	O

fig	O
=	O
plt.figure	B-API
(	O
**figprops	O
)	O
.	O

From	O
your	O
remark	O
that	O
`	O
datetime	O
(	O
2009	O
,	O
10	O
,	O
7	O
,	O
0	O
)	O
.strftime	B-API
(	O
'	O
%Hz%d%b	O
')`	O
gave	O
the	O
right	O
string	O
I	O
would've	O
thought	O
it	O
worked	O
.	O

yerr	O
=	O
np.array	B-API
([	O
])	O

ax	O
=	O
fig.add_subplot	B-API
(	O
111	O
);	O

plt.subplots	B-API
:	O

(	O
It's	O
not	O
the	O
edgecolor	O
option	O
in	O
pcolormesh	B-API
)	O
#CODE	O

`	O
plt.xlabel	B-API
(	O
u'Wavelength	O
(	O
\u03bc	O
m	O
)')`	O

You	O
may	O
see	O
that	O
how	O
`	O
.ix	B-API
`	O
was	O
used	O
for	O
selecting	O
an	O
interval	O
.	O

I	O
am	O
running	O
the	O
tutorial	O
for	O
pylearn2	O
and	O
I'm	O
getting	O
some	O
errors	O
when	O
it	O
imports	O
`	O
matplotlib.pyplot	B-API
`	O
.	O

then	O
a	O
call	O
to	O
`	O
fig.get_size_inches()	B-API
`	O
gives	O
`	O
[	O
6	O
,	O
7.475	O
]`	O
.	O

I	O
am	O
experiencing	O
the	O
same	O
problem	O
as	O
described	O
in	O
import	O
matplotlib.pyplot	B-API
hangs	O
after	O
updating	O
my	O
matplotlib	O
version	O
`	O
import	O
matplotlib.pyplot	B-API
as	O
plt	O
`	O
hangs	O
while	O
executing	O
`	O
fc-list	O
`	O
.	O

Using	O
`	O
pcolor	B-API
`	O
from	O
matplotlib	O
I	O
am	O
unable	O
to	O
do	O
it	O
because	O
my	O
pc	O
goes	O
easily	O
out	O
of	O
memory	O
(	O
more	O
than	O
8G	O
)	O
..	O

+	O
"	O
plt.plot	B-API
([	O
"	O
+	O
d.x1	O
+	O
"	O
,	O
"	O
+	O
d.x2	O
+	O
"]	O
,	O
[	O
"	O
+	O
d.y1	O
+	O
"	O
,	O
"	O
+	O
d.y2	O
+	O
"]	O
,	O
'	O
k-	O
')	O
\n	O
"	O

I	O
am	O
trying	O
to	O
import	O
Pylab	O
using	O
the	O
`	O
import	O
matplotlib.pyplot	B-API
as	O
plt	O
`	O
.	O

The	O
plt.subplots_adjust	B-API
method	O
:	O
#CODE	O

for	O
i	O
,	O
j	O
in	O
np.where	B-API
(	O
significant	O
):	O
print	O
i	O
,	O
j	O

ax	O
=	O
fig.add_subplot	B-API
(	O
111	O
,	O
projection=	O
'	O
3d	O
')	O

matplotlib.pyplot	B-API
does	O
savefig()	B-API
but	O
not	O
show()	B-API
in	O
python3	O
but	O
works	O
in	O
2.7	O

In	O
this	O
case	O
,	O
pcolor	B-API
and	O
pcolormesh	B-API
will	O
produce	O
the	O
same	O
result	O
.	O

From	O
the	O
documentation	O
of	O
`	O
pyplot.errorbar()	B-API
`	O
:	O

Tkinter	O
and	O
pyplot	B-API
running	O
out	O
of	O
memory	O

You've	O
already	O
created	O
`	O
ax	O
`	O
with	O
`	O
plt.subplots	B-API
`	O
so	O
don't	O
you	O
just	O
need	O
to	O
pass	O
`	O
ax=ax	O
`	O
to	O
`	O
merged2.fcast.plot	O
`	O
instead	O
of	O
setting	O
`	O
ax=	O
...	O

You	O
also	O
should	O
not	O
import	O
pylab	O
,	O
you	O
should	O
do	O
`	O
import	O
matplotlib.pyplot	B-API
as	O
plt	O
`	O

Alright	O
I	O
got	O
it	O
to	O
work	O
by	O
using	O
plt.minorticks_on()	B-API
and	O
changing	O
the	O
range	O
of	O
x	O
to	O
x	O
=	O
np.arange	B-API
(	O
0	O
,	O
361	O
,	O
50	O
,	O
dtype	B-API
=	O
int	O
)	O
....	O

You	O
can	O
control	O
the	O
ratio	O
of	O
the	O
height	O
/	O
width	O
with	O
`	O
ax.set_aspect	B-API
(	O
number	O
)`	O
.	O

What	O
about	O
plt.title	B-API
?	O

Is	O
there	O
a	O
way	O
to	O
darken	O
or	O
make	O
smoother	O
the	O
HSV	B-API
colours	O
so	O
they	O
look	O
more	O
like	O
this	O

plt.plot	B-API
(	O
arr	O
[	O
'	O
lapse_time	O
']	O
,	O
arr	O
[	O
'	O
contact_angle	O
'])	O

We	O
are	O
looking	O
for	O
some	O
`	O
.line	B-API
`	O
objects	O
and	O
there	O
are	O
two	O
.	O

My	O
`	O
rcParams	B-API
`	O
are	O
not	O
read	O
.	O

Now	O
,	O
it	O
says	O
`	O
get_data	B-API
`	O
is	O
not	O
defined	O
...	O
after	O
I	O
move	O
`	O
my_plot	O
`	O
before	O
the	O
loop	O

Xaxis	B-API
interval	O
y1	O
(	O
y2	O
)	O
ax2	O
:	O
(	O
-	O
100000.0	O
,	O
800000.0	O
)	O
(	O
wrong	O
)	O

Then	O
define	O
a	O
method	O
`	O
show()	B-API
`	O
that	O
calls	O
`	O
plt.show()	B-API
`	O
.	O

`	O
plt.setp	B-API
(	O
plt.gca()	B-API
,	O
'	O
yticklabels	O
'	O
,	O
ylabels	O
)`	O

matplotlib.get_backend()	B-API
MacOSX	O

#	O
matplotlib.use	B-API
(	O
'	O
tkagg	O
')	O
;	O

From	O
the	O
`	O
savefig()	B-API
`	O
docs	O
,	O
#CODE	O

You	O
might	O
also	O
want	O
to	O
look	O
into	O
[	O
`	O
numpy.vectorize	B-API
`]	O
(	O
#URL	O
)	O
.	O

'	O
and	O
'	O
plt.figure()	B-API
;	O
for	O
result	O
in	O
results	O
:	O
plt.clf()	B-API
...	O

Python	O
,	O
matplotlib	O
pyplot	B-API
show()	B-API
not	O
blocking	O

According	O
to	O
the	O
docs	O
`	O
numpy.loadtxt	B-API
`	O
is	O

What	O
is	O
ravel()	B-API
?	O

`	O
plot()	B-API
`	O
in	O
`	O
pandas	O
`	O
are	O
build	O
on	O
`	O
matplotlib	O
`	O
.	O

`	O
plt.colorbar()	B-API
`	O
?	O

from	O
matplotlib.ticker	B-API
import	O
LinearLocator	B-API
,	O
FormatStrFormatter	B-API

Is	O
matplotlib.pyplot.hist()	B-API
what	O
you	O
are	O
looking	O
for	O
?	O

File	O
"	O
/	O
usr	O
/	O
lib	O
/	O
python2.7	O
/	O
dist-packages	O
/	O
pandas	O
/	O
core	O
/	O
generic.py	O
"	O
,	O
line	O
2018	O
,	O
in	O
astype	B-API

Perhaps	O
you	O
should	O
be	O
calling	O
`	O
axvline	B-API
`	O
on	O
`	O
ax2	O
`	O
instead	O
of	O
calling	O
the	O
`	O
pyplot	B-API
`	O
method	O
?	O

(	O
`	O
np.all	B-API
(	O
np.diff	B-API
(	O
lats	O
,	O
axis=0	O
)	O
>	O
0	O
))`	O
,	O
similar	O
fro	O
`	O
lons	O
`)	O

The	O
problem	O
is	O
'	O
cause	O
my	O
np.mgrid	B-API
should	O
vary	O
from	O
-1	O
to	O
1	O
and	O
have	O
the	O
self.width	O
and	O
self.height	O
.	O

awesome	O
.	O
and	O
thanks	O
for	O
the	O
np.transpose	B-API
tip	O
,	O
too	O
.	O

And	O
using	O
`	O
set_bad	B-API
`	O
,	O
`	O
set_under	B-API
`	O
and	O
`	O
set_over	B-API
`	O
is	O
not	O
a	O
very	O
good	O
option	O
as	O
I	O
would	O
like	O
to	O
have	O
a	O
possibility	O
to	O
mark	O
different	O
pixels	O
with	O
different	O
colours	O
,	O
and	O
this	O
limits	O
their	O
number	O
to	O
3	O
.	O

@USER	O
numpy's	O
`	O
memmap	B-API
`	O
will	O
probably	O
not	O
be	O
useful	O
for	O
you	O
.	O

How	O
about	O
using	O
`	O
interpolate()	B-API
`	O
:	O
#CODE	O

cb.ax.set_major_formatter	O
(	O
ticker.FuncFormatter	B-API
(	O
myfmt	O
))	O

special	O
method	O
`	O
__call__()	B-API
`	O
is	O
a	O
good	O
approach	O
for	O
such	O

`	O
plt.tight_layout()	B-API
`	O
might	O
do	O
it	O
.	O

Unfortunately	O
,	O
I	O
don't	O
think	O
you	O
can	O
simply	O
do	O
np.minimum	B-API
(	O
array1	O
,	O
array2	O
,	O
array3	O
)	O
like	O
to	O
have	O
above	O
,	O
so	O
I	O
think	O
you	O
need	O
to	O
nest	O
the	O
np.minimum	B-API
calls	O
.	O

Using	O
`	O
set_offsets	B-API
`	O
doesn't	O
seem	O
to	O
behave	O
as	O
I	O
expect	O
it	O
to	O
.	O

Does	O
the	O
import	O
of	O
Axes3D	O
do	O
something	O
behind	O
the	O
scenes	O
to	O
alter	O
the	O
import	O
of	O
pyplot	B-API
?	O

`	O
get_color()	B-API
`	O
just	O
returns	O
the	O
color	O
attribute	O
from	O
a	O
line	O
.	O

plt.tight_layout()	B-API
`	O

(	O
Now	O
I	O
see	O
:	O
`	O
linspace	B-API
`	O
is	O
even	O
mentioned	O
in	O
`	O
arange	B-API
`'	O
s	O
docstring	O
...	O
)	O

By	O
default	O
`	O
animation.MovieWriter	B-API
`	O
uses	O
a	O
`	O
subprocess.PIPE	O
`	O
to	O
feed	O
the	O
frames	O
to	O
the	O
writer	O
.	O

With	O
your	O
`	O
matplotlibrc	O
`	O
file	O
,	O
the	O
variables	O
`	O
mp.rcParams	O
[	O
'	O
lines.linestyle	O
']`	O
and	O
`	O
mp.rcParams	O
[	O
'	O
axes.grid	B-API
']`	O
are	O
correctly	O
specified	O
and	O
this	O
works	O
as	O
expected	O
...	O

pyplot	B-API
interface	O

(	O
and	O
`	O
set_yticklabels	B-API
`	O
for	O
the	O
y-axis	O
)	O

but	O
`	O
from	O
matplotlib.path	B-API
import	O
Path	B-API
`	O
throws	O
#CODE	O

The	O
Path	B-API
object	O
does	O
not	O
store	O
the	O
points	O
along	O
a	O
Bezier	O
curve	O
,	O
just	O
the	O
minimum	O
parameters	O
it	O
needs	O
.	O

Users	O
of	O
Path	B-API
objects	O
should	O
not	O
access	O
the	O
vertices	O
and	O
codes	O
arrays	O

The	O
trick	O
is	O
to	O
use	O
Path	B-API
and	O
PathPatch	B-API
.	O

This	O
allows	O
you	O
to	O
get	O
the	O
Matplotlib	O
Path	B-API
vertices	O
and	O
codes	O
in	O
the	O
projection	O
coordinates	O
which	O
you	O
can	O
then	O
convert	O
into	O
a	O
new	O
Path	B-API
.	O

I	O
was	O
playing	O
with	O
PathCollection	B-API
(	O
my	O
dyslexia	O
is	O
killing	O
me	O
switching	O
between	O
Path	B-API
and	O
Patch	O
)	O
,	O
and	O
if	O
I	O
can	O
get	O
a	O
unit	O
circle	O
drawn	O
and	O
I	O
can	O
apply	O
an	O
affine	O
transformation	O
to	O
it	O
,	O
then	O
I	O
think	O
it	O
should	O
work	O
,	O
as	O
it	O
doesn't	O
seem	O
to	O
inherently	O
fill	O
the	O
space	O
.	O

Thus	O
the	O
`	O
Spine	B-API
`	O
objects	O
are	O
in	O
question	O

You	O
can	O
use	O
the	O
`	O
set_position()	B-API
`	O
method	O
of	O
the	O
`	O
Spine	B-API
`	O
class	O
:	O
#CODE	O

You	O
mentioned	O
the	O
use	O
of	O
`	O
Locator	B-API
`	O
and	O
`	O
Formatter	B-API
`	O
objects	O
in	O
your	O
comment	O
.	O

You	O
need	O
to	O
call	O
remove	O
on	O
the	O
instance	O
of	O
the	O
`	O
Annotation	B-API
`	O
object	O
.	O

Using	O
Annotation	B-API
Instead	O
of	O
Ticklabels	O

What	O
is	O
the	O
difference	O
between	O
a	O
Text	B-API
instance	O
and	O
string	O
in	O
python	O
?	O

I	O
believe	O
the	O
anti-aliasing	O
of	O
Text	B-API
objects	O
is	O
up	O
to	O
the	O
font	O
engine	O
being	O
used	O
.	O

What	O
you	O
should	O
do	O
is	O
save	O
a	O
reference	O
to	O
the	O
first	O
`	O
Text	B-API
`	O
object	O
and	O
update	O
its	O
contents	O
by	O
calling	O
its	O
`	O
set_text()	B-API
`	O
method	O
.	O

There	O
is	O
a	O
references	O
to	O
the	O
Text	B-API
object	O
returned	O
by	O
the	O
original	O
setting	O
of	O
suptitle	B-API
in	O
figure.texts	O
.	O

I'm	O
having	O
a	O
problem	O
with	O
the	O
Text	B-API
object	O
that	O
matplotlib	O
use	O
to	O
represent	O
the	O
ticklabels	O
.	O

I	O
think	O
you	O
need	O
to	O
displace	O
the	O
`	O
Text	B-API
`	O
object	O
,	O
using	O
the	O
`	O
set_position	B-API
((	O
x	O
,	O
y	O
))`	O
method	O
.	O

This	O
includes	O
`	O
Text	B-API
`	O
objects	O
,	O
`	O
Line2D	B-API
`	O
objects	O
,	O
`	O
collection	O
`	O
objects	O
,	O
`	O
Patch	O
`	O
objects	O
...	O

Text	B-API
object	O
in	O
matplotlib	O
doesnt	O
respond	O
to	O
zooming	O
properly	O

After	O
drawing	O
a	O
matplotlib	O
Text	B-API
instance	O
and	O
then	O
interactively	O
panning	O
,	O
the	O
resulting	O
drawn	O
text	O
is	O
clipped	O
to	O
the	O
data	O
window	O
but	O
not	O
the	O
surrounding	O
bounding	O
box	O
.	O

Something	O
like	O
`	O
Text	B-API
(	O
2	O
,	O
0	O
,	O
u'Text	O
(	O
2	O
,	O
0	O
,	O
u	O
"	O
Text	B-API
(	O
0.4	O
,	O
u\	O
'	O
0.4	O
\	O
')")')`	O
.	O

`	O
plt.legend	B-API
`	O
returns	O
a	O
`	O
Legend	B-API
`	O
object	O
with	O
methods	O
that	O
allow	O
you	O
to	O
modify	O
the	O
appearance	O
of	O
the	O
legend	O
.	O

So	O
first	O
we'll	O
save	O
the	O
`	O
Legend	B-API
`	O
object	O
:	O
#CODE	O

In	O
practice	O
I	O
seem	O
to	O
end	O
up	O
mixing	O
them	O
both	O
myself	O
in	O
SW	O
;	O
it's	O
largely	O
a	O
matter	O
of	O
taste	O
whether	O
you	O
go	O
through	O
the	O
pyplot	B-API
API	O
or	O
access	O
the	O
objects	O
.	O
pyplot	B-API
is	O
certainly	O
very	O
convenient	O
although	O
as	O
you	O
want	O
to	O
do	O
more	O
complex	O
/	O
exotic	O
things	O
you'll	O
find	O
what	O
you	O
can	O
do	O
with	O
pyplot	B-API
alone	O
limited	O
and	O
you'll	O
need	O
to	O
get	O
to	O
know	O
at	O
least	O
the	O
full	O
API's	O
Axes	B-API
,	O
Figure	B-API
,	O
Legend	B-API
and	O
Path	B-API
objects	O
better	O
.	O

There	O
was	O
a	O
refactor	O
of	O
the	O
`	O
Legend	B-API
`	O
class	O
awhile	O
back	O
.	O

call	O
`	O
Legend.get_texts()	B-API
`	O
will	O
get	O
a	O
list	O
of	O
Text	B-API
object	O
in	O
the	O
legend	B-API
object	O
:	O
#CODE	O

Some	O
parameters	O
could	O
easily	O
be	O
read	O
from	O
the	O
`	O
Legend	B-API
`	O
object	O
,	O
others	O
(	O
like	O
`	O
title	O
`	O
,	O
`	O
fancybox	O
`)	O
required	O
some	O
'	O
artistics	O
'	O
.	O

42	O
from	O
matplotlib.legend	B-API
import	O
Legend	B-API

How	O
do	O
I	O
choose	O
the	O
optimum	O
width	O
for	O
a	O
matplotlib	O
Button	B-API
?	O

I	O
removed	O
`	O
color=axcolor	O
`	O
from	O
the	O
`	O
Button	B-API
`	O
call	O
;	O
and	O
I	O
added	O
a	O
`	O
plt.show()	B-API
`	O
before	O
the	O
event	O
connect	O
,	O
otherwise	O
a	O
figure	O
window	O
didn't	O
appear	O
for	O
me	O
(	O
neither	O
through	O
`	O
ipython	O
`	O
,	O
nor	O
with	O
`	O
python	O
`)	O
.	O

button	O
=	O
Button	B-API
(	O
ax=reset_axis	O
,	O
label=	O
'	O
Reset	O
'	O
,	O
color=	O
'	O
lightblue	O
'	O
,	O
hovercolor=	O
'	O
0.975	O
')	O

`	O
tbar.add_button	O
(	O
a	O
Button	B-API
object	O
);	O
`	O

For	O
your	O
buttons	O
to	O
work	O
,	O
you	O
need	O
to	O
keep	O
a	O
reference	O
to	O
the	O
`	O
Button	B-API
`	O
object	O
around	O
.	O

I	O
know	O
how	O
to	O
add	O
and	O
work	O
with	O
single	O
cursor	O
`	O
self.cursor	O
=	O
Cursor	B-API
(	O
self.static_canvas.Dataplot	O
,	O
useblit=True	O
,	O
color=	O
'	O
red	O
'	O
,	O
linewidth=2	O
)`	O
,	O
but	O
what	O
I	O
should	O
do	O
to	O
create	O
to	O
cursors	O
?	O

I	O
think	O
tillsten	O
is	O
right	O
--	O
study	O
how	O
the	O
[	O
`	O
Cursor	B-API
`	O
class	O
]	O
(	O
#URL	O
)	O
does	O
it	O
.	O

I've	O
checked	O
briefly	O
,	O
but	O
I	O
don't	O
know	O
how	O
to	O
modify	O
the	O
properties	O
of	O
an	O
`	O
Arc	B-API
`	O
instance	O
,	O
although	O
I'm	O
sure	O
it's	O
possible	O
.	O

You	O
could	O
also	O
try	O
passing	O
a	O
`	O
Arrow	B-API
`	O
instance	O
as	O
marker	O
,	O
but	O
I'm	O
not	O
sure	O
whether	O
that	O
works	O
.	O

Currently	O
,	O
I	O
don't	O
think	O
it	O
possible	O
as	O
the	O
`	O
Arrow	B-API
`	O
class	O
only	O
supports	O
`'	O
solid	O
'	O
|	O
'	O
dashed	O
'	O
|	O
'	O
dashdot	O
'	O
|	O
'	O
dotted	O
'`	O
four	O
different	O
linestyles	O
.	O

The	O
Node	B-API
class	O
has	O
the	O
coordinates	O
stored	O
as	O
members	O
(	O
x	O
and	O
y	O
)	O
,	O
as	O
well	O
as	O
index	O
.	O

You	O
can	O
scale	O
your	O
z	O
values	O
to	O
fit	O
this	O
range	O
with	O
Normalize	B-API
.	O

You	O
should	O
mention	O
the	O
`	O
Normalize	B-API
`	O
methods	O
here	O
as	O
well	O
.	O

Matplotlib	O
provides	O
the	O
class	O
`	O
Normalize	B-API
`	O
for	O
that	O
:	O
#CODE	O

Create	O
a	O
new	O
instance	O
of	O
`	O
Normalize	B-API
`	O
for	O
each	O
image	O
you	O
want	O
to	O
process	O
.	O

And	O
then	O
you	O
can	O
also	O
replicate	O
the	O
functionality	O
of	O
`	O
Normalize	B-API
`	O
(	O
since	O
you	O
seem	O
to	O
not	O
like	O
it	O
):	O
#CODE	O

Use	O
these	O
to	O
create	O
a	O
`	O
Normalize	B-API
`	O
instance	O
(	O
other	O
normalisation	O
classes	O
are	O
available	O
,	O
e.g.	O
log	O
scale	O
)	O
.	O

The	O
documentation	O
of	O
Normalize	B-API
might	O
be	O
a	O
bit	O
deceiving	O
here	O
:	O
`	O
process_value	O
`	O
is	O
a	O
function	O
which	O
is	O
only	O
used	O
for	O
preprocessing	O
(	O
and	O
static	O
)	O
.	O

I	O
have	O
several	O
questions	O
regarding	O
the	O
Normalize	B-API
class	O
in	O
Matplotlib	O
.	O

with	O
your	O
own	O
`	O
Normalize	B-API
`	O
function	O
.	O

This	O
might	O
be	O
why	O
they've	O
avoided	O
implementing	O
it	O
as	O
part	O
of	O
Normalize	B-API
.	O

matplotlib	O
:	O
can	O
I	O
create	O
AxesSubplot	O
objects	O
,	O
then	O
add	O
them	O
to	O
a	O
Figure	B-API
instance	O
?	O

Also	O
do	O
you	O
use	O
Figure	B-API
from	O
the	O
local	O
matplotlib	O
installation	O
or	O
not	O
?	O

Additionally	O
,	O
there	O
are	O
functions	O
from	O
the	O
pyplot	B-API
interface	O
and	O
there	O
are	O
methods	O
on	O
the	O
`	O
Figure	B-API
`	O
class	O
.	O

I'll	O
assume	O
in	O
the	O
following	O
that	O
`	O
fig	O
`	O
is	O
an	O
instance	O
of	O
a	O
`	O
Figure	B-API
`	O
:	O

Both	O
the	O
Figure	B-API
and	O
the	O
Axes	B-API
class	O
have	O
a	O
`	O
clear()	O
`	O
method	O
.	O

I	O
had	O
this	O
same	O
problem	O
,	O
and	O
it	O
was	O
caused	O
by	O
calling	O
`	O
show()	O
`	O
on	O
the	O
Figure	B-API
object	O
instead	O
of	O
the	O
pyplot	B-API
object	O
.	O

It	O
got	O
wrapped	O
up	O
in	O
a	O
subclass	O
of	O
Figure	B-API
,	O
which	O
also	O
got	O
level-of-detail	O
functionality	O
and	O
context	O
menus	O
.	O

I	O
have	O
an	O
external	O
function	O
which	O
returns	O
a	O
Figure	B-API
object	O
,	O
and	O
in	O
this	O
situation	O
,	O
each	O
Figure	B-API
object	O
is	O
composed	O
of	O
exactly	O
one	O
Axes	B-API
object	O
.	O

While	O
a	O
search	O
did	O
lead	O
me	O
to	O
see	O
that	O
pyplot.figure()	B-API
is	O
the	O
development	O
team's	O
recommended	O
instantiation	O
technique	O
,	O
it	O
doesn't	O
change	O
the	O
question	O
:	O
is	O
there	O
any	O
way	O
to	O
do	O
Axes	B-API
/	O
Figure	B-API
copy	O
construction	O
and	O
Figure	B-API
construction	O
via	O
composition	O
of	O
copied	O
Axes	B-API
?	O

A	O
reference	O
to	O
it	O
isn't	O
stored	O
in	O
the	O
`	O
Figure	B-API
`	O
object	O
,	O
but	O
is	O
stored	O
in	O
the	O
`	O
Canvas	O
`	O
object	O
so	O
you	O
can	O
destroy	O
a	O
window	O
via	O
#CODE	O

Removing	O
the	O
`	O
frameon=False	O
`	O
option	O
from	O
`	O
f	O
=	O
Figure	B-API
(	O
figsize	O
=(	O
12	O
,	O
5	O
)	O
,	O
dpi=100	O
,	O
frameon=False	O
)`	O
solved	O
the	O
issue	O
.	O

I	O
used	O
the	O
Bar	B-API
class	O
to	O
plot	O
length	O
of	O
videos	O
vs	O
views	O
which	O
are	O
the	O
x	O
and	O
y	O
values	O
of	O
the	O
tuples	O
.	O

Animation	B-API
will	O
not	O
work	O
on	O
inplace	O
(	O
or	O
inline	O
?	O
)	O
on	O
notebook	O
.	O

By	O
artifacts	O
,	O
I	O
now	O
understand	O
to	O
mean	O
you	O
are	O
creating	O
a	O
second	O
`	O
Animation	B-API
`	O
object	O
and	O
what	O
you	O
get	O
is	O
both	O
of	O
them	O
running	O
in	O
parallel	O
(	O
which	O
I	O
am	O
not	O
sure	O
what	O
I	O
expect	O
to	O
happen	O
there	O
)	O
.	O

by	O
`	O
ax	O
`	O
I	O
mean	O
the	O
current	O
`	O
Axis	B-API
`	O
object	O
,	O
which	O
you	O
were	O
getting	O
at	O
via	O
`	O
gca	B-API
`	O
.	O

Your	O
legend	B-API
command	O
is	O
using	O
the	O
markers	O
,	O
not	O
the	O
lines	O
as	O
inputs	O
by	O
using	O
`	O
plot	O
[	O
0	O
]`	O
.	O

`	O
Axes	B-API
`	O
objects	O
know	O
about	O
things	O
like	O
tick	O
location	O
and	O
labels	O
and	O
the	O
display	O
range	O
(	O
which	O
it	O
does	O
by	O
knowing	O
about	O
`	O
Axis	B-API
`	O
object	O
,	O
but	O
that	O
is	O
getting	O
even	O
more	O
into	O
the	O
weeds	O
)	O
.	O

Use	O
Axis	B-API
method	O
`	O
set_xscale	B-API
`	O
or	O
`	O
set_yscale	B-API
`	O
.	O

`	O
ax.set_xticks()	B-API
`	O
is	O
a	O
method	O
of	O
the	O
`	O
Axes	B-API
`	O
object	O
,	O
whilst	O
`	O
matplotlib.ticker.FixedLocator	B-API
`	O
is	O
an	O
object	O
which	O
is	O
used	O
with	O
the	O
`	O
Axis	B-API
`	O
object	O
(	O
`	O
Axes	B-API
`	O
is	O
basically	O
the	O
whole	O
graph	O
,	O
whilst	O
`	O
Axis	B-API
`	O
is	O
just	O
one	O
of	O
the	O
(	O
two	O
)	O
axis	O
(	O
x	O
or	O
y	O
)	O
.	O

This	O
is	O
not	O
particularly	O
well	O
documented	O
,	O
but	O
`	O
Polygon	B-API
`	O
objects	O
have	O
a	O
pair	O
of	O
methods	O
`	O
get_xy	B-API
`	O
and	O
`	O
set_xy	B-API
`	O
.	O

In	O
particular	O
,	O
if	O
/	O
when	O
there	O
are	O
so	O
many	O
circles	O
for	O
the	O
figure	O
to	O
become	O
connected	O
,	O
the	O
`	O
for	O
polygon	O
in	O
polygons	O
:	O
`	O
fails	O
with	O
a	O
`	O
TypeError	O
:	O
'	O
Polygon	B-API
'	O
object	O
is	O
not	O
iterable	O
`	O
.	O

Just	O
use	O
the	O
`	O
Polygon	B-API
`	O
or	O
`	O
Rectangle	B-API
`	O
classes	O
:	O
#CODE	O

As	O
you	O
can	O
see	O
,	O
the	O
edge	O
is	O
center-positioned	O
along	O
the	O
border	O
of	O
the	O
domain	O
of	O
the	O
Rectangle	B-API
object	O
,	O
and	O
so	O
bleeds	O
into	O
this	O
domain	O
.	O

You	O
can	O
do	O
this	O
by	O
overplotting	O
a	O
Rectangle	B-API
patch	O
on	O
the	O
cell	O
that	O
you	O
would	O
want	O
to	O
highlight	O
.	O

Currently	O
I	O
use	O
the	O
`	O
add_patch	B-API
(	O
Rectangle	B-API
(	O
...	O
))`	O
,	O
but	O
it	O
does	O
not	O
fit	O
really	O
well	O
.	O

Unfortunately	O
it	O
seems	O
that	O
the	O
'	O
Rectangle	B-API
'	O
object	O
has	O
no	O
attribute	O
'	O
set_bottom	O
'	O
.	O

You	O
can	O
acces	O
ALL	O
the	O
properties	O
of	O
a	O
`	O
Tick	B-API
`	O
object	O
using	O
this	O
approach	O
:	O
#CODE	O

Fortunately	O
you	O
can	O
update	O
figures	O
you've	O
moved	O
to	O
where	O
you	O
want	O
them	O
pretty	O
easily	O
,	O
by	O
using	O
the	O
object	O
interface	O
specifically	O
,	O
and	O
updating	O
the	O
Axes	B-API
object	O
without	O
creating	O
a	O
new	O
figure	O
.	O

(	O
You	O
can	O
also	O
always	O
find	O
this	O
data	O
inside	O
the	O
`	O
Axes	B-API
`	O
object	O
if	O
you	O
know	O
where	O
to	O
look	O
.	O
)	O

By	O
deferring	O
the	O
expense	O
of	O
drawing	O
you	O
can	O
greatly	O
improve	O
the	O
performance	O
of	O
functions	O
that	O
make	O
many	O
calls	O
to	O
`	O
Figure	B-API
`	O
and	O
`	O
Axes	B-API
`	O
methods	O
.	O

`	O
Figure.add_subplot()	B-API
`	O
returns	O
an	O
`	O
Axes	B-API
`	O
instance	O
,	O
and	O
`	O
pyplot.subplots()	B-API
`	O
returns	O
an	O
`	O
Axis	B-API
`	O
object	O
as	O
second	O
output	O
parameter	O
.	O

As	O
a	O
side	O
note	O
,	O
it	O
is	O
better	O
to	O
pass	O
`	O
Axes	B-API
`	O
objects	O
into	O
your	O
function	O
than	O
create	O
them	O
(	O
implicitly	O
)	O
internally	O
.	O

Here	O
,	O
what	O
you	O
have	O
done	O
is	O
capture	O
the	O
`	O
Axes	B-API
`	O
instance	O
that	O
is	O
returned	O
from	O
`	O
add_subplot()	B-API
`	O
.	O

If	O
I	O
create	O
an	O
`	O
Axes	B-API
`	O
object	O
in	O
`	O
matplotlib	O
`	O
and	O
mutate	O
it	O
(	O
i.e.	O
by	O
plotting	O
some	O
data	O
)	O
and	O
then	O
I	O
call	O
a	O
function	O
without	O
passing	O
my	O
`	O
Axes	B-API
`	O
object	O
to	O
that	O
function	O
then	O
that	O
function	O
can	O
still	O
mutate	O
my	O
`	O
Axes	B-API
`	O
.	O

Pass	O
multiple	O
matplotlib	O
Axes	B-API
objects	O
from	O
generator	O
function	O
and	O
display	O
them	O

The	O
`	O
transform	B-API
`	O
in	O
this	O
case	O
is	O
a	O
`	O
BboxTransformTo	O
`	O
object	O
,	O
which	O
:	O

You	O
can	O
use	O
the	O
`	O
transform	B-API
`	O
keyword	O
:	O
#CODE	O

But	O
when	O
I	O
checked	O
the	O
source	O
code	O
of	O
draw_networkx_nodes	O
draw_networkx	O
,	O
I	O
realized	O
that	O
it	O
is	O
not	O
a	O
straight	O
forward	O
task	O
as	O
the	O
draw	B-API
function	O
stores	O
the	O
positions	O
(	O
nodes	O
and	O
edges	O
)	O
in	O
a	O
numpy	O
array	O
,	O
send	O
it	O
to	O
the	O
ax.scatter	B-API
function	O
of	O
matplotlib	O
(	O
sourcecode	O
)	O
which	O
is	O
a	O
bit	O
hard	O
to	O
manipulate	O
without	O
messing	O
something	O
up	O
.	O

This	O
simply	O
calls	O
the	O
draw	B-API
method	O
of	O
the	O
figure	O
periodically	O
.	O

Then	O
I	O
just	O
added	O
a	O
signal	O
to	O
the	O
custom	O
version	O
and	O
overrode	O
the	O
draw	B-API
method	O
.	O

EDIT	O
:	O
I'm	O
super	O
blind	O
,	O
sorry	O
for	O
that	O
,	O
you're	O
calling	O
the	O
draw	B-API
method	O
,	O
however	O
it	O
is	O
a	O
good	O
idea	O
to	O
add	O
*	O
args	O
and	O
**	O
kwargs	O
to	O
any	O
overriden	O
methods	O
..	O
try	O
that	O
,	O
and	O
perhaps	O
call	O
FancyBboxPatch.draw	O
at	O
the	O
end	O
of	O
overriden	O
method	O

I'm	O
also	O
panning	O
and	O
zooming	O
into	O
this	O
figure	O
,	O
and	O
have	O
been	O
using	O
the	O
draw	B-API
method	O
to	O
show	O
the	O
new	O
perspectives	O
from	O
zooming	O
in	O
(	O
using	O
set_xlim	B-API
and	O
set_ylim	B-API
)	O
and	O
from	O
panning	O
(	O
drag_pan	B-API
and	O
start_pan	B-API
)	O
.	O

I	O
would	O
use	O
`	O
ax.autoscale	B-API
(	O
enable=False	O
)`	O
before	O
your	O
call	O
to	O
`	O
scatter	B-API
`	O
.	O

You	O
can	O
perhaps	O
loop	O
into	O
your	O
`	O
AxesSubplot	O
`	O
objects	O
and	O
call	O
`	O
autoscale	B-API
`	O
passing	O
the	O
`	O
axis	O
`	O
parameter	O
:	O
#CODE	O

@USER	O
-	O
If	O
you'd	O
like	O
to	O
rescale	O
the	O
axes	B-API
,	O
call	O
`	O
ax.autoscale()	B-API
`	O
every	O
time	O
you	O
remove	O
a	O
point	O
.	O

Python	O
:	O
Add	O
x-y	O
margins	B-API
automatically	O
with	O
autoscale	B-API
(	O
pyplot	B-API
)	O

Ahh	O
,	O
it's	O
the	O
call	O
to	O
`	O
autoscale	B-API
`	O
that	O
I'm	O
missing	O
.	O

The	O
trick	O
is	O
at	O
the	O
end	O
,	O
in	O
the	O
custom	O
refresh	B-API
method	O
!	O

For	O
what	O
you're	O
wanting	O
to	O
do	O
,	O
you'd	O
be	O
far	O
better	O
off	O
using	O
annotate	B-API
.	O

It's	O
quite	O
simple	O
to	O
do	O
manually	O
with	O
`	O
annotate	B-API
`	O
.	O

If	O
you're	O
wanting	O
to	O
adjust	O
vertical	O
alignment	O
,	O
horizontal	O
alignment	O
,	O
etc	O
,	O
just	O
add	O
those	O
as	O
arguments	O
to	O
`	O
annotate	B-API
`	O
(	O
e.g.	O
`	O
horizontalalignment=	O
'	O
right	O
'`	O
or	O
equivalently	O
`	O
ha=	O
'	O
right	O
'`)	O

It's	O
quite	O
simple	O
to	O
do	O
manually	O
with	O
`	O
annotate	B-API
`	O
.	O

You	O
can	O
use	O
the	O
annotate	B-API
command	O
to	O
place	O
text	O
annotations	O
at	O
any	O
x	O
and	O
y	O
values	O
you	O
want	O
.	O

+1	O
Just	O
as	O
a	O
side	O
note	O
,	O
annotate	B-API
has	O
"	O
offseting	O
the	O
annotations	O
a	O
little	O
"	O
built-in	O
.	O

While	O
there's	O
nothing	O
wrong	O
with	O
Ofri's	O
answer	O
,	O
`	O
annotate	B-API
`	O
is	O
intended	O
especially	O
for	O
this	O
purpose	O
:	O
#CODE	O

Just	O
use	O
`	O
annotate	B-API
`	O
and	O
specify	O
axis	O
coordinates	O
.	O

It	O
is	O
also	O
possible	O
to	O
make	O
an	O
arrow	O
with	O
text	O
,	O
using	O
the	O
annotate	B-API
method	O
.	O

The	O
closest	O
thing	O
I	O
have	O
been	O
able	O
to	O
find	O
while	O
searching	O
here	O
is	O
the	O
annotate	B-API
command	O
,	O
but	O
that	O
appears	O
to	O
create	O
a	O
fixed	O
label	O
on	O
the	O
plot	O
.	O

`	O
plt.annotate	B-API
(	O
'	O
Something	O
'	O
,	O
(	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
-20	O
)	O
,	O
xycoords=	O
'	O
axes	B-API
fraction	O
'	O
,	O
textcoords=	O
'	O
offset	O
points	O
'	O
,	O
va=	O
'	O
top	O
')`	O

@USER	O
I'm	O
not	O
sure	O
why	O
we	O
have	O
to	O
explicitly	O
call	O
`	O
draw	B-API
`	O
.	O

The	O
`	O
boxplot	B-API
`	O
function	O
only	O
plots	O
quartiles	O
(	O
0	O
,	O
25	O
,	O
50	O
,	O
75	O
,	O
100	O
)	O
.	O

Is	O
there	O
any	O
way	O
I	O
can	O
extract	O
these	O
values	O
for	O
use	O
in	O
my	O
downstream	O
code	O
from	O
the	O
boxplot	B-API
object	O
?	O

I	O
am	O
plotting	O
a	O
non-normal	O
distribution	O
using	O
boxplot	B-API
and	O
interested	O
in	O
finding	O
out	O
about	O
outliers	O
using	O
boxplot	B-API
function	O
of	O
matplotlib	O
.	O

You	O
should	O
use	O
the	O
`	O
hist	B-API
`	O
function	O
.	O

An	O
similar	O
example	O
of	O
such	O
plot	O
is	O
"	O
hist	B-API
(	O
x	O
,	O
orientation=	O
'	O
horizontal	O
')"	O
.	O

I	O
need	O
to	O
use	O
the	O
`	O
hist	B-API
`	O
function	O
,	O
I	O
have	O
looked	O
elsewhere	O
on	O
this	O
site	O
but	O
I	O
haven't	O
found	O
anything	O
.	O

I	O
tried	O
doing	O
hist	B-API
(	O
x	O
/	O
x.sum()	O
)	O
but	O
that	O
reduce	O
the	O
values	O
of	O
the	O
numbers	O
in	O
the	O
x	O
axis	O
and	O
does	O
not	O
change	O
their	O
frequency	O
,	O
so	O
the	O
y-axis	O
is	O
unchanged	O

How	O
can	O
I	O
prevent	O
the	O
labels	O
of	O
`	O
xticks	B-API
`	O
from	O
overlapping	O
with	O
the	O
labels	O
of	O
`	O
yticks	B-API
`	O
when	O
using	O
`	O
hist	B-API
`	O
(	O
or	O
other	O
plotting	O
commands	O
)	O
in	O
matplotlib	O
?	O

An	O
easy	O
way	O
to	O
do	O
this	O
is	O
to	O
add	O
`	O
0j	O
`	O
to	O
the	O
argument	O
of	O
the	O
`	O
sqrt	B-API
`	O
,	O
like	O
this	O
,	O
`	O
sqrt	B-API
(	O
1+0j-	O
(	O
1	O
/	O
x	O
)	O
**2	O
)`	O
.	O

You	O
need	O
to	O
set	O
the	O
new	O
coordinates	O
of	O
the	O
datapoints	O
in	O
the	O
internal	O
`	O
_ofsets3d	O
`	O
variable	O
of	O
the	O
`	O
Line3DCollection	O
`	O
object	O
returned	O
by	O
the	O
`	O
scatter	B-API
`	O
function	O
.	O

You	O
can	O
use	O
`	O
scatter	B-API
`	O
for	O
this	O
,	O
but	O
that	O
requires	O
having	O
numerical	O
values	O
for	O
your	O
`	O
key1	O
`	O
,	O
and	O
you	O
won't	O
have	O
a	O
legend	B-API
,	O
as	O
you	O
noticed	O
.	O

The	O
`	O
scatter	B-API
`	O
and	O
`	O
hist	B-API
`	O
commands	O
really	O
expect	O
`	O
x	O
`	O
and	O
`	O
y	O
`	O
to	O
be	O
1D	O
arrays	O
.	O

I'm	O
wondering	O
if	O
it	O
is	O
possible	O
to	O
have	O
individual	O
alpha	O
values	O
for	O
each	O
point	O
to	O
be	O
plotted	O
using	O
the	O
scatter	B-API
function	O
of	O
Matplotlib	O
.	O

The	O
problem	O
happens	O
when	O
I	O
call	O
the	O
scatter	B-API
function	O
.	O

I	O
tried	O
adding	O
`	O
/	O
Library	O
/	O
TeX	O
/	O
Root	O
/	O
bin	O
/	O
universal-darwin	O
`	O
to	O
the	O
Global	O
Python	O
Path	O
of	O
the	O
Project	O
Properties	O
,	O
but	O
I	O
still	O
get	O
the	O
same	O
errors	O
.	O

My	O
Python	O
Path	O
is	O
correct	O
C	O
:\	O
Python27_32bit\	O
and	O
I	O
also	O
have	O
\lib\	O
site-packages	O
\	O
and	O
\DLL	O
included	O
correctly	O
.	O

Adding	O
Python	O
Path	O
on	O
Windows	O
7	O

Append	O
`	O
;	O
C	O
:\	O
python27\Scripts	O
`	O
to	O
the	O
end	O
of	O
`	O
Path	O
`	O
variable	O

I	O
use	O
a	O
palette	O
file	O
that	O
I'm	O
able	O
to	O
create	O
using	O
GIMP	O
+	O
Sample	O
a	O
Gradient	O
Along	O
a	O
Path	O
.	O

Spine	O
position	O
is	O
specified	O
by	O
a	O
2	O
tuple	O
of	O
(	O
position	O
type	O
,	O
amount	O
)	O
.	O

Annotation	O
along	O
a	O
curve	O
in	O
matplotlib	O

Heat	O
Map	O
Annotation	O
with	O
text	O

Annotation	O
on	O
top	O
of	O
the	O
bars	O
show	O
the	O
actual	O
percentage	O
of	O
that	O
category	O
.	O

Text	O
File	O
.	O

I'm	O
using	O
Ubuntu	O
and	O
Sublime	O
Text	O
.	O

Check	O
this	O
page	O
:	O
Text	O
Rendering	O
with	O
LaTeX	O
.	O

When	O
using	O
Matplotlib	O
(	O
Pylab	O
)	O
for	O
rendering	O
Text	O
with	O
the	O
same	O
metrics	O
as	O
AriaL	O
(	O
e.g.	O
,	O
Arial	O
itself	O
or	O
Liberation	O
Sans	O
)	O
output	O
looks	O
ok	O
-	O
e.g.	O
,	O
the	O
width	O
of	O
the	O
legend	O
box	O
is	O
right	O
.	O

Text	O
box	O
with	O
line	O
wrapping	O
in	O
matplotlib	O
?	O

Python	O
/	O
Matplotlib	O
-	O
Quickly	O
Updating	O
Text	O
on	O
Axes	O

Text	O
display	O
problem	O
with	O
matplotlib	O
?	O

Network	O
from	O
Table	O
(	O
Text	O
/	O
MS	O
Excel	O
)	O
..	O

Text	O
alignment	O
in	O
a	O
Matplotlib	O
legend	O

Underlining	O
Text	O
in	O
Python	O
/	O
Matplotlib	O

(	O
Text	O
colour	O
comes	O
out	O
as	O
black	O
)	O
#CODE	O

However	O
,	O
when	O
I	O
paste	O
this	O
code	O
into	O
my	O
Wordpress	O
page	O
(	O
using	O
the	O
Text	O
editor	O
,	O
not	O
the	O
visual	O
editor	O
)	O
nothing	O
happens	O
.	O

This	O
generates	O
the	O
plot	O
but	O
the	O
Legend	O
B	O
is	O
placed	O
at	O
the	O
upper	O
right	O
hand	O
side	O
corner	O
and	O
Legend	O
A	O
is	O
by	O
default	O
placed	O
at	O
the	O
left	O
hand	O
upper	O
corner	O
.	O

Multiple	O
Pie	O
Charts	O
with	O
a	O
Shared	O
Legend	O

You	O
can	O
add	O
a	O
legend	O
to	O
only	O
the	O
first	O
axes	O
after	O
plotting	O
all	O
of	O
your	O
pie	O
charts	O
:	O
#CODE	O

Matplotlib	O
:	O
Legend	O
not	O
displayed	O
properly	O

Legend	O
colors	O
in	O
Matplotlib	O
table	O
function	O
?	O

Legend	O
transparency	O
,	O
when	O
using	O
secondary	O
axis	O

Legend	O
is	O
outside	O
the	O
frame	O
.	O

Button	O
1	O
:	O
Shows	O
the	O
pointscores	O
of	O
John	O
and	O
Daniel	O
in	O
level	O
1	O
.	O

How	O
to	O
get	O
multiple	O
Button	O
Click	O
Events	O
in	O
Python	O

Button	O
click	O
->	O
ser.write	O
,	O
ser.read	O
,	O
draw	O

Another	O
solution	O
is	O
print	O
a	O
'	O
Cursor	O
'	O
or	O
marker	O
line	O
on	O
the	O
plot	O
,	O
and	O
change	O
its	O
coordinates	O
with	O
the	O
mouse	O
events	O
.	O

An	O
Arrow	O
would	O
appear	O
with	O
data	O
about	O
the	O
invisible	O
line	O
one	O
,	O
but	O
he	O
wanted	O
info	O
about	O
line	O
two	O
.	O

Arrow	O
on	O
a	O
line	O
plot	O
with	O
matplotlib	O

Drawing	O
Arrow	O
in	O
(	O
x	O
,	O
y	O
)	O
coordinate	O
in	O
Python	O

To	O
be	O
more	O
specific	O
,	O
if	O
you	O
look	O
at	O
the	O
answer	O
to	O
that	O
question	O
it	O
shows	O
how	O
to	O
make	O
a	O
3D	O
Arrow	O
and	O
use	O
that	O
to	O
annotate	O
points	O
in	O
a	O
3D	O
plot	O
--	O
I	O
have	O
used	O
this	O
recipe	O
extensively	O
and	O
it	O
makes	O
me	O
wonder	O
how	O
it	O
isn't	O
built	O
into	O
matplotlib	O
yet	O
.	O

Node	O
positions	O
are	O
generated	O
,	O
stored	O
and	O
assigned	O
like	O
this	O
#CODE	O

Node	O
and	O
edge	O
attribute	O
files	O
are	O
simply	O
formatted	O
:	O
a	O
node	O
attribute	O
file	O
begins	O
with	O
the	O
name	O
of	O
the	O
attribute	O
on	O
the	O
first	O
line	O
(	O
note	O
that	O
it	O
cannot	O
contain	O
spaces	O
)	O
.	O

Normalize	O
the	O
input	O
data	O
and	O
scale	O
it	O

Normalize	O
your	O
data	O
.	O

Normalize	O
again	O

Normalize	O
your	O
data	O
,	O
apply	O
the	O
colormap	O
,	O
save	O
the	O
image	O
.	O
matplotlib	O
provides	O
all	O
the	O
necessary	O
functionality	O
:	O
#CODE	O

Normalize	O
histogram2d	B-API
by	O
bin	O
area	O

Python	O
/	O
Matplotlib	O
-	O
Figure	O
Borders	O
in	O
wxPython	O

gcf()	B-API
means	O
Get	O
Current	O
Figure	O

Figure	O
title	O
with	O
several	O
colors	O
in	O
matplotlib	O

However	O
,	O
the	O
sample	O
consists	O
of	O
distinct	O
solutions	O
which	O
form	O
lines	O
in	O
the	O
parameter	O
space	O
such	O
that	O
putting	O
everything	O
into	O
a	O
matrix	O
and	O
using	O
`	O
imshow	B-API
`	O
is	O
not	O
desirable	O
because	O
of	O
the	O
pixelation	O
artefacts	O
(	O
Figure	O
1	O
)	O
.	O

See	O
Figure	O
2	O
.	O

Labeling	O
Figure	O
from	O
String	O
List	O

I	O
was	O
able	O
to	O
maximize	O
Figure	O
windows	O
for	O
TkAgg	O
,	O
QT4Agg	O
,	O
and	O
wxAgg	O
using	O
the	O
following	O
lines	O
:	O
#CODE	O

I	O
want	O
to	O
draw	O
a	O
quadratic	O
Figure	O
.	O

So	O
I	O
guess	O
I	O
have	O
to	O
built	O
each	O
button	O
separately	O
,	O
and	O
don't	O
know	O
how	O
to	O
"	O
order	O
"	O
them	O
in	O
the	O
buttons_frame	O
2	O
)	O
I	O
am	O
working	O
within	O
a	O
for	O
loop	O
,	O
so	O
I	O
can't	O
use	O
a	O
"	O
main	O
"	O
specifically	O
for	O
this	O
.	O
and	O
3	O
)	O
the	O
Figure	O
to	O
plot	O
is	O
already	O
created	O
before	O
entering	O
the	O
GUI	O
.	O

But	O
I	O
wonder	O
if	O
there	O
is	O
a	O
way	O
to	O
do	O
the	O
same	O
link	O
(	O
when	O
zooming	O
on	O
figure	O
1	O
,	O
I	O
get	O
the	O
same	O
zoom	O
on	O
figure	O
2	O
on	O
one	O
particular	O
axis	O
)	O
when	O
defining	O
2	O
different	O
Figures	O
(	O
I	O
want	O
those	O
graph	O
to	O
appear	O
far	O
from	O
each	O
other	O
so	O
I	O
guess	O
that	O
I	O
can't	O
put	O
them	O
in	O
the	O
same	O
Figure	O
...	O
)	O

I	O
removed	O
the	O
reference	O
to	O
Figure	O
0	O
and	O
had	O
it	O
generate	O
a	O
new	O
figure	O
each	O
time	O
.	O

Right	O
now	O
,	O
that	O
it	O
the	O
best	O
I	O
can	O
do	O
,	O
see	O
Figure	O
.	O

Sounds	O
like	O
you	O
just	O
want	O
an	O
image	O
in	O
the	O
Figure	O
,	O
right	O
?	O

You	O
should	O
perhaps	O
try	O
some	O
converter	O
that	O
can	O
produce	O
DXF	O
from	O
EPS	O
,	O
PDF	O
,	O
SVG	O
...	O

For	O
example	O
see	O
this	O
converter	O
in	O
java	O
:	O

You	O
already	O
have	O
one	O
such	O
converter	O
in	O
your	O
`	O
np.loadtxt	B-API
`	O
method	O
call	O
.	O

You	O
can	O
achieve	O
this	O
by	O
defining	O
a	O
converter	O
that	O
converts	O
sting	O
representations	O
of	O
dates	O
to	O
`	O
datenums	O
`	O
.	O

Matplotlib	O
Plot	O
Lines	O
Above	O
Each	O
Bar	O

Matplotlib.pyplot	B-API
Bar	O
Plot	O
Grouping	O
Subplots	O

How	O
to	O
properly	O
give	O
Annotations	O
to	O
Pandas	O
Bar	O
Charts	O
?	O

Pandas	O
,	O
Bar	O
Chart	O
Settings	O
Customization	O

Two	O
Bar	O
Plots-	O
Non	O
side	O
by	O
side	O

I	O
have	O
a	O
couple	O
of	O
problems	O
with	O
the	O
Bar	O
Chart	O
that	O
I'm	O
trying	O
to	O
create	O
in	O
python	O
.	O

I	O
have	O
a	O
simple	O
graph	O
with	O
a	O
Navigation	O
Tool	O
Bar	O
.	O

Matplotlib	O
Subplot	O
Animation	O
with	O
Basemap	O

Basic	O
Animation	O
with	O
matplotlib's	O
pyplot	B-API

How	O
Can	O
I	O
Save	O
Animation.Artist	O
animation	O
?	O

Animation	O
with	O
contours	O
matplotlib	O

I	O
found	O
the	O
solution	O
(	O
set	O
2	O
)	O
that	O
uses	O
the	O
mpl	O
Toolkit	O
and	O
AA	O
to	O
allow	O
sharing	O
of	O
the	O
X	O
Axis	O
and	O
present	O
more	O
than	O
2	O
scales	O
.	O
by	O
changing	O
the	O
code	O
to	O
allow	O
for	O
set	O
2	O
is	O
where	O
I	O
noticed	O
problems	O
.	O

The	O
side	O
scales	O
almost	O
look	O
good	O
(	O
exception	O
is	O
repeats	O
on	O
the	O
right	O
)	O
and	O
I	O
CANNOT	O
CHANGE	O
FONT	O
size	O
of	O
the	O
X-Axis	O
labels	O
and	O
Y	O
Axis	O
labels	O
.	O

Reverse	O
Z	O
Axis	O
on	O
matplotlib	O
3D	O
Plot	O

You	O
have	O
an	O
example	O
of	O
the	O
broken	O
axis	O
in	O
the	O
matplotlib	O
examples	O
:	O
Broken	O
Axis	O

Axis	O
scale	O
with	O
bins	O

Converting	O
Integer	O
(	O
Day	O
Count	O
)	O
X	O
Axis	O
to	O
Months	O

3d	O
Polygon	O
Plot	O
in	O
matplotlib	O
baselines	O
slanted	O

I	O
am	O
trying	O
to	O
create	O
my	O
own	O
version	O
of	O
the	O
3D	O
Polygon	O
plot	O
as	O
shown	O
on	O
the	O
Matplotlib	O
web	O
site	O
:	O

I'm	O
trying	O
to	O
make	O
a	O
polar	O
chart	O
with	O
matplotlib	O
and	O
python	O
2.7	O
,	O
but	O
I'm	O
struggling	O
on	O
how	O
to	O
increase	O
the	O
space	O
between	O
the	O
X-Axis	O
and	O
the	O
Tick	O
Labels	O
for	O
that	O
same	O
axis	O
.	O

Tick	O
labels	O
on	O
x-axis	O
aren't	O
symmetric	O
(	O
Matplotlib	O
)	O

Tick	O
label	O
displayed	O
when	O
clicking	O
on	O
graph	O
?	O

Axes	O
missing	O
when	O
plotting	O
in	O
matplotlib	O
using	O
vispy	O
as	O
backend	O

In	O
theory	O
,	O
this	O
code	O
doesn't	O
change	O
any	O
coordinates	O
;	O
it	O
just	O
gets	O
the	O
coordinates	O
of	O
each	O
label	O
,	O
maps	O
it	O
to	O
Axes	O
coordinates	O
using	O
the	O
Text	O
object's	O
internally-stored	O
transform	O
,	O
and	O
then	O
sets	O
the	O
position	O
.	O

@USER	O
I	O
think	O
he	O
wants	O
to	O
get	O
the	O
tick	O
positions	O
in	O
Axes	O
coordinates	O
.	O

Axes	O
fonts	O
with	O
text.usetex	O
'	O
true	O
'	O
does	O
not	O
use	O
set	O
font	O

Also	O
,	O
when	O
specifying	O
where	O
you	O
want	O
the	O
subplot	O
to	O
be	O
placed	O
(	O
the	O
third	O
input	O
to	O
`	O
Figure.add_subplot()	B-API
`)	O
,	O
you	O
do	O
not	O
want	O
to	O
use	O
`	O
y+1	O
`	O
because	O
that	O
would	O
start	O
at	O
`	O
1	O
`	O
and	O
end	O
at	O
`	O
6	O
`	O
which	O
would	O
go	O
out	O
of	O
the	O
available	O
range	O
of	O
0-5	O
.	O

While	O
it	O
might	O
be	O
possible	O
to	O
hack	O
`	O
JointGrid	O
`	O
to	O
get	O
this	O
to	O
work	O
,	O
I	O
suspect	O
it	O
will	O
very	O
likely	O
be	O
easier	O
to	O
just	O
use	O
`	O
kdeplot	O
`	O
on	O
one	O
Axes	O
,	O
using	O
`	O
twinx	B-API
`	O
and	O
`	O
twiny	B-API
to	O
place	O
and	O
scale	O
the	O
marginal	O
plots	O
properly	O
.	O

I	O
want	O
to	O
transform	O
those	O
intensity	O
values	O
to	O
pixel	O
intensity	O
values	O
as	O
in	O
the	O
following	O
image	O
:	O

The	O
longer	O
the	O
time	O
period	O
becomes	O
,	O
the	O
sharper	O
the	O
peaks	O
will	O
become	O
(	O
the	O
Fourier	O
transform	O
of	O
the	O
cosines	O
):	O
#CODE	O

How	O
to	O
transform	O
your	O
data	O
into	O
this	O
format	O
is	O
a	O
simple	O
question	O
,	O
maybe	O
a	O
bit	O
googling	O
and	O
trial	O
and	O
error	O
.	O

Perhaps	O
it	O
is	O
best	O
to	O
transform	O
to	O
linear	O
coordinates	O
and	O
calculate	O
how	O
to	O
produce	O
the	O
curved	O
grid	O
for	O
the	O
declination	O
and	O
the	O
radial	O
lines	O
for	O
the	O
right	O
ascension	O
.	O

Since	O
my	O
data	O
is	O
roughly	O
spherical	O
I	O
triangulate	O
the	O
azimuth	O
and	O
zenith	O
angles	O
from	O
the	O
spherical	O
coordinate	O
transform	O
of	O
my	O
data	O
points	O
.	O

A	O
workaround	O
might	O
be	O
to	O
log10	B-API
transform	O
the	O
data	O
before	O
plotting	O
,	O
but	O
the	O
approaches	O
I	O
have	O
tried	O
,	O
#CODE	O

Without	O
having	O
to	O
transform	O
everything	O
to	O
string	O
or	O
another	O
kind	O
of	O
object	O
?	O

However	O
,	O
the	O
dates	O
mysteriously	O
transform	O
themselves	O
to	O
an	O
ugly	O
and	O
unreadable	O
format	O
when	O
plotting	O
the	O
same	O
data	O
as	O
a	O
bar	O
plot	O
.	O

I	O
want	O
to	O
find	O
out	O
how	O
to	O
transform	O
magnitude	O
value	O
of	O
accelerometer	O
to	O
frequency	O
domain	O
.	O

Then	O
,	O
you	O
need	O
to	O
set	O
the	O
transform	O
for	O
the	O
new	O
lines	O
on	O
`	O
a_all	O
`	O
to	O
move	O
them	O
to	O
the	O
new	O
axis	O
.	O

What	O
about	O
applying	O
a	O
rectangular	O
(	O
ok	O
,	O
in	O
three	O
dimensions	O
,	O
cubic	O
)	O
window	O
to	O
your	O
field	O
before	O
fourier	O
transform	O
?	O

The	O
canvas	O
seems	O
to	O
get	O
locked	O
and	O
after	O
the	O
call	O
of	O
the	O
pick	O
event	O
I	O
can	O
not	O
use	O
the	O
other	O
functionalities	O
as	O
well	O
.	O

When	O
it	O
is	O
triggered	O
it	O
seems	O
that	O
the	O
canvas	O
gets	O
locked	O
and	O
I	O
can	O
not	O
use	O
any	O
other	O
functionality	O
.	O

I	O
am	O
trying	O
to	O
draw	O
an	O
arrow	O
on	O
the	O
scatterplot	O
.	O

Are	O
these	O
parametric	O
orbits	O
,	O
so	O
that	O
you	O
could	O
draw	O
vertical	O
lines	O
for	O
each	O
time	O
(	O
or	O
whatever	O
)	O
_t_	O
?	O

I've	O
follow	O
this	O
subject	O
:	O
How	O
to	O
draw	O
planes	O
from	O
a	O
set	O
of	O
linear	O
equations	O
in	O
Python	O
?	O

I	O
am	O
able	O
to	O
draw	O
with	O
all	O
variables	O
at	O
x-axis	O
if	O
convert	O
it	O
to	O
bar	O
graph	O
.	O

But	O
in	O
my	O
particular	O
case	O
I	O
have	O
to	O
draw	O
Line2D	B-API
instances	O
using	O
Points	O
coordinates	O
on	O
top	O
of	O
the	O
regular	O
plots	O
that	O
are	O
all	O
using	O
Data	O
coordinates	O
.	O

I'm	O
using	O
imshow()	B-API
to	O
draw	O
a	O
2D	O
numpy	O
array	O
,	O
so	O
for	O
example	O
:	O
#CODE	O

I	O
need	O
to	O
draw	O
all	O
function	O
in	O
the	O
same	O
window	O

I	O
want	O
to	O
draw	O
a	O
small	O
red	O
box	O
around	O
one	O
of	O
the	O
ticklabels	O
,	O
as	O
so	O
:	O

I	O
draw	O
4D	O
plot	O
.	O

I'd	O
like	O
to	O
draw	O
/	O
plot	O
an	O
horizontal	O
line	O
on	O
top	O
of	O
the	O
heatmap	O
like	O
in	O
this	O
figure	O

How	O
do	O
I	O
draw	O
edge	O
labels	O
for	O
MultiGraph	O
in	O
NetworkX	O
?	O

How	O
to	O
draw	O
a	O
contour	O
plot	O
using	O
Python	O
?	O

I	O
tried	O
to	O
draw	O
a	O
contour	O
plot	O
using	O
Python	O
.	O

The	O
second	O
option	O
is	O
a	O
touch	O
more	O
verbose	O
,	O
but	O
has	O
the	O
advantage	O
that	O
the	O
y-axis	O
limits	O
on	O
the	O
second	O
plot	O
will	O
autoscale	O
as	O
you'd	O
expect	O
.	O

I	O
included	O
the	O
code	O
to	O
autoscale	O
the	O
viewport	O
,	O
but	O
that's	O
not	O
strictly	O
necessary	O
.	O

possible	O
duplicate	O
of	O
[	O
How	O
to	O
autoscale	O
y	O
axis	O
in	O
matplotlib	O
?	O
]	O
(	O
#URL	O
)	O

if	O
the	O
image	O
is	O
a	O
NxM	O
array	O
of	O
any	O
type	O
,	O
it	O
is	O
interpreted	O
through	O
the	O
colormap	O
(	O
autoscale	O
,	O
if	O
not	O
indicated	O
otherwise	O
)	O
.	O

Now	O
I	O
changed	O
my	O
mind	O
and	O
decide	O
to	O
autoscale	O
the	O
data	O
(	O
and	O
the	O
view	O
)	O
,	O
expecting	O
these	O
limits	O
:	O

I	O
would	O
like	O
to	O
be	O
able	O
to	O
autoscale	O
a	O
matplotlib	O
figure	O
to	O
make	O
arbitrarily	O
placed	O
text	O
annotations	O
visible	O
.	O

I	O
have	O
to	O
refresh	O
the	O
page	O
to	O
get	O
the	O
tooltips	O
back	O
.	O

will	O
refresh	O
your	O
system's	O
reference	O
to	O
the	O
bash_profile	O
and	O
you	O
should	O
be	O
good	O
to	O
go	O
in	O
importing	O
and	O
using	O
matplotlib	O

Id	O
like	O
the	O
user	O
to	O
be	O
able	O
to	O
update	O
an	O
existing	O
and	O
open	O
axis	O
i.e.	O
to	O
refresh	O
the	O
axis	O
.	O

Placing	O
Custom	O
Images	O
in	O
a	O
Plot	O
Window	O
--	O
as	O
custom	O
data	O
markers	O
or	O
to	O
annotate	O
those	O
markers	O

How	O
to	O
annotate	O
/	O
highlight	O
a	O
3d	O
plot	O
in	O
MatPlotLib	O

Well	O
,	O
it	O
takes	O
a	O
loop	O
to	O
annotate	O
all	O
data	O
points	O
,	O
I	O
thought	O
that	O
there	O
should	O
be	O
a	O
function	O
that	O
does	O
just	O
that	O
.	O

You	O
can	O
annotate	O
a	O
specific	O
point	O
in	O
the	O
image	O
using	O
`	O
plt.text	B-API
(	O
x	O
,	O
y	O
,	O
str	O
)`	O
.	O

To	O
state	O
it	O
in	O
a	O
general	O
form	O
,	O
I'm	O
looking	O
for	O
a	O
way	O
to	O
join	O
several	O
points	O
with	O
a	O
gradient	O
color	O
line	O
using	O
matplotlib	O
,	O
and	O
I'm	O
not	O
finding	O
it	O
anywhere	O
.	O

Now	O
instead	O
of	O
straight	O
arrows	O
,	O
I	O
want	O
to	O
join	O
points	O
by	O
curve	O
arrows	O
.	O

If	O
you	O
like	O
,	O
join	O
circos's	O
google	O
group	O
to	O
discuss	O
:	O
#URL	O

Then	O
join	O
me	O
in	O
upvoting	O
that	O
answer	O
and	O
that	O
comment	O
:-)	O

Because	O
there	O
have	O
been	O
closest	O
points	O
but	O
when	O
we	O
join	O
them	O
they	O
lead	O
to	O
an	O
intersections	O
or	O
points	O
which	O
are	O
not	O
so	O
close	O
but	O
should	O
be	O
joined	O
together	O

@USER	O
Because	O
Python	O
automatically	O
join	O
two	O
adjacent	O
strings	O
into	O
one	O
string	O
.	O

This	O
produces	O
a	O
figure	O
like	O
the	O
first	O
figure	O
at	O
How	O
to	O
join	O
overlapping	O
circles	O
?	O

Matplotlib	O
has	O
lots	O
of	O
pre-defined	O
colormaps	O
for	O
you	O
to	O
use	O
.	O

Here	O
are	O
all	O
of	O
the	O
predefined	O
colormaps	O
.	O

For	O
me	O
the	O
simplest	O
way	O
is	O
plotting	O
directly	O
the	O
masks	O
with	O
imshow	B-API
,	O
passing	O
different	O
colormaps	O
.	O

There	O
is	O
a	O
list	O
of	O
colormaps	O
to	O
choose	O
from	O
here	O
.	O

There	O
is	O
also	O
a	O
way	O
to	O
define	O
custom	O
colormaps	O
.	O

The	O
list	O
of	O
available	O
colormaps	O
by	O
default	O
is	O
here	O
.	O

You	O
can	O
add	O
your	O
own	O
colormaps	O
to	O
`	O
_cm.py	O
`	O
in	O
your	O
mpl	O
directory	O
and	O
then	O
change	O
your	O
rc	O
file	O
.	O

How	O
can	O
I	O
tell	O
append_axes	O
that	O
I	O
want	O
to	O
append	O
the	O
y	O
subplot	O
to	O
the	O
right	O
of	O
the	O
"	O
main	O
axes	O
"	O
containing	O
the	O
scatter	O
plot	O
?	O

I	O
got	O
a	O
boxplot	O
graph	O
like	O
this	O
:	O

add	O
boxplot	O
to	O
other	O
graph	O
in	O
python	O

but	O
when	O
I	O
do	O
it	O
on	O
the	O
'	O
link	O
'	O
series	O
I	O
can	O
draw	O
the	O
boxplot	O
correctly	O
.	O

and	O
I	O
am	O
able	O
to	O
draw	O
the	O
boxplot	O
.	O

Or	O
,	O
more	O
generally	O
,	O
modify	O
/	O
transform	O
to	O
you	O
heart's	O
content	O
,	O
and	O
then	O
boxplot	O
.	O

If	O
the	O
ticklabels	O
are	O
already	O
set	O
to	O
a	O
string	O
as	O
in	O
e.g.	O
a	O
boxplot	O
,	O
this	O
is	O
still	O
working	O
.	O

How	O
do	O
I	O
add	O
inset	O
axes	O
and	O
zoom	O
in	O
on	O
the	O
first	O
boxplot	O
of	O
the	O
two	O
?	O

I'm	O
not	O
setting	O
the	O
position	O
of	O
each	O
boxplot	O
,	O
so	O
I	O
don't	O
know	O
where	O
they	O
will	O
appear	O
exactly	O
.	O

I	O
wanted	O
to	O
add	O
a	O
box	O
for	O
each	O
boxplot	O
with	O
a	O
zoomed-in	O
view	O
on	O
a	O
specific	O
y-axis	O
range	O
.	O

UserWarning	O
:	O
2D	O
hist	O
input	O
should	O
be	O
nsamples	O
x	O
nvariables	O
;	O

I'm	O
doing	O
a	O
hist	O
plot	O
and	O
I	O
want	O
some	O
numbers	O
shown	O
in	O
the	O
plot	O
,	O
so	O
I	O
put	O
in	O
a	O
text	O
box	O
using	O
mathtext	B-API
for	O
the	O
text	O
,	O
but	O
I	O
doesn't	O
work	O
and	O
I	O
can't	O
see	O
why	O
.	O

This	O
lead	O
to	O
my	O
colorbar	O
having	O
sqrt	O
ticks	O
and	O
I	O
want	O
to	O
translate	O
them	O
back	O
to	O
the	O
original	O
values	O
.	O

avoiding	O
the	O
local	O
cache	O
when	O
fetching	O
yahoo	O
finance	O
data	O
from	O
matplotlib.finance	B-API
in	O
python	O

I	O
am	O
fetching	O
yahoo	O
finance	O
data	O
in	O
Python	O
through	O
this	O
interface	O
:	O

It	O
would	O
also	O
be	O
worth	O
looking	O
at	O
the	O
scatter	O
plot	O
documentation	O
at	O
#URL	O
#CODE	O

I	O
am	O
using	O
the	O
following	O
code	O
to	O
stitch	O
a	O
.png	O
into	O
a	O
scatter	O
plot	O
from	O
MATPLOTLIB	O
.	O

Now	O
I	O
want	O
to	O
create	O
a	O
scatter	O
plot	O
with	O
the	O
data	O
above	O
..	O

matplotlib	O
:	O
Understanding	O
and	O
changing	O
axis	O
labels	O
for	O
a	O
scatter	O
plot	O
updated	O
incrementally	O

I	O
have	O
a	O
script	O
that	O
generates	O
scatter	O
plots	O
for	O
data	O
being	O
generated	O
by	O
a	O
hardware	O
device	O
.	O

I	O
build	O
a	O
scatter	O
plot	O
using	O
matplotlib	O
and	O
python2.7	O

Build	O
a	O
scatter	O
plot	O
for	O
baz	O
based	O
on	O
the	O
x-axis	O
(	O
foo	O
)	O
and	O
y-axis	O
(	O
bar	O
)	O

It	O
is	O
a	O
normal	O
scatter	O
plot	O
.	O

Value	O
Error	O
with	O
color	O
array	O
when	O
slicing	O
values	O
for	O
scatter	O
plot	O

I	O
want	O
to	O
specify	O
the	O
frequency	O
of	O
markers	O
that	O
are	O
printed	O
in	O
my	O
scatter	O
plot	O
.	O

Regression	O
line	O
and	O
fitted	O
curve	O
for	O
scatter	O
plots	O
in	O
matplotlib	O

If	O
you	O
are	O
trying	O
to	O
create	O
an	O
animation	O
,	O
look	O
in	O
to	O
the	O
`	O
animation	B-API
`	O
module	O
of	O
matplotlib	O
,	O
it	O
takes	O
care	O
of	O
a	O
lot	O
of	O
the	O
details	O
for	O
you	O
.	O

Apparently	O
,	O
the	O
"	O
Animation	B-API
"	O
class	O
of	O
Matplotlib	O
runs	O
the	O
animation	O
in	O
a	O
separate	O
thread	O
.	O

It	O
simply	O
don't	O
use	O
the	O
Animation	B-API
class	O
and	O
builds	O
its	O
own	O
animation	O
from	O
crash	O
.	O

I	O
think	O
`	O
axis	O
([	O
xo	O
,	O
x1	O
,	O
y0	O
,	O
y1	O
])`	O
is	O
in	O
terms	O
of	O
proportion	O
of	O
the	O
Figure	B-API
,	O
not	O
the	O
data	O
transform	O
.	O

I	O
used	O
the	O
scatter	B-API
function	O
and	O
plotted	O
the	O
points	O
,	O
but	O
the	O
surface	O
function	O
is	O
not	O
working	O
(	O
the	O
window	O
is	O
empty	O
)	O
.	O

The	O
solution	O
I	O
found	O
for	O
this	O
involves	O
using	O
Normalize	B-API
to	O
make	O
a	O
normalised	O
colour	O
list	O
based	O
on	O
the	O
relevant	O
data	O
,	O
mapping	O
it	O
to	O
a	O
ScalarMappable	B-API
,	O
and	O
using	O
that	O
to	O
set	O
the	O
face	O
colour	O
and	O
c	O
limits	O
on	O
each	O
frame	O
of	O
the	O
animation	O
.	O

But	O
because	O
all	O
of	O
the	O
functionality	O
relies	O
on	O
the	O
hook	O
,	O
when	O
the	O
canvas	O
is	O
finally	O
shown	O
I	O
presume	O
Python's	O
garbage	O
collection	O
has	O
removed	O
the	O
Animation	B-API
instance	O
---	O
since	O
it	O
was	O
never	O
assigned	O
to	O
a	O
variable	O
---	O
and	O
therefore	O
the	O
animation	O
can	O
never	O
be	O
started	O
.	O

Notice	O
`	O
xytext	O
=	O
(	O
0	O
,	O
0	O
)`	O
means	O
no	O
offset	O
,	O
and	O
omitting	O
`	O
arrowprops	O
`	O
causes	O
`	O
plt.annotate	B-API
`	O
to	O
not	O
draw	O
an	O
arrow	O
.	O

OLS	O
solution	O
using	O
pinv	B-API
/	O
svd	B-API
#CODE	O

The	O
bad	O
days	O
are	O
eliminated	O
,	O
and	O
the	O
good	O
ones	O
are	O
kept	O
.	O

@USER	O
Well	O
in	O
this	O
case	O
,	O
`	O
searchsorted	B-API
`	O
is	O
basically	O
looking	O
for	O
places	O
or	O
indices	O
where	O
elements	O
from	O
`	O
message	O
`	O
exists	O
in	O
the	O
keys	O
of	O
`	O
codes	O
`	O
.	O

@USER	O
--	O
I	O
had	O
a	O
hard	O
time	O
remembering	O
how	O
`	O
translate	B-API
`	O
and	O
`	O
maketrans	B-API
`	O
work	O
for	O
quite	O
a	O
while	O
too	O
,	O
but	O
I've	O
gotten	O
used	O
to	O
it	O
.	O

Edit	O
:	O
if	O
you're	O
using	O
a	O
version	O
of	O
numpy	O
>	O
=	O
1.8.0	O
,	O
then	O
`	O
np.linalg.eigvals	B-API
`	O
operates	O
over	O
the	O
last	O
two	O
dimensions	O
of	O
whatever	O
array	O
you	O
hand	O
it	O
,	O
so	O
if	O
you	O
reshape	O
your	O
input	O
to	O
an	O
`	O
(	O
n_subarrays	O
,	O
nrows	O
,	O
ncols	O
)`	O
array	O
you'll	O
only	O
have	O
to	O
call	O
`	O
eigvals	B-API
`	O
once	O
:	O
#CODE	O

`	O
reshape	B-API
`	O
returns	O
a	O
view	O
of	O
the	O
original	O
array	O
,	O
not	O
a	O
copy	O
,	O
so	O
the	O
conversion	O
to	O
3D	O
only	O
requires	O
altering	O
the	O
`	O
shape	O
`	O
and	O
`	O
strides	O
`	O
attributes	O
of	O
the	O
array	O
,	O
without	O
having	O
to	O
copy	O
any	O
of	O
the	O
actual	O
data	O
.	O

sum	O
this	O
new	O
array	O
along	O
particular	O
axes	O
;	O
and	O
then	O
maybe	O

Calling	O
`	O
reshape	B-API
`	O
returns	O
a	O
view	O
,	O
so	O
it	O
doesn't	O
incur	O
any	O
big	O
copying	O
costs	O
or	O
anything	O
like	O
that	O
.	O

so	O
at	O
some	O
point	O
in	O
the	O
execution	O
it	O
will	O
max	O
my	O
memory	O
.	O

Note	O
that	O
extension	O
to	O
even	O
higher	O
combinatorics	O
should	O
be	O
trivial	O
,	O
along	O
the	O
lines	O
presented	O
;	O
but	O
keep	O
an	O
eye	O
on	O
the	O
n	O
used	O
in	O
that	O
case	O
.	O

These	O
functions	O
return	O
a	O
list	O
,	O
which	O
I	O
convert	O
to	O
a	O
numpy	O
array	O
and	O
then	O
sum	O
over	O
.	O

Can	O
it	O
be	O
because	O
of	O
the	O
many	O
zeros	O
in	O
the	O
initial	O
table	O
?	O

I	O
also	O
understand	O
that	O
sum	B-API
(	O
A	O
,	O
axis=1	O
)	O
will	O
sum	O
each	O
row	O
.	O

But	O
what	O
I	O
really	O
want	O
to	O
do	O
,	O
is	O
to	O
bin	O
`	O
array	O
[:	O
,	O
1	O
]`	O
by	O
day	O
(	O
as	O
derived	O
by	O
the	O
unix	O
timestamps	O
in	O
array	O
[:	O
,	O
0	O
])	O
,	O
and	O
plot	O
these	O
as	O
a	O
stacked	O
histogram	O
,	O
with	O
each	O
(	O
colored	O
)	O
stack	O
representing	O
a	O
day	O
.	O

It's	O
interesting	O
to	O
see	O
that	O
when	O
I	O
go	O
back	O
to	O
`	O
nloop=1000	O
`	O
,	O
`	O
nreps=3	O
`	O
I	O
actually	O
see	O
a	O
slightly	O
*	O
greater	O
*	O
rate	O
of	O
cache	O
misses	O
for	O
the	O
row	O
sum	O
(	O
17%	O
vs	O
13%	O
)	O
,	O
even	O
though	O
it's	O
faster	O
than	O
the	O
column	O
sum	O
.	O

You	O
can	O
concatenate	O
arrays	O
in	O
`	O
numpy	O
`	O
.	O

If	O
you	O
are	O
100%	O
sure	O
that	O
l2	O
would	O
only	O
be	O
one	O
column	O
then	O
you	O
can	O
reshape	O
that	O
array	O
to	O
make	O
it	O
one	O
dimensional	O
before	O
doing	O
the	O
subtraction	O
.	O

You	O
won't	O
be	O
able	O
to	O
create	O
a	O
2D	O
array	O
that	O
way	O
,	O
and	O
@USER	O
method	O
of	O
returning	O
a	O
1D	O
array	O
that	O
you	O
reshape	O
afterwards	O
is	O
a	O
sure	O
go	O
.	O

I	O
have	O
a	O
square	O
matrix	O
A	O
(	O
could	O
be	O
any	O
size	O
)	O
and	O
I	O
want	O
to	O
take	O
the	O
upper	O
triangular	O
part	O
and	O
place	O
those	O
values	O
in	O
an	O
array	O
without	O
the	O
values	O
below	O
the	O
center	O
diagonal	O
(	O
k=0	O
)	O
.	O

You	O
can	O
mimic	O
this	O
behavior	O
with	O
a	O
simple	O
function	O
to	O
flatten	O
a	O
list	O
:	O
#CODE	O

So	O
`	O
popt	O
`	O
,	O
according	O
to	O
the	O
documentation	O
,	O
returns	O
*	O
"	O
Optimal	O
values	O
for	O
the	O
parameters	O
so	O
that	O
the	O
sum	O
of	O
the	O
squared	O
error	O
of	O
f	O
(	O
xdata	O
,	O
popt	O
)	O
-	O
ydata	O
is	O
minimized	O
"	O
.	O

And	O
I'd	O
like	O
indices	O
`	O
i	O
`	O
such	O
that	O
,	O
#CODE	O

But	O
,	O
`	O
resize	B-API
`	O
looks	O
like	O
it	O
just	O
might	O
be	O
the	O
thing	O
I'm	O
looking	O
for	O
...	O

`	O
rfft	B-API
`	O
,	O
apart	O
from	O
repeated	O
terms	O
excluded	O
,	O
and	O
an	O
almost	O
2x	O
speed-up	O
,	O
returns	O
the	O
exact	O
same	O
you	O
would	O
get	O
from	O
`	O
fft	B-API
`	O
.	O

Plus	O
,	O
if	O
I	O
have	O
4	O
dimensions	O
,	O
I	O
thought	O
I	O
should	O
have	O
4	O
eigenvalues	O
and	O
not	O
150	O
like	O
the	O
eig	B-API
gives	O
me	O
.	O

If	O
I	O
run	O
your	O
code	O
to	O
generate	O
`	O
d	O
`	O
and	O
`	O
dx	O
`	O
with	O
`	O
eig	B-API
`	O
I	O
get	O
the	O
following	O
:	O
#CODE	O

In	O
other	O
words-	O
it	O
is	O
not	O
just	O
taking	O
a	O
min	O
or	O
max	O
.	O

D	O
[	O
I+1	O
,	O
J+1	O
]	O
=	O
map	B-API
(	O
norm	O
,	O
x	O
[	O
I	O
]	O
-y	O
[	O
J	O
])	O
+	O
np.minimum	B-API
(	O
np.minimum	B-API
(	O
D	O
[	O
I	O
,	O
J	O
]	O
,	O
D	O
[	O
I	O
,	O
J+1	O
])	O
,	O
D	O
[	O
I+1	O
,	O
J	O
])	O
?	O

`	O
dot	B-API
`	O
just	O
has	O
tighter	O
code	O
for	O
a	O
specific	O
combination	O
of	O
dimensions	O
.	O

numpy	O
sum	B-API
does	O
not	O
agree	O

Since	O
you	O
are	O
only	O
adding	O
many	O
`	O
1	O
`	O
s	O
you	O
can	O
convert	O
`	O
diff	O
`	O
to	O
`	O
bool	O
`	O
:	O
#CODE	O

It	O
isn't	O
mathematically	O
possible	O
to	O
represent	O
0	O
on	O
a	O
log	O
scale	O
,	O
so	O
the	O
first	O
value	O
will	O
have	O
to	O
either	O
be	O
masked	O
or	O
clipped	O
to	O
a	O
very	O
small	O
positive	O
number	O
.	O

possible	O
duplicate	O
of	O
[	O
Efficiently	O
count	O
the	O
number	O
of	O
occurrences	O
of	O
unique	O
subarrays	O
in	O
NumPy	O
?	O
]	O
(	O
#URL	O
)	O

Or	O
you	O
could	O
mask	O
the	O
x	O
value	O
as	O
well	O
,	O
so	O
the	O
indices	O
were	O
consistent	O
between	O
x	O
and	O
y	O
#CODE	O

Here	O
the	O
`	O
outer	B-API
`	O
method	O
of	O
the	O
`	O
multiply	B-API
`	O
ufunc	O
is	O
used	O
to	O
create	O
the	O
new	O
20x20	O
array	O
.	O

I	O
have	O
a	O
3D	O
numpy	O
array	O
consisting	O
of	O
1's	O
and	O
zeros	O
defining	O
open	O
versus	O
filled	O
space	O
in	O
a	O
porous	O
solid	O
(	O
it's	O
currently	O
a	O
numpy	O
Int64	O
array	O
)	O
.	O

You	O
are	O
attempting	O
to	O
broadcast	O
a	O
4-D	O
array	O
together	O
with	O
a	O
3-D	O
array	O
.	O

Scipy	O
NDimage	O
correlate	O
:	O
unbearably	O
slow	O

I	O
know	O
that	O
I	O
can	O
reshape	O
the	O
array	O
to	O
a	O
100	O
x	O
2	O
array	O
of	O
grid	O
points	O
:	O
#CODE	O

You	O
probably	O
could	O
get	O
`	O
append	B-API
`	O
to	O
work	O
,	O
but	O
it	O
just	O
does	O
a	O
step	O
by	O
step	O
concatenate	O
,	O
which	O
is	O
slower	O
.	O

This	O
produces	O
a	O
random	O
permutation	O
of	O
each	O
column's	O
indices	O
.	O

As	O
it	O
happens	O
,	O
the	O
histogram	O
is	O
enough	O
for	O
the	O
former	O
.	O

I	O
see	O
how	O
the	O
symmetry	O
of	O
the	O
trace	O
lets	O
you	O
replace	O
the	O
final	O
`	O
dot	B-API
`	O
.	O

In	O
that	O
question	O
,	O
I	O
sought	O
to	O
sum	O
values	O
in	O
a	O
numpy	O
structured	O
array	O
based	O
on	O
multiple	O
criteria	O
,	O
including	O
matches	O
in	O
a	O
list	O
.	O

to	O
delete	O
the	O
lines	O
that	O
had	O
zeros	O
in	O
them	O
!	O

Fill	O
scipy	O
/	O
numpy	O
matrix	O
based	O
on	O
indices	O
and	O
values	O

It	O
looks	O
like	O
a	O
vector	O
product	O
followed	O
by	O
a	O
sum	O
along	O
the	O
resulting	O
array	O
.	O

The	O
trick	O
is	O
that	O
this	O
convolve	B-API
function	O
can	O
be	O
used	O
in-place	O
so	O
the	O
double	O
for	O
loop	O
:	O
#CODE	O

But	O
this	O
reshape	B-API
should	O
produce	O
a	O
`	O
(	O
n	O
,	O
1	O
,	O
1	O
)`	O
array	O
,	O
not	O
your	O
`	O
(	O
1	O
,	O
1	O
,	O
1	O
,...	O
)`	O
array	O
.	O

For	O
an	O
extreme	O
example	O
,	O
consider	O
a	O
sequence	O
that	O
consists	O
of	O
9	O
zeros	O
followed	O
by	O
the	O
result	O
of	O
a	O
coin	O
toss	O
,	O
9	O
zeros	O
and	O
another	O
coin	O
toss	O
,	O
etc	O
.	O

If	O
so	O
then	O
`	O
np.array	B-API
(	O
a	O
)`	O
is	O
a	O
2d	O
array	O
,	O
and	O
you	O
can	O
sum	O
over	O
`	O
axis=1	O
`	O
.	O

I	O
am	O
trying	O
to	O
create	O
a	O
lat	O
/	O
lon	O
grid	O
that	O
contains	O
an	O
array	O
of	O
found	O
indices	O
where	O
two	O
conditions	O
are	O
met	O
for	O
a	O
lat	O
/	O
lon	O
combination	O
.	O

This	O
NAMBE	O
is	O
the	O
absolute	O
difference	O
between	O
a	O
base	O
vector	O
and	O
another	O
vector	O
,	O
divided	O
by	O
the	O
base	O
vector	O
and	O
multiplied	O
by	O
a	O
hundred	O
,	O
in	O
pseudo-code	O
notation	O
:	O
#CODE	O

this	O
my	O
code	O
to	O
and	O
i	O
want	O
to	O
use	O
histogram	O
data	O
to	O
plot	O
scatter	O
where	O
y	O
axis	O
is	O
counts	O
center	O
from	O
the	O
histogram	O
,	O
is	O
there	O
any	O
direct	O
command	O
or	O
way	O
to	O
do	O
this	O
?	O

Please	O
compile	O
with	O
`	O
cython	O
-a	O
`	O
,	O
then	O
show	O
us	O
the	O
C	O
code	O
that	O
the	O
`	O
a	O
[	O
0	O
]	O
+=	O
sum	O
`	O
line	O
turns	O
into	O
.	O

The	O
revised	O
question	O
is	O
still	O
a	O
duplicate	O
,	O
see	O
[	O
this	O
question	O
]	O
(	O
#URL	O
)	O
,	O
and	O
[	O
this	O
question	O
]	O
(	O
#URL	O
)	O
for	O
finding	O
the	O
indices	O
.	O

ValueError	O
:	O
operands	O
could	O
not	O
be	O
broadcast	O
together	O
with	O
different	O
shapes	O
in	O
numpy	O
?	O

There	O
are	O
thousands	O
of	O
numbers	O
below	O
the	O
ones	O
shown	O
here	O
.	O

Assuming	O
you	O
want	O
to	O
align	O
all	O
the	O
arrays	O
to	O
the	O
left	O
,	O
and	O
pad	O
to	O
the	O
right	O
with	O
zeros	O
,	O
then	O
you	O
could	O
first	O
find	O
the	O
maximum	O
length	O
with	O
#CODE	O

How	O
to	O
do	O
the	O
same	O
If	O
I	O
want	O
to	O
apply	O
norm	O
column-wise	O
to	O
a	O
matrix	O
?	O

The	O
easiest	O
approach	O
is	O
to	O
reshape	O
to	O
data	O
to	O
a	O
long	O
format	O
using	O
`	O
.stack	B-API
`	O
,	O
which	O
can	O
be	O
be	O
passed	O
straight	O
into	O
rolling	O
mean	O
.	O

It's	O
pretty	O
low-level	O
,	O
and	O
mostly	O
focused	O
on	O
how	O
to	O
address	O
the	O
more	O
difficult	O
problem	O
of	O
how	O
to	O
pass	O
C++	O
data	O
to	O
and	O
from	O
NumPy	O
without	O
copying	O
,	O
but	O
here's	O
how	O
you'd	O
do	O
a	O
copied	O
std	O
::	O
vector	O
return	O
with	O
that	O
:	O
#CODE	O

`	O
std	O
=	O
RMS	O
(	O
data	O
-	O
mean	O
)`	O
.	O

This	O
generalized	O
diagonal	O
would	O
be	O
defined	O
as	O
those	O
elements	O
of	O
the	O
array	O
whose	O
0th	O
and	O
2nd	O
index	O
coincide	O
,	O
and	O
would	O
have	O
shape	O
(	O
3	O
,	O
3	O
,	O
7	O
)	O
.	O

I	O
have	O
a	O
given	O
array	O
`	O
[	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
2	O
,	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
1	O
,	O
2	O
,	O
1	O
,	O
0	O
,	O
2	O
,	O
3	O
`]	O
(	O
arbitrary	O
elements	O
from	O
0-5	O
)	O
and	O
I	O
want	O
to	O
have	O
a	O
counter	O
for	O
the	O
occurence	O
of	O
zeros	O
in	O
a	O
row	O
.	O

To	O
see	O
the	O
benefits	O
of	O
this	O
,	O
you	O
need	O
to	O
use	O
`	O
z	O
,	O
p	O
,	O
k	O
=	O
butter	O
(	O
output=	O
'	O
zpk	O
')`	O
and	O
then	O
work	O
with	O
poles	O
and	O
zeros	O
instead	O
of	O
numerator	O
and	O
denominator	O
.	O

In	O
that	O
case	O
you	O
would	O
"	O
extrapolate	O
"	O
zeros	O
to	O
the	O
left	O
and	O
the	O
right	O
.	O

can	O
numpy	O
interpret	O
column	O
of	O
indices	O
like	O
matlab	O
does	O

To	O
get	O
the	O
diagonal	O
elements	O
you	O
can	O
get	O
their	O
indices	O
with	O
`	O
np.triu_indices	B-API
`	O
(	O
or	O
,	O
for	O
the	O
lower	O
triangle	O
,	O
`	O
np.tril_indices	B-API
`)	O
and	O
then	O
index	O
by	O
them	O
.	O

The	O
question	O
states	O
that	O
the	O
input	O
array	O
is	O
of	O
shape	O
`	O
(	O
128	O
,	O
36	O
,	O
8)	O
`	O
and	O
we	O
are	O
interested	O
in	O
finding	O
unique	O
subarrays	O
of	O
length	O
`	O
8	O
`	O
in	O
the	O
last	O
dimension	O
.	O

What	O
does	O
work	O
,	O
however	O
is	O
nesting	O
append	B-API
and	O
concatenate	B-API
#CODE	O

(	O
`	O
b	O
`	O
will	O
be	O
broadcast	O
along	O
(	O
?	O
)	O
the	O
first	O
axis	O
)	O
#CODE	O

As	O
he	O
points	O
out	O
,	O
the	O
`	O
[	O
0	O
]	O
[	O
1	O
]`	O
element	O
is	O
what	O
you'd	O
want	O
for	O
`	O
cov	B-API
(	O
a	O
,	O
b	O
)`	O
.	O

returns	O
`	O
1	O
`	O
,	O
making	O
the	O
sum	O
not	O
commutative	O
!	O

But	O
as	O
I	O
have	O
a	O
log	O
of	O
values	O
(	O
10000+	O
)	O
,	O
this	O
will	O
be	O
quite	O
slow	O
.	O

@USER	O
-	O
good	O
point	O
.	O
anyway	O
,	O
`	O
diff	B-API
`	O
works	O
on	O
python	O
lists	O
too	O
.	O

It	O
will	O
also	O
work	O
if	O
they	O
are	O
both	O
arrays	O
that	O
can	O
be	O
broadcast	O
.	O

It's	O
column	O
stack	O
that	O
requires	O
equal	O
length	O
strings	O
.	O

In	O
the	O
end	O
it	O
is	O
usually	O
not	O
too	O
complicated	O
,	O
especially	O
if	O
you	O
use	O
[	O
`	O
mgrid	B-API
`]	O
(	O
#URL	O
)	O
or	O
similar	O
to	O
get	O
the	O
indices	O
.	O

The	O
absolute	O
error	O
will	O
be	O
at	O
most	O
1	O
/	O
2	O
ULP	O
,	O
2	O
-150	O
.	O

AttributeError	O
:	O
'	O
Add	O
'	O
object	O
has	O
no	O
attribute	O
'	O
log	O
'	O
Python	O

Or	O
,	O
you	O
could	O
initialize	O
an	O
array	O
of	O
all	O
zeros	O
if	O
you	O
know	O
the	O
size	O
of	O
the	O
array	O
ahead	O
of	O
time	O
.	O

Are	O
you	O
checking	O
shape	O
or	O
number	O
of	O
nonzero	O
values	O
?	O

Something	O
like	O
`	O
eigvals	O
,	O
eigvecs	O
=	O
la.eigh	O
(	O
mat	B-API
)`	O
`	O
principal	O
=	O
eigvecs	O
[:	O
,	O
eigvals.argmax()	O
]`	O
`	O
if	O
(	O
principal	O
>	O
=	O
0	O
)	O
.all()	O
or	O
(	O
pricipal	O
<=	O
0	O
)	O
.all()	O
:	O
print	O
'	O
all	O
the	O
same	O
'`	O
?	O

I	O
also	O
want	O
bins	O
to	O
have	O
a	O
width	O
of	O
.5	O
so	O
that	O
I	O
can	O
have	O
a	O
bin	O
from	O
10.5	O
to	O
11	O
or	O
24	O
to	O
24.5	O
etc	O
...	O
because	O
otherwise	O
,	O
python	O
outputs	O
the	O
histogram	O
with	O
the	O
bins	O
random	O
and	O
undetermined	O
.	O

Maximum	O
is	O
always	O
bigger	O
than	O
the	O
minimum	O
(	O
more	O
to	O
the	O
right	O
on	O
a	O
1d	O
axis	O
,	O
not	O
by	O
absolute	O
value	O
)	O
.	O

should	O
give	O
the	O
sum	O
of	O
the	O
columns	O
.	O

Suppose	O
,	O
You	O
wanna	O
check	O
how	O
many	O
times	O
you	O
will	O
get	O
six	O
if	O
you	O
roll	O
dice	O
10	O
times	O
.	O

With	O
this	O
option	O
,	O
the	O
result	O
will	O
broadcast	O
correctly	O

Do	O
you	O
mean	O
`	O
indices	O
=	O
np.where	B-API
(	O
a	O
==	O
a.max()	O
)`	O
in	O
line	O
3	O
?	O

The	O
problem	O
I	O
have	O
much	O
later	O
on	O
in	O
the	O
code	O
is	O
that	O
if	O
one	O
of	O
these	O
parameters	O
isn't	O
in	O
the	O
ASCII	O
file	O
it	O
throws	O
errors	O
up	O
so	O
I	O
have	O
to	O
keep	O
adding	O
in	O
ones	O
I	O
don't	O
need	O
.	O

`	O
append	B-API
`	O
adds	O
them	O
to	O
the	O
end	O
of	O
the	O
list	O
,	O
which	O
is	O
exactly	O
what	O
you	O
want	O
.	O

I	O
have	O
two	O
3dim	O
numpy	O
matrices	O
and	O
I	O
want	O
to	O
do	O
a	O
dot	O
product	O
according	O
to	O
one	O
axis	O
without	O
using	O
a	O
loop	O
in	O
theano	O
.	O

you	O
have	O
at	O
most	O
4	O
in	O
that	O
dimension	O
(	O
see	O
your	O
reshape	O
line	O
)	O
,	O
so	O
the	O
index	O
it	O
will	O
count	O
are	O
0	O
and	O
2	O
(	O
1	O
and	O
3	O
are	O
skipped	O
,	O
and	O
3	O
is	O
the	O
last	O
element	O
)	O
.	O

Once	O
we	O
have	O
the	O
indices	O
to	O
sort	O
`	O
data	O
`	O
,	O
to	O
get	O
a	O
sorted	O
copy	O
of	O
the	O
array	O
it	O
is	O
faster	O
to	O
use	O
the	O
indices	O
than	O
to	O
re-sort	O
the	O
array	O
:	O
#CODE	O

I	O
hope	O
this	O
will	O
help	O
you	O
perform	O
your	O
transpose	O
and	O
column-wise	O
operations	O

It	O
is	O
better	O
to	O
specify	O
that	O
I'm	O
looking	O
for	O
something	O
that	O
performs	O
the	O
log-sum-exp	O
trick	O
,	O
doing	O
a	O
simply	O
succession	O
of	O
exp	O
elem-wise	O
,	O
summing	O
the	O
rows	O
and	O
doing	O
a	O
log	O
elem-wise	O
is	O
trivial	O
in	O
`	O
scipy.sparse	O
`	O
.	O

Scipy	O
uses	O
`	O
int32	O
`	O
to	O
store	O
`	O
indptr	O
`	O
and	O
`	O
indices	O
`	O
for	O
the	O
sparse	O
formats	O
.	O

But	O
not	O
able	O
to	O
plot	O
it	O
as	O
a	O
graph	O
(	O
something	O
like	O
a	O
histogram	O
)	O
...	O
that	O
is	O
the	O
problem	O
.	O

It	O
gave	O
error	O
testing	O
doesnot	O
have	O
attribute	O
append	O
as	O
its	O
of	O
None	O
Type	O
.	O

In	O
both	O
cases	O
,	O
you	O
can	O
access	O
individual	O
elements	O
by	O
indices	O
,	O
like	O
`	O
R	O
[	O
0	O
]`	O
(	O
which	O
would	O
give	O
you	O
a	O
specific	O
object	O
,	O
a	O
`	O
np.void	O
`	O
,	O
that	O
still	O
gives	O
you	O
the	O
possibility	O
to	O
access	O
the	O
fields	O
separately	O
)	O
,	O
or	O
by	O
slices	O
`	O
R	O
[	O
1	O
:	O
-1	O
]`	O
...	O

I	O
think	O
you	O
can	O
have	O
a	O
sum	O
over	O
a	O
sliding	O
window	O
(	O
or	O
a	O
rolling	O
window	O
)	O
or	O
a	O
mean	O
over	O
a	O
sliding	O
window	O
.	O

I	O
got	O
your	O
point	O
and	O
I	O
find	O
it	O
more	O
logical	O
,	O
but	O
when	O
trying	O
the	O
code	O
you've	O
suggested	O
to	O
get	O
rid	O
of	O
the	O
second	O
error	O
I	O
got	O
another	O
error	O
:	O
`	O
AttributeError	O
:	O
flatten	B-API
`	O

`	O
dot	B-API
`	O
does	O
many	O
things	O
under	O
the	O
hood	O
,	O
it	O
is	O
apparent	O
that	O
`	O
np.dot	B-API
(	O
A	O
,	O
x	O
)`	O
is	O
not	O
calling	O
BLAS	O
and	O
is	O
somehow	O
defaulting	O
over	O
to	O
numpy's	O
internal	O
GEMM	O
routine	O
.	O

Below	O
is	O
some	O
code	O
which	O
uses	O
a	O
callback	O
to	O
print	O
out	O
the	O
current	O
azimuthal	O
and	O
elevation	O
angles	O
,	O
as	O
well	O
as	O
append	O
them	O
to	O
a	O
list	O
for	O
further	O
use	O
later	O
.	O

It's	O
super	O
alex	O
,	O
here	O
to	O
answer	O
NumPy	O
questions	O
in	O
the	O
blink	O
of	O
an	O
eye	O
:)	O

Your	O
solution	O
of	O
searching	O
the	O
eigenvalues	O
for	O
the	O
ones	O
you	O
want	O
seems	O
plausible	O
enough	O
.	O

If	O
d	O
is	O
larger	O
than	O
8	O
or	O
9	O
,	O
then	O
bases	O
will	O
be	O
sufficiently	O
long	O
that	O
you	O
probably	O
would	O
be	O
better	O
off	O
going	O
with	O
the	O
other	O
version	O
using	O
the	O
dot	O
product	O
.	O

I'm	O
not	O
sure	O
which	O
indices	O
i	O
need	O
to	O
change	O
to	O
achieve	O
the	O
minimum	O
and	O
not	O
the	O
maximum	O
values	O
.	O

The	O
dimension	O
of	O
`	O
result	O
`	O
has	O
been	O
set	O
earlier	O
to	O
the	O
correct	O
dimension	O
,	O
so	O
can	O
check	O
it	O
,	O
but	O
it	O
would	O
be	O
nice	O
to	O
only	O
use	O
the	O
length	O
of	O
`	O
indices	O
`	O
to	O
determine	O
it	O
.	O

Alternatively	O
,	O
what	O
about	O
applying	O
the	O
same	O
function	O
without	O
indices	O
along	O
the	O
depth	O
axes	O
?	O

Here's	O
an	O
O	O
(	O
n	O
log	O
n	O
)	O
algorithm	O
for	O
your	O
problem	O
.	O

You	O
need	O
to	O
add	O
axes	O
to	O
`	O
coeffs	O
`	O
so	O
it	O
will	O
broadcast	O
in	O
the	O
dimension	O
(	O
s	O
)	O
you	O
want	O
.	O

If	O
you	O
want	O
to	O
search	O
for	O
a	O
certain	O
rank	O
on	O
B	O
randomly	O
,	O
you	O
need	O
to	O
start	O
off	O
with	O
a	O
valid	O
B	O
with	O
max	O
rank	O
,	O
and	O
rotate	O
a	O
random	O
column	O
j	O
of	O
a	O
random	O
B	O
i	O
by	O
a	O
random	O
amount	O
.	O

I	O
want	O
to	O
save	O
some	O
histogram	O
data	O
in	O
a	O
csv	O
file	O
.	O

I	O
want	O
to	O
read	O
a	O
mat	O
file	O
back	O
in	O
python	O
but	O
I	O
have	O
trouble	O
going	O
back	O
to	O
a	O
graph	O
,	O
because	O
the	O
mat	O
file	O
gives	O
a	O
numpy.ndarray	B-API
type	O
file	O
and	O
I	O
need	O
a	O
sparse	O
matrix	O
to	O
reconstruct	O
my	O
graph	O
.	O

numpy	O
makes	O
it	O
easy	O
to	O
translate	O
python	O
objects	O
into	O
numpy	O
ndarrays	O
,	O
and	O
will	O
even	O
pick	O
an	O
appropriate	O
resulting	O
data	O
type	O
if	O
one	O
is	O
not	O
specified	O
:	O
#CODE	O

This	O
`	O
T	O
`	O
and	O
`	O
X	O
`	O
broadcast	O
together	O
just	O
fine	O
,	O
for	O
example	O
`	O
T*X	O
`	O
works	O
.	O

I	O
have	O
a	O
numpy	O
matrix	O
A	O
and	O
I	O
need	O
a	O
function	O
that	O
will	O
count	O
(	O
A	O
[	O
i	O
,	O
j	O
]	O
/	O
sum	O
of	O
all	O
elements	O
in	O
i-th	O
column	O
)	O
-	O
A	O
[	O
i	O
,	O
j	O
]	O
/	O
sum	O
of	O
all	O
elements	O
in	O
j-th	O
row	O

This	O
also	O
works	O
if	O
,	O
instead	O
of	O
a	O
single	O
index	O
,	O
you	O
provide	O
an	O
array	O
of	O
indices	O
:	O
#CODE	O

How	O
to	O
solve	O
nonlinear	O
equation	O
without	O
sympy	O
(	O
max	O
and	O
min	O
)	O
?	O

Bivariate	O
Legendre	O
Polynomial	O
Fitting	O
to	O
find	O
orthogonal	O
coefficents	O

I	O
have	O
a	O
big	O
n-square	O
diagonal	O
matrix	O
,	O
in	O
the	O
scipy's	O
sparse	O
DIA	O
format	O

To	O
find	O
the	O
most	O
frequent	O
value	O
of	O
a	O
flat	O
array	O
,	O
use	O
`	O
unique	B-API
`	O
,	O
`	O
bincount	B-API
`	O
and	O
`	O
argmax	B-API
`	O
:	O
#CODE	O

The	O
funny	O
thing	O
is	O
in	O
the	O
above	O
function	O
If	O
i	O
pass	O
an	O
extra	O
argument	O
and	O
just	O
divide	O
sum	O
by	O
it	O
,	O
then	O
the	O
times	O
are	O
the	O
same	O
again	O
.	O

are	O
the	O
same	O
as	O
the	O
ones	O
posted	O
in	O
the	O
examples	O
of	O
this	O
web	O
page	O
.	O

How	O
to	O
remove	O
rings	O
from	O
convolve	O
healpix	O
map	O
?	O

With	O
the	O
information	O
of	O
the	O
full	O
stack	O
trace	O
report	O
the	O
bug	O
to	O
the	O
ubuntu	O
team	O
.	O

fastest	O
way	O
to	O
get	O
lookup	O
table	O
indices	O
with	O
numpy	O

Well	O
,	O
a	O
few	O
more	O
,	O
anyway	O
:	O
`	O
cos	B-API
`	O
,	O
`	O
pi	B-API
`	O
,	O
`	O
diag	B-API
`	O

I	O
implemented	O
a	O
LOWESS	O
smoother	O
(	O
which	O
is	O
the	O
curve	O
you	O
see	O
)	O
with	O
a	O
tight	O
fit	O
to	O
eliminate	O
noise	O
,	O
since	O
the	O
real	O
waveforms	O
have	O
a	O
non-trivial	O
noise	O
component	O
,	O
and	O
then	O
tried	O
doing	O
a	O
rolling	O
max	O
with	O
a	O
window	O
over	O
the	O
data	O
,	O
but	O
I	O
can't	O
get	O
anything	O
solid	O
.	O

But	O
sum	B-API
function	O
from	O
numpy	O
doesn't	O
suport	O
"	O
1:3	O
"	O

(	O
the	O
`	O
np.nonzero	B-API
`	O
should	O
return	O
a	O
tuple	O
with	O
one	O
element	O
,	O
an	O
array	O
of	O
indices	O
)	O
.	O

Can	O
the	O
"	O
small	O
values	O
of	O
derivative	O
"	O
be	O
small	O
with	O
respect	O
to	O
the	O
sin	O
curve	O
?	O

6	O
columns	O
,	O
92370574	O
rows	O
,	O
2496502	O
locations	O
,	O
37	O
months	O
each	O
,	O
unique	O
amounts	O
for	O
each	O
value	O
.	O

Note	O
that	O
where	O
possible	O
,	O
`	O
reshape	B-API
`	O
will	O
give	O
you	O
a	O
view	O
of	O
the	O
array	O
.	O

Here	O
you	O
append	O
only	O
a	O
REFERENCE	O
to	O
your	O
only	O
one	O
existing	O
`	O
energy	O
`	O
array	O
.	O

And	O
you	O
can	O
combine	O
the	O
summation	O
and	O
multiplication	O
into	O
a	O
dot	O
product	O
:	O
#CODE	O

For	O
example	O
,	O
`	O
a	O
`	O
is	O
generated	O
from	O
`	O
a	O
=	O
z	O
[	O
z	O
!	O
=0	O
]`	O
;	O
`	O
a	O
`	O
then	O
changes	O
through	O
some	O
processing	O
,	O
and	O
now	O
I	O
need	O
to	O
insert	O
`	O
nan	O
`	O
s	O
where	O
there	O
were	O
originally	O
zeros	O
.	O

I	O
frequently	O
use	O
the	O
numpy.where	B-API
function	O
to	O
gather	O
a	O
tuple	O
of	O
indices	O
of	O
a	O
matrix	O
having	O
some	O
property	O
.	O

I	O
suspect	O
the	O
original	O
formula	O
was	O
right	O
but	O
you	O
didn't	O
encode	O
it	O
right	O
in	O
Python	O
.	O

This	O
gets	O
me	O
the	O
sum	O
of	O
all	O
red	O
combined	O
in	O
original	O
-	O
all	O
red	O
combined	O
in	O
mutated	O
.	O

`	O
p2	O
=	O
einsum	B-API
(	O
'	O
nk	O
,	O
nk	O
->	O
n	O
'	O
,	O
p1	O
,	O
delta	O
)`	O
is	O
the	O
pairwise	O
dot	O
product	O
of	O
the	O
rows	O
of	O
`	O
p1	O
`	O
and	O
`	O
delta	O
`	O
.	O

I	O
did	O
the	O
reshape	B-API
,	O
just	O
so	O
that	O
both	O
arrays	O
are	O
same	O
shape	O
,	O
but	O
I	O
do	O
not	O
think	O
you	O
really	O
need	O
the	O
reshaping	O
,	O
with	O
the	O
list	O
comprehension	O
the	O
shape	O
of	O
array	O
you	O
get	O
is	O
`	O
(	O
length	O
of	O
string	O
,	O
)`	O

Also	O
,	O
I	O
expect	O
the	O
positions	O
of	O
the	O
zeros	O
to	O
be	O
relatively	O
sparse	O
(	O
~1%	O
of	O
all	O
bit	O
positions	O
)	O
.	O

Slicing	O
arrays	O
with	O
meshgrid	B-API
/	O
array	O
indices	O
in	O
Numpy	O

(	O
An	O
nonzero	O
exit	O
status	O
usually	O
indicates	O
an	O
error	O
on	O
Unix	O
style	O
systems	O
.	O
A	O
couple	O
programs	O
are	O
different	O
,	O
e.g.	O
,	O
`	O
diff	B-API
`	O
.	O
)	O
Try	O
examining	O
the	O
`	O
stderr	O
`	O
produced	O
by	O
the	O
subprocess	O
to	O
see	O
what	O
error	O
messages	O
are	O
printed	O
there	O
.	O

To	O
achieve	O
exactly	O
what	O
you	O
are	O
asking	O
for	O
I	O
would	O
apply	O
a	O
`	O
[	O
3x3	O
]`	O
box-filter	O
on	O
the	O
image	O
and	O
than	O
I	O
would	O
resize	O
the	O
matrix	O
using	O
nearest	O
neighbor	O
interpolation	O
.	O

Is	O
there	O
a	O
quick	O
way	O
to	O
reshape	O
my	O
`	O
csr_matrix	O
`	O
without	O
copying	O
everything	O
in	O
it	O
?	O

The	O
catch	O
is	O
that	O
I	O
need	O
to	O
keep	O
the	O
colors	O
exactly	O
the	O
way	O
they	O
are	O
(	O
background	O
:	O
I'm	O
resizing	O
a	O
map	O
where	O
provinces	O
are	O
color-coded	O
)	O
,	O
and	O
so	O
I	O
cannot	O
just	O
perform	O
a	O
resize	O
with	O
bicubic	O
interpolation	O
,	O
because	O
that	O
will	O
also	O
interpolate	O
the	O
pixel	O
colors	O
while	O
smoothing	O
.	O

You	O
should	O
"	O
flatten	O
"	O
the	O
array	O
of	O
arrays	O
first	O
.	O
unfortunately	O
,	O
there's	O
no	O
builtin	O
method	O
,	O
see	O
#URL	O

then	O
concatenate	O
the	O
saved	O
objects	O
whit	O
this	O
code	O
:	O
#CODE	O

For	O
something	O
like	O
a	O
dot	O
product	O
,	O
pandas	O
`	O
DataFrames	O
`	O
are	O
generally	O
going	O
to	O
be	O
slower	O
than	O
a	O
numpy	O
array	O
since	O
pandas	O
is	O
doing	O
**	O
a	O
lot	O
more	O
stuff	O
**	O
aligning	O
labels	O
,	O
potentially	O
dealing	O
with	O
heterogenous	O
types	O
,	O
and	O
so	O
on	O
.	O

I	O
want	O
to	O
pass	O
an	O
array	O
of	O
indices	O
and	O
column	O
names	O
and	O
get	O
a	O
list	O
of	O
objects	O
that	O
are	O
found	O
in	O
the	O
corresponding	O
index	O
and	O
column	O
name	O
.	O

From	O
this	O
you	O
would	O
expect	O
the	O
total	O
sum	O
to	O
be	O
`	O
100,679,697	O
=	O
200*	O
(	O
1,000,000	O
-	O
499,097	O
)	O
+	O
499,097	O
`	O

The	O
histogram	O
way	O
is	O
not	O
the	O
fastest	O
,	O
and	O
can't	O
tell	O
the	O
difference	O
between	O
an	O
arbitrarily	O
small	O
separation	O
of	O
points	O
and	O
`	O
2	O
*	O
sqrt	B-API
(	O
2	O
)	O
*	O
b	O
`	O
(	O
where	O
`	O
b	O
`	O
is	O
bin	O
width	O
)	O
.	O

}	O
for	O
n=1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
(	O
using	O
Sum	B-API
(	O
c_n	O
exp	B-API
(	O
i	O
2	O
pi	O
n	O
x	O
)	O
)	O
as	O
Fourier	O
series	O
)	O
.	O

I	O
think	O
I	O
can	O
t	O
just	O
simple	O
sum	O
the	O
"	O
seq	O
*	O
"	O
array	O
,	O
because	O
instead	O
of	O
a	O
chord	O
I	O
will	O
get	O
noise	O
.	O

I	O
presume	O
you	O
want	O
to	O
transpose	O
first	O
:	O
#CODE	O

Oh	O
,	O
that's	O
interesting	O
you	O
can	O
do	O
it	O
with	O
stack	B-API
.	O

In	O
this	O
case	O
,	O
using	O
numpy	O
outer	B-API
operations	O
allow	O
you	O
to	O
compute	O
the	O
multiplications	O
and	O
sums	O
at	O
the	O
`	O
C	O
`	O
loop	O
speed	O
.	O

The	O
most	O
efficient	O
way	O
is	O
likely	O
to	O
use	O
'	O
np.empty()	B-API
'	O
to	O
allocate	O
the	O
space	O
/	O
memory	O
for	O
your	O
end	O
dataset	O
and	O
then	O
load	O
data	O
&	O
broadcast	O
within	O
that	O
using	O
slice	O
indexing	O
.	O

Ok	O
,	O
with	O
your	O
histogram	O
I	O
get	O
at	O
least	O
the	O
total	O
number	O
of	O
each	O
pair	O
.	O

This	O
is	O
because	O
python's	O
sum	O
is	O
basically	O
summing	O
a	O
for	O
loop	O
over	O
the	O
object	O
.	O

Then	O
the	O
entire	O
shape	O
changes	O
from	O
(	O
x	O
,	O
y	O
)	O
to	O
merely	O
(	O
x	O
,	O
)	O
and	O
I	O
get	O
'	O
too	O
many	O
indices	O
'	O
errors	O
when	O
I	O
try	O
to	O
use	O
masks	O
.	O

If	O
reps	O
has	O
length	O
d	O
,	O
the	O
result	O
will	O
have	O
dimension	O
of	O
max	B-API
(	O
d	O
,	O
A.ndim	O
)	O
.	O

I	O
want	O
to	O
do	O
this	O
by	O
dividing	O
each	O
histogram	O
by	O
its	O
maximum	O
value	O
so	O
all	O
the	O
distributions	O
have	O
the	O
same	O
scale	O
.	O

An	O
obvious	O
path	O
would	O
be	O
to	O
transpose	O
the	O
array	O
so	O
that	O
the	O
indices	O
that	O
I	O
am	O
selecting	O
would	O
come	O
up	O
first	O
.	O

Now	O
,	O
for	O
mean	O
calculations	O
,	O
those	O
numeric	O
IDs	O
could	O
be	O
used	O
as	O
`"	O
weights	O
"`	O
for	O
binning	O
with	O
`	O
np.bincount	B-API
`	O
,	O
giving	O
us	O
the	O
sum	O
of	O
data	O
elements	O
corresponding	O
to	O
each	O
`	O
ID	O
`	O
.	O

However	O
,	O
what	O
I	O
need	O
is	O
a	O
string	O
containing	O
all	O
the	O
elements	O
in	O
the	O
list	O
linked	O
by	O
'	O
;	O
'	O
,	O
not	O
the	O
list	O
itself	O
,	O
so	O
it	O
seems	O
like	O
I	O
have	O
to	O
sum	O
all	O
the	O
elements	O
in	O
asString	O
with	O
another	O
iteration	O
?	O

the	O
output	O
I	O
need	O
:	O
`	O
S	O
=	O
[	O
2	O
,	O
5	O
,	O
8	O
,	O
11	O
,	O
14	O
]`	O
I	O
thought	O
something	O
like	O
:	O
`	O
S1	O
=	O
np.array	B-API
(	O
L	O
[:	O
]	O
[	O
1	O
,	O
0	O
])`	O
should	O
work	O
but	O
whatever	O
I	O
try	O
I	O
have	O
the	O
error	O
like	O
:	O
`	O
TypeError	O
:	O
list	O
indices	O
must	O
be	O
integers	O
,	O
not	O
tuple	O
`	O
.	O

I	O
need	O
it	O
because	O
in	O
the	O
next	O
part	O
I	O
will	O
sum	O
up	O
this	O
large	O
np.array	B-API
with	O
some	O
delta_array	O
that	O
has	O
the	O
same	O
shape	O
.	O

Used	O
reshape	B-API
to	O
make	O
rows	O
into	O
columns	O
.	O

I	O
understand	O
that	O
you	O
could	O
create	O
an	O
array	O
of	O
zeros	O
and	O
iteratively	O
change	O
the	O
values	O
in	O
each	O
column	O
,	O
but	O
I	O
also	O
understand	O
this	O
is	O
not	O
an	O
efficient	O
method	O
.	O

I'm	O
trying	O
to	O
implement	O
the	O
univariate	O
gradient	O
descent	O
algorithm	O
in	O
python	O
.	O

numpy	O
glossary	O
says	O
the	O
sum	O
along	O
axis	O
argument	O
`	O
axis=1	O
`	O
sums	O
over	O
rows	O
:	O
"	O
we	O
can	O
sum	O
each	O
row	O
of	O
an	O
array	O
,	O
in	O
which	O
case	O
we	O
operate	O
along	O
columns	O
,	O
or	O
axis	O
1	O
"	O
.	O

It	O
also	O
prints	O
out	O
the	O
new	O
indices	O
signature	O
.	O

At	O
first	O
,	O
your	O
`	O
result	O
`	O
does	O
not	O
look	O
like	O
a	O
complex	O
FFT	O
output	O

debug	O
performance	O
diff	O
of	O
Same	O
code	O
on	O
nearly	O
same	O
cpu	O
/	O
ram	O

The	O
HTML	O
file	O
generated	O
by	O
Cython	O
indicates	O
that	O
the	O
bottleneck	O
is	O
the	O
dot	O
products	O
(	O
which	O
is	O
expected	O
of	O
course	O
)	O
.	O

`	O
numpy.unique	B-API
`	O
with	O
`	O
return_index=True	O
`	O
will	O
give	O
you	O
a	O
list	O
of	O
indices	O
to	O
take	O
from	O
.	O

I	O
forgot	O
exactly	O
why	O
,	O
but	O
there	O
is	O
a	O
good	O
reason	O
why	O
you	O
calculate	O
it	O
as	O
the	O
ratio	O
between	O
these	O
two	O
averages	O
,	O
instead	O
of	O
directly	O
averaging	O
`	O
fft	B-API
(	O
y	O
)	O
/	O
fft	B-API
(	O
x	O
)`	O
.	O

Do	O
you	O
really	O
want	O
this	O
'	O
roll	B-API
'	O
?	O

By	O
adding	O
a	O
nonzero	O
number	O
at	O
the	O
end	O
of	O
the	O
array	O
,	O
you	O
can	O
still	O
use	O
np.nonzero	B-API
to	O
get	O
your	O
desired	O
outcome	O
.	O

which	O
simply	O
sorts	O
the	O
terms	O
and	O
then	O
takes	O
the	O
ones	O
which	O
aren't	O
equal	O
to	O
the	O
previous	O
one	O
.	O

4	O
:	O
I	O
am	O
not	O
sure	O
about	O
the	O
indices	O
,	O
by	O
writing	O
couple	O
of	O
code	O
lines	O
I	O
just	O
able	O
to	O
get	O
cluster	O
indices	O
based	O
on	O
fclusterdata	O
.	O

Matlab	O
gives	O
me	O
a	O
norm	O
=	O
2	O
for	O
your	O
matrix	O
.	O

I	O
first	O
generated	O
a	O
labelled	O
array	O
of	O
unique	O
IDs	O
for	O
each	O
discrete	O
region	O
,	O
calculated	O
sizes	O
for	O
each	O
ID	O
,	O
masked	O
the	O
size	O
array	O
to	O
focus	O
only	O
on	O
size	O
==	O
1	O
blobs	O
,	O
then	O
index	O
the	O
original	O
array	O
and	O
set	O
IDs	O
with	O
a	O
size	O
==	O
1	O
to	O
0	O
:	O
#CODE	O

absolute	B-API
(	O
a	O
-	O
b	O
)	O
=	O
(	O
atol	O
+	O
rtol	O
*	O
absolute	B-API
(	O
b	O
))	O

Then	O
I	O
reshape	O
this	O
to	O
form	O
a	O
2D	O
numpy	O
array	O
.	O

n=5	O
(	O
min	O
length	O
of	O
sequence	O
)	O

I	O
have	O
written	O
a	O
function	O
which	O
contains	O
nested	O
loops	O
and	O
a	O
conditional	O
statement	O
;	O
the	O
purpose	O
of	O
the	O
loop	O
is	O
to	O
return	O
a	O
list	O
of	O
indices	O
for	O
the	O
nearest	O
elements	O
in	O
array	O
x	O
when	O
compared	O
to	O
array	O
y	O
.	O

I	O
also	O
want	O
to	O
color	O
the	O
1D	O
histogram	O
bars	O
according	O
to	O
the	O
same	O
normalization	O
.	O

If	O
you	O
are	O
calling	O
it	O
with	O
an	O
empty	O
matrix	O
for	O
[	O
low	O
,	O
high	O
]	O
it	O
will	O
just	O
use	O
whatever	O
the	O
max	O
and	O
min	O
values	O
in	O
the	O
array	O
are	O
.	O

Creating	O
a	O
class	O
deriving	O
from	O
`	O
ndarray	B-API
`	O
and	O
overriding	O
indexing	O
such	O
that	O
the	O
absolute	O
indices	O
are	O
used	O
.	O

One	O
solution	O
is	O
to	O
sort	O
both	O
arrays	O
(	O
adding	O
an	O
index	O
column	O
so	O
that	O
the	O
sorted	O
arrays	O
still	O
contains	O
the	O
original	O
indices	O
)	O
.	O

Use	O
`	O
reshape	B-API
`	O
:	O
#CODE	O

What's	O
wrong	O
with	O
the	O
normal	O
div	O
/	O
mod	O
operations	O
?	O

You	O
can	O
use	O
`	O
argmin	B-API
`	O
to	O
find	O
the	O
False	O
values	O
,	O
and	O
this	O
will	O
be	O
faster	O
and	O
take	O
less	O
memory	O
than	O
using	O
nonzero	B-API
,	O
but	O
this	O
is	O
linear	O
in	O
the	O
length	O
of	O
`	O
a	O
`	O
.	O

I'd	O
like	O
it	O
to	O
be	O
like	O
8x10^8	O
or	O
.8x10	O
^9	O
to	O
save	O
space	O
instead	O
of	O
putting	O
all	O
those	O
zeros	O
.	O

The	O
one	O
I	O
pointed	O
out	O
in	O
a	O
comment	O
to	O
other	O
answer	O
as	O
to	O
encode	O
the	O
binary	O
representation	O
of	O
the	O
array	O
as	O
a	O
Base64	O
text	O
block	O
.	O

due	O
to	O
broadcasting	O
,	O
you	O
don't	O
need	O
to	O
repeat	O
duplicate	O
indices	O
,	O
thus	O
:	O
#CODE	O

Maybe	O
`	O
flatten()	B-API
`	O
the	O
original	O
array	O
,	O
then	O
use	O/
your	O
1D	O
solution	O
,	O
finally	O
calculate	O
the	O
real	O
nD	O
indices	O
using	O
the	O
original	O
shape	O
?	O

Note	O
that	O
the	O
diagonal	O
is	O
always	O
zero	O
since	O
`	O
mahalanobis	B-API
(	O
x	O
,	O
x	O
)`	O
equals	O
zero	O
for	O

possible	O
duplicate	O
of	O
[	O
NumPy	O
min	B-API
/	O
max	B-API
in-place	O
assignment	O
]	O
(	O
#URL	O
)	O

Once	O
the	O
tree	O
structure	O
has	O
been	O
built	O
,	O
go	O
back	O
and	O
collect	O
all	O
the	O
branches	O
and	O
leaves	O
into	O
the	O
array	O
structure	O
and	O
by	O
definition	O
,	O
they	O
will	O
be	O
unique	O
.	O

I	O
wrote	O
the	O
following	O
code	O
but	O
the	O
output	O
only	O
contains	O
the	O
ids	O
(	O
single	O
column	O
)	O
.	O

Maximum	O
is	O
always	O
bigger	O
than	O
the	O
minimum	O
(	O
more	O
to	O
the	O
right	O
on	O
a	O
1d	O
axis	O
,	O
not	O
by	O
absolute	O
value	O
)	O
.	O

Note	O
that	O
`	O
unq_count	O
`	O
doesn't	O
count	O
the	O
occurrences	O
of	O
the	O
last	O
unique	O
item	O
,	O
because	O
that	O
is	O
not	O
needed	O
to	O
split	O
the	O
index	O
array	O
.	O

If	O
yes	O
,	O
you	O
can	O
use	O
the	O
Linux	O
terminal	O
to	O
strip	O
quotes	O
from	O
the	O
ends	O
of	O
the	O
rows	O
quickly	O
.	O

The	O
append	B-API
method	O
for	O
a	O
numpy	O
array	O
returns	O
a	O
copy	O
of	O
the	O
array	O
with	O
new	O
items	O
added	O
to	O
the	O
end	O
.	O

I	O
want	O
to	O
get	O
the	O
norm	B-API
of	O
this	O
array	O
using	O
numpy	O
.	O

The	O
only	O
problem	O
here	O
is	O
that	O
I	O
think	O
it	O
will	O
append	O
directly	O
to	O
the	O
column	O
,	O
when	O
I	O
would	O
prefer	O
it	O
to	O
append	O
to	O
a	O
new	O
column	O
.	O

You	O
don't	O
need	O
to	O
import	O
string	O
,	O
and	O
you	O
don't	O
need	O
to	O
loop	O
through	O
all	O
the	O
lines	O
and	O
append	O
text	O
or	O
count	O
the	O
characters	O
.	O

The	O
transpose	O
of	O
the	O
transpose	O
of	O
a	O
matrix	O
==	O
that	O
matrix	O
,	O
or	O
,	O
[	O
A^T	O
]	O
^T	O
==	O
A	O
.	O

Currently	O
I	O
am	O
looping	O
through	O
the	O
arrays	O
and	O
using	O
numpy.dstack	B-API
to	O
stack	O
the	O
1000	O
arrays	O
into	O
a	O
rather	O
large	O
3d	O
array	O
...	O
and	O
then	O
will	O
calculate	O
the	O
mean	O
across	O
the	O
3rd	O
(	O
?	O
)	O
dimension	O
.	O

If	O
you	O
strip	O
all	O
these	O
out	O
and	O
just	O
call	O
lapack	O
in	O
your	O
for	O
loop	O
(	O
since	O
you	O
already	O
know	O
the	O
dimensions	O
of	O
your	O
matrix	O
and	O
maybe	O
know	O
that	O
it's	O
real	O
,	O
not	O
complex	O
)	O
,	O
things	O
run	O
MUCH	O
faster	O
(	O
Note	O
that	O
I've	O
made	O
my	O
array	O
larger	O
)	O
:	O
#CODE	O

First	O
,	O
you	O
have	O
a	O
binomial	O
response	O
:	O
having	O
or	O
not	O
having	O
a	O
particular	O
behavior	O
.	O

The	O
call	O
to	O
`	O
np.sqrt	B-API
`	O
,	O
which	O
is	O
a	O
Python	O
function	O
call	O
,	O
is	O
killing	O
your	O
performance	O
You	O
are	O
computing	O
the	O
square	O
root	O
of	O
scalar	O
floating	O
point	O
value	O
,	O
so	O
you	O
should	O
use	O
the	O
`	O
sqrt	B-API
`	O
function	O
from	O
the	O
C	O
math	O
library	O
.	O

This	O
would	O
call	O
the	O
function	O
`	O
np.loadtxt	B-API
`	O
which	O
would	O
load	O
the	O
file	O
`	O
GPBUSD1d.txt	O
'`	O
and	O
transpose	O
(	O
"	O
unpack	O
")	O
it	O
.	O

You	O
can't	O
change	O
the	O
typing	O
of	O
the	O
array	O
in-place	O
(	O
unless	O
I'm	O
grossly	O
mistaken	O
)	O
,	O
but	O
you	O
can	O
floor	B-API
.	O

Finally	O
I	O
just	O
transpose	O
the	O
dataframe	O
to	O
get	O
ids	O
as	O
rows	O
and	O
categories	O
as	O
columns	O
.	O

The	O
following	O
way	O
of	O
obtaining	O
the	O
unique	O
elements	O
in	O
all	O
sub-arrays	O
is	O
very	O
fast	O
:	O
#CODE	O

You	O
can't	O
use	O
the	O
numpy	O
reshape	B-API
for	O
a	O
simple	O
reason	O
:	O
you	O
have	O
data	O
duplicity	O
in	O
your	O
original	O
array	O
(	O
time	O
and	O
positions	O
)	O
and	O
not	O
in	O
the	O
result	O
you	O
want	O
.	O

So	O
it	O
does	O
not	O
make	O
much	O
sense	O
to	O
me	O
to	O
reshape	O
it	O
to	O
a	O
"	O
1d-matrix	O
"	O
.	O

Now	O
create	O
5-bit	O
bitstrings	O
from	O
each	O
integer	O
and	O
join	O
them	O
together	O
:	O
#CODE	O

It	O
would	O
probably	O
be	O
just	O
as	O
much	O
work	O
to	O
translate	O
the	O
top	O
Matlab	O
routine	O
from	O
Maurits	O
.	O

In	O
the	O
particular	O
case	O
of	O
your	O
example	O
,	O
where	O
your	O
unique	O
values	O
are	O
sequential	O
integers	O
,	O
you	O
can	O
use	O
`	O
find_objects	B-API
`	O
directly	O
.	O

axis=1	O
refers	O
to	O
working	O
on	O
rows	O
in	O
this	O
2d	O
case	O
(	O
axis=0	O
,	O
in	O
contrast	O
,	O
would	O
be	O
getting	O
you	O
the	O
max	B-API
in	O
each	O
column	O
)	O

There	O
are	O
many	O
other	O
`	O
ufunc	B-API
`	O
,	O
and	O
other	O
iteration	O
modes	O
-	O
`	O
accumulate	B-API
`	O
,	O
`	O
reduceat	B-API
`	O
.	O

All	O
diagonal	O
elements	O
will	O
be	O
of	O
the	O
form	O
`	O
s_i	O
**	O
2	O
/	O
s_i	O
**	O
2	O
==	O
1	O
`	O
.	O

@USER	O
In	O
the	O
example	O
above	O
,	O
I	O
get	O
the	O
following	O
error	O
:	O
Non-broadcastable	O
operand	O
with	O
shape	O
(	O
100	O
)	O
doesn't	O
match	O
the	O
broadcast	B-API
shape	O
(	O
100,100	O
)	O

is	O
calculated	O
such	O
that	O
all	O
but	O
the	O
diagonal	O
#CODE	O

To	O
compute	O
the	O
number	O
of	O
unique	O
elements	O
in	O
a	O
numpy	O
array	O
,	O
you	O
can	O
use	O
`	O
unique	B-API
(	O
x	O
)	O
.size	O
`	O
or	O
`	O
len	B-API
(	O
unique	B-API
(	O
x	O
))`	O
(	O
see	O
`	O
numpy.unique	B-API
`	O
)	O
.	O

Or	O
would	O
that	O
basically	O
require	O
implementing	O
the	O
outer	O
loop	O
in	O
Cython	O
?	O

For	O
a	O
tensor	O
it	O
is	O
not	O
clear	O
how	O
to	O
define	O
an	O
inverse	O
or	O
a	O
transpose	O
.	O

Second	O
,	O
you	O
are	O
doing	O
transpose	B-API
the	O
hard	O
way	O
.	O

Where	O
does	O
log	B-API
(	O
b	O
,	O
2	O
)	O
come	O
from	O
?	O

(	O
The	O
values	O
in	O
the	O
corners	O
correspond	O
to	O
the	O
diagonal	O
elements	O
.	O
)	O

I	O
tried	O
using	O
the	O
scipy.stat	B-API
module	O
by	O
creating	O
my	O
numbers	O
with	O
`	O
np.random.normal	B-API
`	O
,	O
since	O
it	O
only	O
takes	O
data	O
and	O
not	O
stat	O
values	O
like	O
mean	O
and	O
std	O
dev	O
(	O
is	O
there	O
any	O
way	O
to	O
use	O
these	O
values	O
directly	O
)	O
.	O

The	O
asymptotic	O
complexity	O
of	O
both	O
of	O
the	O
`	O
matrix_rank	B-API
`	O
and	O
`	O
det	B-API
`	O
calls	O
are	O
therefore	O
O	O
(	O
n^3	O
)	O
,	O
the	O
complexity	O
of	O
LU	O
decomposition	O
.	O

I	O
think	O
the	O
np.std()	B-API
is	O
just	O
universal	O
std	B-API
.	O

Golub	O
and	O
Van	O
Loan	O
also	O
provide	O
a	O
way	O
of	O
storing	O
a	O
matrix	O
in	O
diagonal	O
dominant	O
form	O
.	O

I	O
see	O
no	O
reason	O
why	O
`	O
numpy	O
`	O
would	O
need	O
to	O
make	O
a	O
copy	O
for	O
an	O
operation	O
like	O
this	O
,	O
as	O
long	O
as	O
it	O
does	O
the	O
necessary	O
checks	O
for	O
overlaps	O
(	O
though	O
of	O
course	O
as	O
others	O
have	O
noted	O
,	O
`	O
resize	B-API
`	O
may	O
itself	O
have	O
to	O
allocate	O
a	O
new	O
block	O
of	O
memory	O
)	O
.	O

I	O
found	O
another	O
stack	O
question	O
about	O
this	O
here	O
,	O
but	O
I	O
am	O
not	O
entirely	O
sure	O
how	O
it	O
was	O
resolved	O
,	O
I'm	O
still	O
a	O
little	O
confused	O
.	O

Maybe	O
`	O
floor	B-API
(	O
arange	B-API
(	O
0	O
,	O
10	O
,	O
0.1	O
))`	O
?	O

In	O
python	O
,	O
I	O
would	O
like	O
to	O
convolve	O
the	O
two	O
matrices	O
along	O
the	O
second	O
axis	O
only	O
.	O

`	O
view	B-API
`	O
is	O
basically	O
taking	O
your	O
two	O
coordinates	O
as	O
a	O
single	O
variable	O
that	O
can	O
be	O
used	O
to	O
find	O
the	O
unique	O
coordinates	O
.	O

Keep	O
in	O
mind	O
that	O
machine	O
precision	O
for	O
a	O
32-bit	O
double	O
is	O
~	O
10^-16	O
,	O
which	O
will	O
be	O
an	O
absolute	O
limiting	O
factor	O
.	O

Also	O
,	O
if	O
there	O
is	O
then	O
I	O
could	O
just	O
append	O
to	O
the	O
b	O
and	O
c	O
arrays	O
each	O
time	O
instead	O
of	O
overwriting	O
and	O
starting	O
from	O
scratch	O
each	O
loop	O
.	O

Use	O
`	O
multiprocessing.Process	B-API
(	O
target	O
=	O
somefunc	O
,	O
args	O
=	O
(	O
sa	O
,	O
)`	O
(	O
and	O
`	O
start	O
`	O
,	O
maybe	O
`	O
join	B-API
`)	O
to	O
call	O
`	O
somefunc	O
`	O
in	O
a	O
separate	O
process	O
,	O
passing	O
the	O
shared	O
array	O
.	O

Take	O
a	O
look	O
a	O
the	O
concatenate	B-API
function	O
.	O

Unlike	O
Joe	O
Kington's	O
answer	O
,	O
the	O
benefit	O
of	O
this	O
is	O
that	O
you	O
don't	O
need	O
to	O
know	O
the	O
original	O
shape	O
of	O
the	O
data	O
in	O
the	O
`	O
.mat	O
`	O
file	O
,	O
i.e.	O
no	O
need	O
to	O
reshape	O
upon	O
reading	O
in	O
.	O

but	O
I	O
think	O
,	O
finding	O
the	O
local	O
max	O
can	O
be	O
simplified	O
to	O
:	O
#CODE	O

@USER	O
`	O
swapaxes	B-API
`	O
seemed	O
to	O
be	O
indistinguishable	O
from	O
`	O
transpose	B-API
(	O
0	O
,	O
2	O
,	O
1	O
)`	O
.	O

Do	O
gradient	B-API
actually	O
compute	O
really	O
a	O
gradient	O
?	O

I	O
would	O
suggest	O
to	O
first	O
program	O
it	O
with	O
`	O
np.nditer	B-API
`	O
and	O
then	O
translate	O
it	O
into	O
C	O
.	O

As	O
you	O
can	O
see	O
,	O
using	O
the	O
join	B-API
function	O
on	O
the	O
list	O
(	O
`	O
binary_list	O
`)	O
works	O
properly	O
,	O
but	O
on	O
the	O
equivalent	O
numpy	O
array	O
(	O
`	O
binary_split_array	O
`)	O
it	O
doesn't	O
:	O
we	O
can	O
see	O
the	O
string	O
returned	O
is	O
only	O
72	O
characters	O
long	O
instead	O
of	O
80	O
.	O

@USER	O
.B	O
.	O
the	O
above	O
question	O
is	O
significantly	O
different	O
from	O
mine	O
;	O
it	O
asks	O
for	O
both	O
min	B-API
and	O
max	B-API
,	O
and	O
it	O
is	O
for	O
2D	O
matrix	O

This	O
will	O
join	O
the	O
rows	O
and	O
write	O
them	O
to	O
a	O
new	O
csv	O
:	O
#CODE	O

The	O
reason	O
I	O
have	O
`	O
-det	O
(	O
mat	O
)`	O
in	O
the	O
energy	O
function	O
is	O
because	O
the	O
simulated	O
annealing	O
algorithm	O
does	O
minimization	O
.	O

Also	O
is	O
`	O
x	O
`	O
unique	O
?	O

Pandas	O
append	B-API
filtered	O
row	O
to	O
another	O
DataFrame	O

Again	O
,	O
the	O
code	O
notes	O
that	O
set	O
of	O
combinations	O
is	O
not	O
unique	O
;	O
but	O
it	O
does	O
have	O
a	O
unique	O
subset	O
,	O
namely	O
[[	O
2	O
3	O
]	O
,	O
[	O
0	O
1	O
]]	O
,	O
which	O
as	O
you	O
just	O
revealed	O
,	O
you	O
do	O
consider	O
a	O
valid	O
combination	O
.	O

That	O
concatenate	B-API
action	O
should	O
be	O
pretty	O
fast	O
.	O

If	O
you	O
want	O
to	O
pass	O
in	O
the	O
transpose	O
,	O
you'll	O
need	O
to	O
set	O
`	O
rowvar	O
`	O
to	O
zero	O
.	O

You	O
can	O
override	O
this	O
behavior	O
by	O
using	O
the	O
arguments	O
`	O
vmin	O
`	O
and	O
`	O
vmax	O
`	O
(	O
or	O
`	O
norm	O
`)	O
of	O
`	O
imshow	B-API
`	O
.	O

@USER	O
,	O
`	O
cs	O
`	O
is	O
sorted	O
and	O
`	O
searchsorted()	B-API
`	O
exploits	O
that	O
to	O
do	O
a	O
binary	O
search	O
-	O
only	O
`	O
O	O
(	O
log	B-API
(	O
len	B-API
(	O
weights	O
)))`	O
comparisons	O
are	O
needed	O
.	O

Think	O
`	O
flatten	B-API
`	O
without	O
the	O
copy	O
.	O

In	O
your	O
case	O
it	O
looks	O
like	O
the	O
weight	O
arrays	O
will	O
have	O
the	O
same	O
dimension	O
as	O
'	O
A	O
'	O
,	O
so	O
you	O
reshape	O
them	O
accordingly	O
and	O
multiply	O
dx	O
and	O
dy	O
by	O
their	O
individual	O
weight	O
vectors	O
.	O

Does	O
this	O
mean	O
the	O
standard	O
error	O
of	O
the	O
gradient	O
or	O
intercept	O
?	O

Also	O
,	O
the	O
algo	O
has	O
a	O
lot	O
of	O
matrices	O
manipulation	O
(	O
fft	B-API
,	O
filters	O
,	O
etc	O
.	O
)	O
,	O
so	O
using	O
numpy	O
/	O
scipy	O
should	O
result	O
in	O
faster	O
run	O
time	O
.	O

You	O
can	O
broadcast	O
that	O
into	O
an	O
array	O
using	O
expressions	O
,	O
for	O
example	O
#CODE	O

If	O
I	O
use	O
the	O
above	O
test	O
on	O
the	O
absolute	O
values	O
of	O
the	O
angles	O
to	O
be	O
tested	O
,	O
everything	O

The	O
returned	O
gradient	O
hence	O
has	O

"	O
In	O
the	O
first	O
case	O
the	O
gradient	O
is	O
1	O
mV	O
/	O
ms	O
,	O
in	O
the	O
second	O
case	O
it	O
is	O
50	O
mV	O
/	O
ms	O
.	O

If	O
True	O
,	O
uses	O
the	O
old	O
behavior	O
from	O
Numeric	O
,	O
(	O
correlate	B-API
(	O
a	O
,	O
v	O
)	O
==	O
correlate	B-API
(	O
v	O
,	O
a	O
)	O
,	O
and	O
the	O
conjugate	O
is	O
not	O
taken	O
for	O
complex	O
arrays	O
)	O
.	O

Why	O
don't	O
you	O
just	O
compress	O
the	O
files	O
with	O
the	O
built-in	O
`	O
gzip	O
`	O
module	O
?	O

So	O
you	O
need	O
to	O
write	O
some	O
function	O
that	O
convert	O
a	O
poly	O
parameters	O
array	O
to	O
a	O
latex	O
string	O
,	O
here	O
is	O
an	O
example	O
:	O
#CODE	O

In	O
your	O
example	O
,	O
the	O
square	O
root	O
is	O
calculated	O
by	O
evaluating	O
the	O
the	O
module	O
and	O
the	O
argument	O
of	O
your	O
complex	O
number	O
(	O
essentially	O
via	O
the	O
log	B-API
function	O
,	O
which	O
returns	O
log	B-API
(	O
module	O
)	O
+	O
i	O
phase	O
)	O
.	O

I	O
am	O
trying	O
to	O
run	O
hstack	B-API
to	O
join	O
a	O
column	O
of	O
integer	O
values	O
to	O
a	O
list	O
of	O
columns	O
created	O
by	O
a	O
TF-IDF	O
(	O
so	O
I	O
can	O
eventually	O
use	O
all	O
of	O
these	O
columns	O
/	O
features	O
in	O
a	O
classifier	O
)	O
.	O

How	O
to	O
pass	O
these	O
`	O
norm	O
`	O
and	O
`	O
cmap	O
`	O
parameters	O
in	O
matplotlib	O
to	O
`	O
plt.show	B-API
`	O
or	O
`	O
imshow()	B-API
`	O
?	O

Forget	O
about	O
the	O
C	O
stack	O
,	O
numpy	O
objects	O
can't	O
use	O
it	O
.	O

You	O
can	O
use	O
the	O
append	B-API
function	O
as	O
he	O
has	O
defined	O
.	O

This	O
can	O
be	O
particularly	O
tricky	O
when	O
trying	O
to	O
append	O
to	O
a	O
numpy	O
array	O
quickly	O
.	O

I	O
have	O
a	O
question	O
regarding	O
to	O
the	O
`	O
fft	B-API
`	O
and	O
`	O
ifft	B-API
`	O
functions	O
.	O

So	O
for	O
now	O
,	O
I	O
just	O
changed	O
the	O
max	B-API
(	O
z	O
)	O
to	O
a	O
number	O
that	O
I	O
know	O
is	O
the	O
max	B-API
(	O
1567	O
)	O
.	O

The	O
`	O
add	B-API
`	O
operation	O
does	O
not	O
do	O
the	O
same	O
thing	O
as	O
`	O
join	B-API
`	O
.	O

You	O
don't	O
specify	O
`	O
x	O
`	O
or	O
`	O
y	O
`	O
,	O
and	O
your	O
`	O
mat	O
[:	O
,	O
i+1	O
]`	O
indexing	O
will	O
not	O
work	O
with	O
a	O
structured	O
array	O
.	O

This	O
is	O
because	O
in	O
some	O
cases	O
it's	O
not	O
just	O
NaNs	O
and	O
1s	O
,	O
but	O
other	O
integers	O
,	O
which	O
gives	O
a	O
std	O
>	O
0	O
.	O

`	O
fromiter	B-API
`	O
wants	O
a	O
1d	O
input	O
,	O
e.g.	O
`	O
[	O
1	O
,	O
2	O
,	O
3	O
]`	O
(	O
or	O
the	O
generator	O
equivalent	O
)	O
.	O

read	O
more	O
:	O
take()	B-API

For	O
this	O
I'm	O
using	O
an	O
instance	O
of	O
the	O
`	O
numpy	O
`	O
class	O
`	O
RandomState	B-API
`	O
.	O

You	O
can	O
write	O
a	O
thinly	O
wrapped	O
subclass	O
to	O
`	O
np.ndarray	B-API
`	O
.	O

Using	O
`	O
ndarray.reshape	B-API
`	O
#CODE	O

E.g.	O
this	O
works	O
in	O
the	O
interpreter	O
:	O
`	O
>>>	O
a	O
=	O
np.arange	B-API
(	O
10	O
,	O
dtype=float	O
)	O
.resize	B-API
(	O
1	O
,	O
5	O
)`	O
,	O
because	O
the	O
interpreter	O
doesn't	O
"	O
see	O
"	O
the	O
intermediate	O
value	O
.	O

I	O
attempted	O
your	O
suggestion	O
but	O
got	O
stuck	O
trying	O
to	O
iterate	O
through	O
the	O
existing	O
dtype	B-API
.	O

`	O
numpy.setdiff1d	B-API
(	O
a	O
,	O
a	O
[	O
sel_id	O
])`	O
should	O
do	O
the	O
trick	O
.	O

Instead	O
of	O
disabling	O
the	O
behavior	O
you	O
could	O
try	O
using	O
np.select	B-API
:	O

+1	O
I	O
liked	O
you	O
approach	O
,	O
but	O
how	O
to	O
make	O
`	O
np.copyto()	B-API
`	O
work	O
with	O
a	O
memoryvew	O
?	O

Just	O
import	O
Decimal	O
and	O
for	O
the	O
printing	O
just	O
write	O
print	O
Decimal	O
(	O
ndarray	B-API
[	O
i	O
])	O
.	O

Or	O
,	O
for	O
that	O
matter	O
,	O
numpy.genfromtxt	B-API
.	O

glad	O
to	O
hear	O
it	O
-	O
I	O
only	O
recently	O
found	O
out	O
about	O
`	O
np.einsum	B-API
`	O
myself	O
,	O
and	O
it	O
has	O
rocked	O
my	O
world	O
ever	O
since	O

The	O
`	O
dtype	B-API
`	O
could	O
be	O
deduced	O
from	O
one	O
(	O
or	O
more	O
)	O
of	O
the	O
dictionary	O
items	O
:	O
#CODE	O

I	O
didn't	O
realize	O
`	O
array_split	B-API
`	O
existed	O
!	O

However	O
,	O
in	O
that	O
case	O
,	O
you	O
could	O
just	O
do	O
:	O
(	O
`	O
searchsorted	B-API
`	O
uses	O
bisection	O
)	O
#CODE	O

Btw	O
.	O
you	O
can	O
also	O
implicitly	O
force	O
the	O
`	O
dtype	B-API
`	O
to	O
be	O
`	O
float	O
`	O
when	O
using	O
dots	O
:	O
#CODE	O

dtypes	B-API
.	O

I	O
would	O
prefer	O
using	O
the	O
xor	O
ufunc	O
I	O
think	O
,	O
which	O
is	O
`	O
bitwise_xor	B-API
`	O
(	O
or	O
`	O
logical_xor	B-API
`)	O
:	O
#CODE	O

This	O
is	O
the	O
root	O
of	O
why	O
your	O
`	O
fromarrays	B-API
`	O
works	O
,	O
but	O
not	O
the	O
`	O
append_fields	O
`	O
.	O

The	O
dtype	B-API
should	O
be	O
big	O
endian	O
.	O

parameterArray	O
+=	O
line.split()	O
\nline	O
=	O
self.inputBuffer.next()	O
\	O
nnp.parameterArray	O
=	O
np.array	B-API
(	O
parameterArray	O
)	O

As	O
JoshAdel	O
points	O
out	O
,	O
`	O
vectorize	B-API
`	O
wraps	O
`	O
frompyfunc	B-API
`	O
.	O

Sorry	O
,	O
the	O
line	O
was	O
output	O
[	O
i	O
,	O
j	O
]	O
=	O
np.sum	B-API
(	O
ssd_difference	O
[	O
#URL	O
(	O
)	O
)	O

(	O
or	O
`	O
np.array	B-API
([[	O
1	O
]	O
,	O
[	O
2	O
]	O
,	O
[	O
3	O
]	O
,	O
[	O
4	O
]])	O
.shape	B-API
`)	O

Thank	O
you	O
for	O
the	O
great	O
tipp	O
with	O
`	O
plt.hist	B-API
(	O
img.ravel()	O
)`	O
!	O

The	O
`	O
recarray	B-API
`	O
class	O
accepts	O
an	O
aligned	O
parameter	O
,	O
but	O
looks	O
to	O
lose	O
it	O
in	O
`	O
format_parser	B-API
`	O
.	O

In	O
case	O
someone	O
comes	O
past	O
this	O
,	O
numpy	O
(	O
as	O
of	O
1.8	O
I	O
think	O
)	O
support	O
higher	O
that	O
2D	O
generation	O
of	O
position	O
grids	O
with	O
meshgrid	B-API
.	O

`	O
numpy.random.choice	B-API
`	O
is	O
not	O
implemented	O
in	O
Python	O
but	O
in	O
a	O
`	O
.pyx	O
`	O
file	O
which	O
needs	O
to	O
be	O
compiled	O
to	O
C	O
using	O
Cython	O
.	O

A	O
plain	O
`	O
.copy	B-API
`	O
did	O
work	O
for	O
me	O
.	O

`	O
A	O
[	O
np.ix_	B-API
(	O
x	O
,	O
y	O
)]`	O

einsum	B-API
:	O
5.2	O
s	O

10**423	O
exceeds	O
the	O
largest	O
int	O
representable	O
as	O
an	O
integer	O
(	O
or	O
float	O
)	O
NumPy	O
dtype	B-API
,	O
so	O
there	O
is	O
no	O
point	O
in	O
using	O
NumPy	O
here	O
:	O
`	O
np.iinfo	B-API
(	O
'	O
int64	O
')	O
.max	B-API
<	O
10**423	O
`	O
.	O

Probably	O
,	O
better	O
performance	O
is	O
by	O
using	O
`	O
numpy.fromiter	B-API
`	O
:	O
#CODE	O

Why	O
are	O
the	O
polyfit	B-API
constants	O
from	O
the	O
third	O
case	O
listed	O
as	O
NAN	O
?	O

Try	O
`	O
numpy.array_split	B-API
`	O
.	O

Using	O
np.repeat	B-API
on	O
sub-arrays	O

shows	O
that	O
'	O
region	O
'	O
has	O
an	O
`	O
object	O
`	O
dtype	B-API
:	O
#CODE	O

What	O
I	O
am	O
looking	O
for	O
is	O
something	O
along	O
the	O
original	O
functionality	O
of	O
`	O
np.unique	B-API
`	O
#CODE	O

In	O
my	O
opinion	O
,	O
np.matrix	B-API
should	O
override	O
for	O
addition	O
and	O
subtraction	O
as	O
well	O
.	O

or	O
`	O
np.vstack	B-API
`	O
,	O
`	O
np.dstack	B-API
`	O
`	O
np.r_	B-API
`	O
,	O
`	O
np.c_	B-API
`	O
,	O
`	O
np.concatenate	B-API
`	O
depending	O
on	O
the	O
desired	O
shapes	O
.	O

TypeError	O
when	O
using	O
SymPy	O
matrices	O
for	O
numpy.linalg.eig	B-API

Doing	O
`	O
a.astype	O
(	O
float	O
)`	O
actually	O
creates	O
a	O
*	O
new	O
*	O
ndarray	B-API
which	O
is	O
of	O
type	O
`	O
float	O
`	O
.	O

Trying	O
to	O
vectorize	B-API
the	O
code	O
also	O
resulted	O
in	O
very	O
poor	O
performance	O
,	O

Also	O
look	O
into	O
the	O
genfromtxt	B-API
and	O
loadtxt	B-API
family	O
of	O
Numpy	O
functions	O
.	O

`	O
coll	O
[	O
1	O
]	O
.set_color	B-API
(	O
"	O
r	O
")	O
#	O
this	O
does	O
not	O
work	O
,	O
coll	O
not	O
indexable	O
this	O
way	O
`	O

The	O
`	O
testing.assert_equal	B-API
`	O
approach	O
is	O
almost	O
good	O
,	O
except	O
that	O
it	O
presumably	O
fails	O
if	O
`	O
__debug__	O
`	O
is	O
False	O
!	O

I've	O
just	O
checked	O
and	O
found	O
out	O
that	O
my	O
implementation	O
is	O
about	O
2.x	O
times	O
*	O
faster	O
*	O
than	O
using	O
`	O
numpy.convolve	B-API
`	O
.	O

Not	O
as	O
concise	O
as	O
I	O
wanted	O
(	O
I	O
was	O
experimenting	O
with	O
`	O
mask_indices	B-API
`	O
)	O
,	O
but	O
this	O
will	O
also	O
do	O
the	O
work	O
:	O
#CODE	O

The	O
documentation	O
of	O
`	O
numpy.nonzero()	B-API
`	O
describes	O
how	O
its	O
result	O
must	O
be	O
interpreted	O
.	O

scikits-learn	O
pca	B-API
dimension	O
reduction	O
issue	O

`	O
np.mean	B-API
`	O
can	O
also	O
preserve	O
dimensions	O
if	O
needed	O
.	O

Are	O
there	O
alternatives	O
to	O
do	O
the	O
sorts	O
of	O
things	O
`	O
einsum	B-API
`	O
can	O
do	O
with	O
sparse	O
matrices	O
?	O

Your	O
immediate	O
problem	O
is	O
`	O
numpy.putmask	B-API
`	O
.	O

why	O
not	O
`	O
np.array	B-API
([	O
o.value1	O
for	O
o	O
in	O
objects	O
])`	O
?	O

In	O
a	O
comment	O
to	O
`	O
@USER	O
`	O
s	O
answer	O
I	O
suggested	O
`	O
np.delete	B-API
`	O
.	O

I	O
have	O
a	O
`	O
numpy.ndarray	B-API
`	O
.	O

I	O
believe	O
it	O
comes	O
down	O
to	O
the	O
fact	O
that	O
Python	O
calls	O
a	O
`	O
__getitem__	B-API
`	O
on	O
your	O
objects	O
and	O
treats	O
the	O
entire	O
block	O
of	O
code	O
of	O
`	O
for	O
`	O
loop	O
as	O
an	O
inline	O
statement	O
.	O

In	O
the	O
Notes	O
section	O
to	O
column_stack	B-API
,	O
it	O
points	O
out	O
this	O
:	O

`	O
logical_or	B-API
(	O
a	O
,	O
logical_or	B-API
(	O
b	O
,	O
c	O
))`	O

How	O
about	O
reading	O
them	O
in	O
correctly	O
as	O
numpy.datetime64	O
objects	O
using	O
numpy.loadtxt	B-API
(	O
they	O
are	O
coming	O
from	O
a	O
csv	O
file	O
)	O
?	O

Also	O
-	O
I	O
see	O
that	O
np.getfromtxt()	O
has	O
a	O
'	O
dtype	B-API
'	O
option	O
which	O
allows	O
the	O
user	O
to	O
specify	O
the	O
datatype	O
of	O
each	O
column	O
.	O

No	O
worries	O
,	O
the	O
dtype	B-API
is	O
inferred	O
as	O
`	O
int64	O
`	O
unless	O
you	O
pass	O
it	O
explicitly	O

whats	O
the	O
result	O
of	O
`	O
print	O
a	O
`	O
after	O
`	O
a	O
=	O
np.loadtxt	B-API
`	O

Keep	O
in	O
mind	O
that	O
`	O
np.cov	B-API
`	O
is	O
basically	O
doing	O
`	O
data.dot	O
(	O
data.T	O
)`	O
.	O

If	O
you	O
want	O
to	O
vectorize	B-API
operations	O
,	O
you	O
need	O
to	O
think	O
in	O
terms	O
of	O
these	O
higher	O
dimensional	O
arrays	O
.	O

Does	O
`	O
s2	O
=	O
pd.Series	B-API
(	O
s	O
,	O
dtype	B-API
=o	O
bject	O
)`	O
work	O
?	O

`	O
PyArray_DATA	B-API
`	O
is	O
defined	O
in	O

`	O
a	O
[:	O
,	O
:	O
,	O
5	O
]	O
.shape	B-API
=	O
(	O
10	O
,	O
10	O
,	O
1	O
)`	O

Can	O
you	O
print	O
`	O
datas	O
[	O
0	O
]	O
.shape	B-API
`	O
?	O

actually	O
used	O
is	O
this	O
line	O
within	O
the	O
definition	O
for	O
`	O
np.array_repr	B-API
`	O

That's	O
why	O
`	O
dstack	B-API
`	O
behaves	O
the	O
way	O
it	O
does	O
.	O

>>>	O
x	O
=	O
np.asanyarray	B-API
(	O
[	O
]	O
,	O
dtype=	O
'	O
float64	O
')	O

This	O
doesn't	O
work	O
for	O
floating	O
point	O
types	O
(	O
it	O
will	O
not	O
consider	O
+	O
0.0	O
and	O
-	O
0.0	O
the	O
same	O
value	O
)	O
,	O
and	O
`	O
np.intersect1d	B-API
`	O
uses	O
sorting	O
,	O
so	O
it	O
is	O
has	O
linearithmic	O
,	O
not	O
linear	O
,	O
performance	O
.	O

But	O
off	O
course	O
,	O
isreal	B-API
would	O
be	O
more	O
readable	O
:-)	O

mshgrd	O
=	O
ax.pcolormesh	B-API
(	O
X	O
,	O
Y	O
,	O
Z	O
)	O

Otherwise	O
,	O
the	O
performance	O
advantages	O
of	O
using	O
numpy	O
are	O
quickly	O
nullified	O
,	O
regardless	O
of	O
how	O
you	O
implement	O
your	O
ringbuffer	B-API
.	O

The	O
answer	O
is	O
numpy.clip	B-API
#CODE	O

Can	O
you	O
please	O
go	O
into	O
more	O
depth	O
about	O
nesting	O
a	O
recarray	B-API
in	O
another	O
by	O
using	O
the	O
np.object	O
method	O
?	O

Why	O
do	O
you	O
need	O
`	O
vectorize	B-API
`	O
for	O
that	O
?	O

I	O
did	O
try	O
gc.colletc()	O
without	O
success	O
but	O
adding	O
a	O
clf()	B-API
inside	O
the	O
loop	O
does	O
the	O
trick	O
!	O

not	O
a	O
bad	O
solution	O
;	O
though	O
I	O
am	O
somewhat	O
wary	O
of	O
the	O
performance	O
of	O
random.shuffle	B-API
.	O

date2num	B-API
,	O
ValueError	O
:	O
ordinal	O
must	O
be	O
>	O
=	O
1	O

And	O
you	O
could	O
override	O
`	O
__mul__	B-API
`	O
,	O
`	O
__add__	B-API
`	O
,	O
`	O
__sub__	B-API
`	O
accordingly	O
,	O
but	O
I	O
don't	O
know	O
exactly	O
how	O
numpy-like	O
you	O
actually	O
*	O
need	O
*	O
this	O
to	O
be	O
,	O
so	O
I	O
can't	O
say	O
for	O
sure	O
.	O

`	O
np.array	B-API
=	O
partial	O
(	O
np.array	B-API
,	O
dtype=	O
np.float32	O
)`	O
with	O
`	O
partial	O
`	O
from	O
the	O
`	O
functools	O
`	O
module	O
.	O

A	O
solution	O
that	O
worked	O
uses	O
griddata	B-API
.	O

Numpy	O
1.7.0	O
assert_array_almost_equal	B-API
documentation	O

You	O
can	O
read	O
matlab	O
(	O
.mat	B-API
)	O
files	O
in	O
Python	O
,	O
try	O
this	O
:	O
#CODE	O

Thanks	O
for	O
the	O
idea	O
of	O
genfromtxt()	B-API
.	O

If	O
you	O
are	O
using	O
numpy	O
,	O
for	O
multidimensional	O
lists	O
`	O
numpy.repeat	B-API
`	O
is	O
your	O
best	O
bet	O
.	O

If	O
the	O
following	O
equation	O
is	O
element-wise	O
True	O
,	O
then	O
allclose	B-API
returns	O
`	O
True	O
`	O
:	O
#CODE	O

`	O
np.vstack	B-API
`	O
just	O
vertically	O
stacks	O
the	O
arrays	O
you	O
pass	O
to	O
it	O
,	O
and	O
so	O
something	O
else	O
in	O
your	O
code	O
may	O
be	O
cutting	O
off	O
the	O
rest	O
of	O
the	O
results	O
inadvertently	O
.	O

If	O
you	O
move	O
the	O
line	O
`	O
np_verticies=	O
np.array	B-API
(	O
verticies	O
)`	O
outside	O
of	O
`	O
Fnumpy	O
`	O
and	O
the	O
timed	O
section	O
your	O
results	O
will	O
be	O
very	O
different	O
:	O
#CODE	O

`	O
fromiter	B-API
`'	O
s	O
example	O
is	O
essentially	O
this	O
:	O
`	O
np.fromiter	B-API
((	O
x*x	O
for	O
x	O
in	O
range	O
(	O
5	O
))	O
,	O
int	O
)`	O
.	O

In	O
Python	O
,	O
I	O
have	O
a	O
numpy.array	B-API
of	O
integers	O
`	O
[	O
2	O
,	O
4	O
,	O
7	O
,	O
8	O
,	O
9	O
,	O
10	O
,	O
15	O
,	O
10	O
8]	O
`	O
.	O

I	O
will	O
go	O
with	O
newaxis	B-API
then	O
.	O

pcolormesh	B-API
returns	O
a	O
QuadMesh	B-API
.	O

And	O
when	O
I	O
call	O
each	O
of	O
the	O
instructions	O
inside	O
f()	B-API
individually	O
it	O
gives	O
me	O
an	O
other	O
result	O
(	O
which	O
is	O
correct	O
):	O
#CODE	O

Here's	O
one	O
vectorized	O
approach	O
based	O
on	O
`	O
np.einsum	B-API
`	O
-	O
#CODE	O

What's	O
the	O
`	O
dtype	B-API
`	O
of	O
these	O
arrays	O
?	O

try	O
adding	O
a	O
`	O
show()	B-API
`	O
in	O
the	O
end	O

`	O
pandas.DataFrame	B-API
`	O

You	O
can	O
define	O
your	O
own	O
types	O
by	O
creating	O
a	O
class	O
and	O
writing	O
a	O
`	O
__add__	B-API
`	O
or	O
`	O
__sub__	B-API
`	O
method	O
.	O

On	O
the	O
other	O
hand	O
,	O
if	O
I	O
did	O
with	O
`	O
genfromtxt	B-API
`	O
,	O
the	O
third	O
column	O
is	O
problem	O
because	O
it	O
includes	O
comma	O
inside	O
double-quota	O
.	O

Apparently	O
,	O
if	O
there	O
is	O
no	O
'	O
missing_value	O
'	O
attribute	O
Netcdf4	O
defaults	O
to	O
a	O
missing	O
value	O
appropriate	O
for	O
the	O
dtype	B-API
.	O

do	O
be	O
aware	O
that	O
if	O
you	O
have	O
NaNs	O
,	O
there	O
is	O
an	O
equivalent	O
np.nanstd	B-API
with	O
the	O
similar	O
ddof	O
options	O

[	O
True	O
,	O
True	O
]]	O
,	O
dtype	B-API
=b	O
ool	O
)`	O

@USER	O
true	O
,	O
although	O
`	O
np.array	B-API
([	O
x	O
for	O
bb	O
in	O
b	O
for	O
x	O
in	O
bb	O
])`	O
will	O
do	O
the	O
job	O
.	O

return	O
matrix_power	B-API
(	O
self	O
,	O
other	O
)	O

vector	O
=	O
numpy.array	B-API
(	O
vector	O
);	O

If	O
so	O
then	O
you	O
should	O
have	O
no	O
problem	O
fitting	O
the	O
`	O
numpy.fft.rfftfreq	B-API
`	O
method	O
into	O
your	O
own	O
code	O
.	O

This	O
fails	O
:	O
`	O
einsum	B-API
(	O
'	O
i	O
...,	O
i	O
...	O

Python	O
&	O
Numpy	O
-	O
create	O
dynamic	O
,	O
arbitrary	O
subsets	O
of	O
ndarray	B-API

For	O
example	O
,	O
I	O
have	O
a	O
`	O
ndarray	B-API
`	O
that	O
is	O
:	O
#CODE	O

I	O
went	O
with	O
the	O
np.memmap	B-API
because	O
the	O
performance	O
is	O
similar	O
to	O
hdf5	O
and	O
I	O
already	O
have	O
numpy	O
in	O
production	O
.	O

its	O
np.log	B-API
not	O
m.log	O

what	O
happens	O
if	O
you	O
[	O
`	O
Py_INCREF	O
(	O
self	O
)`]	O
(	O
#URL	O
)	O
after	O
`	O
.base	B-API
`	O
assignment	O
?	O

File	O
"	O
/	O
usr	O
/	O
lib64	O
/	O
python2.6	O
/	O
site-packages	O
/	O
numpy	O
/	O
core	O
/	O
fromnumeric.py	O
"	O
,	O
line	O
806	O
,	O
in	O
searchsorted	B-API

why	O
isn't	O
the	O
`	O
ndarray	B-API
`	O
constructor	O
mentioned	O
here	O
?	O

>>>	O
z	O
=	O
numpy.array	B-API
([	O
1	O
,	O
2	O
]	O

Is	O
there	O
an	O
equivelent	O
to	O
`	O
fseek	O
`	O
when	O
using	O
`	O
fromfile	B-API
`	O
to	O
skip	O
the	O
beginning	O
of	O
the	O
file	O
?	O

The	O
linear	O
algebra	O
functions	O
are	O
generally	O
grouped	O
in	O
`	O
numpy.linalg	B-API
`	O
.	O

np.mean	B-API
:	O
#CODE	O

As	O
others	O
have	O
said	O
,	O
32-bit	O
versions	O
of	O
numpy	O
still	O
support	O
64-bit	O
dtypes	B-API
.	O

`	O
vstack	B-API
`	O
is	O
coercing	O
the	O
type	O
of	O
`	O
d	O
`	O
to	O
the	O
type	O
of	O
`	O
e	O
`	O
.	O

`	O
df.plot	B-API
`	O
returns	O
an	O
AxesSubplot	O
,	O
which	O
has	O
a	O
`	O
axvspan	B-API
`	O
method	O
.	O

With	O
the	O
variables	O
defined	O
above	O
,	O
`	O
np.searchsorted	B-API
(	O
lat	O
,	O
x	O
)`	O
is	O
16x	O
faster	O
than	O
the	O
equivalent	O
call	O
`	O
np.nanargmin	B-API
((	O
lat-x	O
)	O
**2	O
)`	O
on	O
my	O
computer	O
.	O

Pypy	O
with	O
iterators	O
is	O
still	O
solving	O
this	O
about	O
3x	O
faster	O
than	O
CPython	O
+	O
Numpy	O
,	O
even	O
when	O
using	O
`	O
np.searchsorted	B-API
`	O
(	O
see	O
my	O
solution	O
)	O
.	O

and	O
`	O
hstack	B-API
((	O
a	O
,	O
z	O
))`	O
?	O

Have	O
you	O
tried	O
passing	O
`	O
interpolation=	O
'	O
nearest	O
'`	O
to	O
`	O
imshow	B-API
`	O
?	O

`	O
cumsum	B-API
`	O
might	O
not	O
be	O
the	O
best	O
example	O
.	O

I	O
think	O
you're	O
after	O
`	O
plt.axis	B-API
([	O
xmin	O
,	O
xmax	O
,	O
ymin	O
,	O
ymax	O
])`	O
:	O
#CODE	O

Is	O
`	O
(	O
dry	O
,	O
unrch	O
)	O
=	O
((	O
G	O
==	O
3	O
)	O
.sum()	B-API
,	O
(	O
G	O
==	O
1	O
)	O
.sum()	B-API
)`	O
more	O
vectorized	O
?	O

Then	O
,	O
`	O
np.array	B-API
(	O
np.matrix	B-API
(	O
s.strip	O
(	O
'	O
[	O
]')))`	O
will	O
do	O
the	O
same	O
magic	O
.	O

I'm	O
trying	O
to	O
vectorize	B-API
Z	O
,	O
but	O
I'm	O
finding	O
it	O
rather	O
difficult	O
for	O
a	O
triple	O
for	O
loop	O
.	O

How	O
would	O
that	O
be	O
done	O
using	O
np.dot	B-API
?	O

I	O
was	O
surprised	O
how	O
descending	O
sorting	O
of	O
np.array	B-API
seem	O
so	O
un-pythonic	O
.	O

`	O
numpy.genfromtxt	B-API
`	O
accepts	O
generators	O
,	O
so	O
you	O
can	O
chain	O
`	O
genfromtext	O
`	O
and	O
`	O
ifilter	O
`	O
:	O
#CODE	O

I	O
also	O
tried	O
`	O
df.query()	B-API
`	O
,	O
but	O
no	O
much	O
improvement	O
.	O

According	O
to	O
the	O
documentation	O
(	O
e.g.	O
,	O
here	O
)	O
,	O
`	O
PyArray_SimpleNew	B-API
`	O
has	O
a	O
return	O
of	O
type	O
`	O
PyObject	O
*	O
`	O
and	O
thus	O
the	O
above	O
should	O
be	O
perfectly	O
fine	O
.	O

Edit	O
:	O
`	O
np.where	B-API
`	O
is	O
optional	O
,	O
thanks	O
@USER	O
.	O

or	O
with	O
`	O
numpy.concatenate	B-API
`	O
?	O

@USER	O
you	O
can	O
do	O
it	O
,	O
it's	O
easy	O
with	O
`	O
np.histogram	B-API
`	O
.	O

`	O
numpy.base_repr	B-API
`	O
uses	O
this	O
,	O
but	O
only	O
operates	O
on	O
scalars	O
.	O

Python	O
apply_along_axis	B-API
of	O
multiple	O
arrays	O

`	O
numpy.average()	B-API
`	O
has	O
a	O
weights	O
option	O
,	O
but	O
`	O
numpy.std()	B-API
`	O
does	O
not	O
.	O

I	O
wanted	O
to	O
write	O
`	O
M.det()	O
`	O
instead	O
of	O
`	O
numpy.linalg.det	B-API
(	O
M	O
)`	O
,	O

I	O
was	O
working	O
with	O
something	O
like	O
```	O
s	O
=	O
pd.DataFrame	B-API
([	O
'	O
1	O
'	O
,	O
'	O
na	O
'	O
,	O
'	O
3	O
'	O
,	O
'	O
4	O
'])	O
.	O

tested	O
it	O
a	O
bit	O
myself	O
:	O
sympy.sin	O
is	O
much	O
slower	O
than	O
numpy.sin	B-API

I	O
was	O
thinking	O
of	O
something	O
like	O
`	O
frombuffer	B-API
`	O
.	O

Tables	O
have	O
an	O
append	B-API
method	O
that	O
can	O
easily	O
add	O
additional	O
rows	O
.	O

@USER	O
,	O
I	O
tested	O
the	O
append	B-API
method	O
of	O
array	O
by	O
measure	O
the	O
time	O
it	O
cost	O
,	O
since	O
resize	B-API
the	O
array	O
will	O
use	O
more	O
time	O
.	O

I	O
think	O
you	O
need	O
to	O
use	O
append	B-API
function	O
to	O
append	B-API
new	O
array	O
with	O
previous	O
array	O
,	O
asarray	B-API
function	O
converts	O
input	O
to	O
array	O
.	O

I	O
would	O
store	O
all	O
your	O
data	O
in	O
a	O
python	O
list	O
and	O
use	O
the	O
append	B-API
function	O
to	O
add	O
new	O
measurement	O
.	O

The	O
Series	O
must	O
also	O
have	O
a	O
`	O
name	O
`	O
to	O
be	O
used	O
with	O
`	O
join	B-API
`	O
,	O
which	O
gets	O
pulled	O
in	O
as	O
a	O
new	O
field	O
called	O
`	O
name	O
`	O
.	O

NumPy	O
by	O
itself	O
is	O
a	O
fairly	O
low-level	O
tool	O
,	O
and	O
will	O
be	O
very	O
much	O
similar	O
to	O
using	O
MATLAB	O
.	O
pandas	O
on	O
the	O
other	O
hand	O
provides	O
rich	O
time	O
series	O
functionality	O
,	O
data	O
alignment	O
,	O
NA-friendly	O
statistics	O
,	O
groupby	B-API
,	O
merge	O
and	O
join	B-API
methods	O
,	O
and	O
lots	O
of	O
other	O
conveniences	O
.	O

already	O
have	O
all	O
header	O
names	O
listed	O
then	O
you	O
can	O
use	O
"	O
join	B-API
"	O
and	O

Because	O
these	O
are	O
irregular	O
operations	O
,	O
I	O
can't	O
use	O
merge	B-API
/	O
join	B-API
.	O

It	O
should	O
be	O
a	O
bit	O
quicker	O
just	O
by	O
not	O
using	O
so	O
many	O
intermediary	O
bitstrings	O
-	O
it's	O
all	O
done	O
in	O
the	O
join	B-API
method	O
.	O

How	O
can	O
i	O
use	O
the	O
unique	B-API
(	O
a	O
,	O
rows	O
)	O
from	O
MATLab	O
at	O
python	O
?	O

If	O
you	O
convert	O
your	O
2D	O
coordinates	O
into	O
`	O
target_map	O
`	O
into	O
flat	O
indices	O
into	O
it	O
using	O
`	O
np.ravel_multi_index	B-API
`	O
,	O
you	O
can	O
use	O
`	O
np.unique	B-API
`	O
and	O
`	O
np.bincount	B-API
`	O
to	O
speed	O
things	O
up	O
quite	O
a	O
bit	O
:	O
#CODE	O

To	O
get	O
the	O
unobserved	O
values	O
filled	O
,	O
we'll	O
use	O
the	O
`	O
unstack	B-API
`	O
and	O
`	O
stack	B-API
`	O
methods	O
.	O

The	O
actual	O
RMS	O
would	O
be	O
`	O
norm	B-API
(	O
x	O
)	O
/	O
sqrt	B-API
(	O
x.size	O
)`	O
,	O
but	O
for	O
minimization	O
the	O
constant	O
multiplier	O
doesn't	O
make	O
any	O
difference	O
.	O

You	O
could	O
check	O
whether	O
this	O
is	O
the	O
case	O
,	O
and	O
append	O
to	O
your	O
sample	O
in	O
a	O
loop	O
if	O
necessary	O
.	O

The	O
above	O
code	O
works	O
,	O
but	O
would	O
be	O
better	O
to	O
append	O
the	O
data_array	O
retrieved	O
from	O
for	O
loop	O
directly	O
into	O
the	O
numpy	O
array	O
rather	O
than	O
using	O
python	O
list	O
.	O

If	O
you	O
want	O
an	O
intersection	O
between	O
the	O
two	O
arrays	O
you	O
can	O
loop	O
;	O
for	O
i	O
data	O
:	O
and	O
get	O
i	O
from	O
first	O
array	O
,	O
and	O
i	O
from	O
second	O
array.But	O
I'm	O
not	O
sure	O
if	O
I	O
follow	O
it	O
correctly	O
,	O
you	O
have	O
some	O
data	O
which	O
has	O
0	O
occurences	O
in	O
some	O
columns	O
of	O
your	O
array	O
,	O
if	O
you	O
append	O
the	O
other	O
values	O
to	O
a	O
new	O
array	O
the	O
memory	O
of	O
where	O
in	O
the	O
data	O
those	O
values	O
came	O
from	O
is	O
already	O
automatically	O
stored	O
.	O

(	O
2	O
)	O
Collect	O
the	O
parts	O
of	O
the	O
SQL	O
command	O
in	O
a	O
list	O
and	O
do	O
a	O
`	O
str.join	B-API
`	O
in	O
the	O
end	O
,	O
to	O
avoid	O
allocating	O
an	O
increasingly	O
long	O
string	O
each	O
time	O
(	O
you	O
can't	O
really	O
append	O
to	O
a	O
string	O
in	O
Python	O
as	O
they	O
are	O
immutable	O
)	O
.	O

append	O
C	O
:\	O
libav\usr\bin\	O
to	O
the	O
'	O
Path	O
'	O
environment	O
variable	O

How	O
to	O
efficiently	O
join	O
a	O
list	O
of	O
values	O
to	O
a	O
list	O
of	O
intervals	O
?	O

The	O
alignment	O
angles	O
may	O
be	O
unique	O
,	O
a	O
discrete	O
set	O
,	O
or	O
a	O
continuum	O
as	O
below	O
.	O

Alternatively	O
,	O
you	O
could	O
assign	O
the	O
flip	O
cards	O
their	O
own	O
unique	O
IDs	O
and	O
store	O
them	O
in	O
the	O
same	O
record	O
array	O
as	O
the	O
regular	O
cards	O
,	O
as	O
the	O
properties	O
seem	O
to	O
have	O
the	O
same	O
names	O
,	O
and	O
then	O
have	O
a	O
`	O
flip_id	O
`	O
field	O
that	O
would	O
be	O
some	O
set	O
value	O
such	O
as	O
`	O
0	O
`	O
or	O
`	O
None	O
`	O
for	O
cards	O
without	O
flip	O
aspects	O
and	O
then	O
the	O
ID	O
of	O
the	O
flip	O
card	O
for	O
those	O
cards	O
that	O
do	O
have	O
a	O
flip	O
.	O

You	O
can	O
notice	O
that	O
there	O
is	O
gray	O
strip	O
on	O
top	O
and	O
on	O
the	O
left	O
...	O

Furthermore	O
you	O
then	O
go	O
and	O
compute	O
the	O
norm	O
of	O
those	O
three	O
values	O
.	O

My	O
question	O
concerns	O
iterating	O
through	O
the	O
rows	O
of	O
a	O
data	O
frame	O
and	O
on	O
each	O
row	O
setting	O
a	O
field	O
based	O
on	O
information	O
in	O
a	O
different	O
data	O
frame	O
.	O

EDIT	O
:	O
Adding	O
logic	O
to	O
default	O
empty	O
strings	O
to	O
`	O
0	O
`	O
,	O
use	O
a	O
different	O
value	O
if	O
you	O
want	O
to	O
handle	O
empty	O
strings	O
in	O
`	O
years	O
`	O
colomn	O
differently	O
#CODE	O

I	O
would	O
suggest	O
that	O
you	O
use	O
2-dimensional	O
numpy	O
array	O
.	O

I	O
renamed	O
them	O
to	O
aa	O
,	O
ab	O
and	O
ac	O
but	O
still	O
get	O
the	O
same	O
error	O
.	O

In	O
this	O
last	O
case	O
,	O
RAM	O
usage	O
fits	O
the	O
equivalent	O
`	O
chunk	O
`	O
size	O
#CODE	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

It	O
gave	O
me	O
the	O
error	O
:	O
cqid	O
=	O
row	O
[	O
'	O
ClearQuest	O
ID	O
']	O
TypeError	O
:	O
string	O
indices	O
must	O
be	O
integers	O
,	O
not	O
str	O
...........	O

The	O
use	O
case	O
is	O
that	O
I	O
have	O
different	O
time	O
series	O
coming	O
from	O
different	O
data	O
sources	O
.	O

How	O
can	O
I	O
get	O
pandas	O
Timestamp	O
offset	O
by	O
certain	O
amount	O
of	O
months	O
?	O

I	O
managed	O
to	O
get	O
the	O
stats	O
by	O
placing	O
everything	O
in	O
nested	O
dictionary	O
,	O
but	O
I	O
feel	O
that	O
there	O
may	O
be	O
a	O
much	O
easier	O
way	O
to	O
the	O
approach	O
by	O
using	O
pandas	O
dataframes	O
and	O
groubpy	O
.	O

Just	O
to	O
get	O
a	O
sense	O
of	O
what	O
I'm	O
trying	O
to	O
achieve	O
.	O

Which	O
is	O
suspect	O
is	O
due	O
to	O
my	O
data	O
range	O
,,	O
but	O
it	O
may	O
well	O
be	O
that	O
I	O
don't	O
understand	O
the	O
other	O
parameters	O
.	O

Your	O
second	O
one	O
doesn't	O
really	O
make	O
sense	O
as	O
an	O
aggregation	O
.	O

How	O
can	O
I	O
get	O
the	O
index	O
of	O
certain	O
element	O
of	O
a	O
Series	O
in	O
python	O
pandas	O
?	O

(	O
Very	O
,	O
very	O
late	O
reply	O
-	O
apologies	O
.	O
)	O
That's	O
true	O
,	O
you'd	O
use	O
the	O
method	O
EdChum	O
suggested	O
for	O
longer	O
lists	O
of	O
columns	O
.	O

If	O
actual_sum	O
and	O
expected_to_date	O
are	O
equal	O
,	O
put	O
a	O
0	O

`	O
ts	O
[	O
ts	O
[	O
'	O
values	O
']	O
0	O
]`	O
should	O
produce	O
the	O
output	O
you	O
are	O
looking	O
for	O
.	O

And	O
I	O
get	O
the	O
counts	O
:	O
#CODE	O

The	O
standard	O
deviation	O
differs	O
between	O
pandas	O
and	O
numpy	O
.	O

I	O
would	O
like	O
to	O
get	O
rid	O
of	O
the	O
loops	O
,	O
if	O
that	O
is	O
possible	O
.	O

If	O
I	O
change	O
the	O
names	O
then	O
there	O
is	O
nothing	O
to	O
reference	O
.	O

I	O
even	O
tried	O
building	O
from	O
the	O
git	O
,	O
but	O
whatever	O
I	O
seem	O
to	O
do	O
,	O
I	O
get	O
the	O
same	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
do	O
the	O
following	O
operations	O
on	O
the	O
data	O
storage	O
:	O

How	O
do	O
I	O
get	O
it	O
to	O
actually	O
show	O
the	O
graph	O
?	O

#URL	O
shows	O
a	O
way	O
to	O
get	O
the	O
number	O
of	O
days	O
in	O
a	O
month	O
,	O
making	O
the	O
rest	O
more	O
or	O
less	O
trivial	O
as	O
they	O
don't	O
vary	O
.	O

to	O
create	O
average	O
values	O
with	O
an	O
equidistant	O
time-vector	O
.	O

I	O
get	O
something	O
where	O
all	O
"	O
newlines	O
"	O
are	O
escaped	O
.	O

Reproducing	O
without	O
a	O
data	O
file	O
,	O
using	O
Jeff's	O
suggestion	O
:	O
#CODE	O

However	O
,	O
I	O
also	O
want	O
to	O
get	O
it	O
on	O
the	O
basis	O
of	O
the	O
`	O
Group	O
`	O
variable	O
,	O
which	O
means	O
I	O
don't	O
want	O
to	O
get	O
`	O
Bob	O
`'	O
s	O
`	O
Value	O
`	O
based	O
on	O
the	O
`	O
Jared	O
`'	O
s	O
`	O
Value	O
`	O
,	O
since	O
those	O
two	O
records's	O
`	O
Group	O
`	O
value	O
is	O
different	O
-	O
I	O
only	O
compute	O
it	O
within	O
each	O
specific	O
`	O
Group	O
`	O
variable	O
.	O

I	O
try	O
to	O
use	O
jsonlint	O
to	O
validate	O
these	O
json	O
files	O
but	O
encounter	O
some	O
error	O
messages	O
.	O

The	O
logic	O
to	O
arrive	O
at	O
that	O
database	O
is	O
an	O
intricate	O
mix	O
of	O
Python	O
processing	O
and	O
SQL	O
joins	O
done	O
in	O
sqlite3	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
the	O
`	O
str	O
`	O
accessor	O
to	O
split	O
the	O
data	O
into	O
two	O
columns	O
,	O
such	O
that	O
the	O
first	O
column	O
is	O
,	O
Name	O
,	O
contains	O
the	O
actual	O
name	O
(	O
first	O
name	O
last	O
name	O
)	O
,	O
and	O
the	O
second	O
column	O
,	O
Email	O
,	O
contains	O
the	O
email	O
address	O
)	O
.	O

In	O
fact	O
the	O
only	O
really	O
relevant	O
data	O
needed	O
for	O
the	O
plot	O
is	O
the	O
first	O
and	O
second	O
column	O
,	O
namely	O
:	O
`	O
Compression	O
Force	O
`	O
and	O
`	O
Compression	O
Velocity	O
`	O
.	O

How	O
to	O
get	O
special	O
characters	O
from	O
Excel	O
to	O
screen	O
using	O
pandas	O
?	O

And	O
replace	O
`'	O
Month	O
'`	O
with	O
`'	O
Day	O
'`	O
below	O
.	O

But	O
if	O
you	O
have	O
a	O
huge	O
amount	O
to	O
data	O
,	O
it	O
*	O
might	O
*	O
be	O
interesting	O
to	O
think	O
of	O
a	O
more	O
complex	O
data	O
model	O
.	O

What	O
are	O
you	O
trying	O
to	O
do	O
where	O
this	O
is	O
the	O
bottleneck	O
?	O

How	O
can	O
I	O
change	O
that	O
and	O
use	O
insted	O
the	O
first	O
line	O
of	O
output	O
code	O
as	O
a	O
column	O
(	O
In	O
this	O
case	O
line	O
10	O
:	O
Sub-Data	O
Item	O
...	O
)	O

My	O
objective	O
was	O
to	O
have	O
a	O
DTM	O
like	O
the	O
one	O
you	O
get	O
in	O
R	O
tm	O
.	O

So	O
right	O
now	O
all	O
the	O
data	O
comes	O
from	O
each	O
iteration	O
group	O
,	O
and	O
all	O
of	O
its	O
is	O
transformed	O
into	O
one	O
column	O
vector	O
.	O

I	O
am	O
trying	O
to	O
get	O
to	O
the	O
point	O
where	O
I	O
can	O
run	O
#CODE	O

While	O
I	O
don't	O
get	O
that	O
warning	O
with	O
#CODE	O

Thanks	O
@USER	O
-	O
I	O
mean	O
that	O
,	O
if	O
we	O
do	O
`	O
A-B	O
`	O
we	O
should	O
only	O
get	O
the	O
NaNs	O
in	O
`	O
A	O
`	O
,	O
and	O
not	O
the	O
NaNs	O
in	O
`	O
B	O
`	O
.	O

I	O
have	O
three	O
columns	O
in	O
my	O
data	O
set	O
,	O
namely	O
"	O
age	O
"	O
,	O
"	O
race	O
"	O
,	O
"	O
sex	O
"	O
,	O
that	O
I	O
care	O
about	O
.	O

Cannot	O
get	O
the	O
average	O
date	O
using	O
pandas	O

Any	O
suggestions	O
?	O

All	O
values	O
ought	O
to	O
be	O
integers	O
,	O
no	O
floats	O
.	O

Note	O
:	O
this	O
will	O
get	O
tripped	O
up	O
by	O
some	O
strings	O
,	O
so	O
use	O
with	O
caution	O
.	O

The	O
purpose	O
of	O
all	O
these	O
stuff	O
is	O
a	O
geographical	O
representation	O
of	O
data	O
on	O
a	O
spatial	O
grid	O
.	O

Since	O
Name	O
`	O
C	O
`	O
does	O
not	O
have	O
`	O
3	O
`	O
or	O
`	O
5	O
`	O
in	O
the	O
column	O
`	O
Activity	O
`	O
,	O
I	O
do	O
not	O
want	O
to	O
get	O
this	O
data	O
frame	O
.	O

Data	O
has	O
to	O
be	O
collected	O
before	O
local	O
data	O
frame	O
is	O
created	O
.	O

PANDAS	O
:	O
Extracting	O
values	O
from	O
a	O
column	O
by	O
applying	O
a	O
condition	O
on	O
other	O
columnns	O

If	O
you	O
try	O
to	O
produce	O
the	O
groups	O
from	O
my	O
example	O
you'll	O
see	O
what	O
I	O
mean	O
.	O

`	O
pandas	O
`	O
,	O
like	O
`	O
numpy	O
`	O
and	O
many	O
other	O
modules	O
,	O
is	O
not	O
written	O
in	O
pure	O
Python	O
-	O
it	O
has	O
components	O
written	O
in	O
C	O
and	O
Cython	O
that	O
get	O
compiled	O
into	O
version-	O
and	O
platform-specific	O
libraries	O
during	O
the	O
build	O
process	O
.	O

Not	O
sure	O
how	O
to	O
get	O
around	O
this	O
...	O
pretty	O
new	O
to	O
pandas	O
.	O

Here's	O
the	O
product	O
:	O
#CODE	O

However	O
,	O
as	O
the	O
data	O
became	O
large	O
,	O
we	O
played	O
with	O
SQLAlchemy	O
/	O
SQLite3	O
.	O

But	O
this	O
time	O
I	O
get	O
another	O
error	O
:	O
#CODE	O

Makes	O
the	O
change	O
the	O
idea	O
of	O
trying	O
to	O
use	O
this	O
approach	O
all	O
together	O
.	O

For	O
a	O
generalized	O
scenario	O
where	O
there	O
are	O
many	O
different	O
combinations	O
of	O
values	O
under	O
'	O
COL1	O
'	O
and	O
'	O
COL3	O
'	O
,	O
this	O
works	O
but	O
is	O
probably	O
not	O
nearly	O
as	O
efficient	O
as	O
it	O
can	O
be	O
:	O
#CODE	O

Similarly	O
in	O
your	O
example	O
where	O
you	O
plot	O
`	O
col1	O
,	O
col2	O
`	O
differently	O
based	O
on	O
`	O
col3	O
`	O
,	O
what	O
if	O
there	O
are	O
NA	O
values	O
that	O
break	O
the	O
association	O
between	O
`	O
col1	O
,	O
col2	O
,	O
col3	O
`	O
?	O

For	O
example	O
,	O
I	O
want	O
to	O
take	O
values	O
from	O
`	O
col_3	O
`	O
and	O
`	O
col_4	O
`	O
and	O
use	O
them	O
to	O
generate	O
a	O
single	O
values	O
.	O

The	O
speed	O
difference	O
is	O
astonishing	O
.	O

The	O
summation	O
in	O
one	O
group	O
won't	O
reduce	O
the	O
size	O
of	O
the	O
result	O
,	O
the	O
summation	O
I	O
want	O
to	O
do	O
is	O
across	O
different	O
groups	O
.	O

If	O
you	O
really	O
prefer	O
`	O
1	O
`'	O
s	O
and	O
`	O
0	O
`'	O
s	O
replace	O
the	O
last	O
line	O
with	O
:	O
#CODE	O

So	O
traverse	O
the	O
data	O
once	O
and	O
generate	O
both	O
arrays	O
would	O
be	O
preferred	O
.	O

Im	O
not	O
fully	O
adjusted	O
to	O
how	O
Pandas	O
is	O
using	O
matplotlib	O
so	O
i	O
often	O
switch	O
to	O
matplotlib	O
myself	O
if	O
plots	O
get	O
more	O
complicated	O
,	O
eg	O
:	O
#CODE	O

The	O
table	O
that	O
gives	O
this	O
message	O
contains	O
a	O
few	O
columns	O
,	O
none	O
of	O
them	O
have	O
data	O
in	O
them	O
.	O

so	O
yes	O
later	O
i	O
have	O
open	O
the	O
file	O
but	O
thanks	O
to	O
pandas	O
i	O
can	O
use	O
the	O
`	O
chunksize	O
`	O
command	O
to	O
get	O
the	O
information	O
i	O
need	O
.	O

create	O
column	O
names	O
by	O
joining	O
two	O
labels	O
of	O
different	O
levels	O
with	O
pandas	O

@USER	O
so	O
how	O
should	O
i	O
write	O
it	O
so	O
that	O
the	O
program	O
gives	O
seq	O
to	O
'	O
Hsequence	O
'	O
column	O
when	O
'	O
Hcolumn	O
'	O
contains	O
the	O
title	O
from	O
fasta	O
file	O
?	O

Also	O
,	O
in	O
my	O
larger	O
directory	O
,	O
this	O
is	O
taking	O
forever	O
-	O
as	O
in	O
,	O
about	O
a	O
gig	O
of	O
CSVs	O
is	O
timing	O
out	O
for	O
me	O
(	O
by	O
my	O
hand	O
)	O
at	O
around	O
20	O
minutes	O
.	O

The	O
key	O
was	O
unstacking	O
the	O
data	O
first	O
:	O
#CODE	O

I	O
want	O
to	O
get	O
the	O
latitude	O
and	O
longitude	O
coordinates	O
for	O
any	O
one	O
of	O
the	O
columns	O
in	O
the	O
data	O
frame	O
below	O
.	O

Option	O
values	O
are	O
restored	O
automatically	O
when	O
you	O
exit	O
the	O
`	O
with	O
`	O
block	O
.	O

I	O
am	O
finding	O
difficulty	O
to	O
plot	O
reason	O
every	O
csv	O
file	O
starts	O
with	O
different	O
date	O
,	O
that's	O
the	O
reason	O
I	O
was	O
trying	O
to	O
convert	O
into	O
no	O
.	O
of	O
days	O
,	O
so	O
that	O
I	O
can	O
plot	O
all	O
in	O
one	O
go	O
with	O
starting	O
day	O
-	O
1	O
,	O
for	O
example	O
:	O
-	O
csv	O
file	O
2	O
fall	O
short	O
as	O
compared	O
to	O
csv	O
file	O
1	O
.	O

Most	O
of	O
the	O
time	O
you	O
can	O
get	O
away	O
with	O
using	O
something	O
else	O
...	O

In	O
that	O
case	O
the	O
index	O
is	O
composed	O
of	O
integers	O
from	O
0	O
to	O
n	O
:	O
#CODE	O

You	O
have	O
a	O
difference	O
between	O
a	O
mac	O
and	O
a	O
pc	O
,	O
and	O
*	O
presumably	O
*	O
the	O
same	O
code	O
.	O

Suppose	O
you	O
want	O
to	O
find	O
the	O
row	O
or	O
rows	O
where	O
`	O
beef	O
`	O
production	O
was	O
the	O
highest	O
.	O

The	O
number	O
of	O
columns	O
may	O
differ	O
and	O
so	O
does	O
the	O
column	O
names	O
.	O

How	O
do	O
I	O
avoid	O
that	O
and	O
rather	O
generate	O
it	O
in	O
a	O
sparse	O
matrix	O
CSR	O
format	O
?	O

I	O
download	O
and	O
scrape	O
a	O
webpage	O
for	O
some	O
data	O
in	O
TSV	O
format	O
.	O

You	O
can	O
set	O
parameter	O
`	O
labels=False	O
`	O
to	O
get	O
the	O
integer	O
representation	O
#CODE	O

it's	O
not	O
too	O
much	O
of	O
a	O
stretch	O
to	O
insert	O
NaN's	O
into	O
the	O
data	O
using	O
reindexing	O
so	O
that	O
i	O
get	O
this	O
:	O
#CODE	O

Any	O
suggestions	O
?	O

Data-driven	O
DOM	O
manipulation	O
(	O
maybe	O
the	O
hardest	O
thing	O
to	O
wrap	O
one's	O
head	O
around	O
):	O
your	O
data	O
gets	O
transformed	O
into	O
DOM	O
elements	O
.	O

Your	O
regex	O
is	O
matching	O
on	O
all	O
`	O
-	O
`	O
characters	O
:	O
#CODE	O

1	O
)	O
create	O
additional	O
columns	O
with	O
clock	O
time	O
headings	O
for	O
5	O
minute	O
intervals	O
between	O
9:30	O
and	O
4:00	O
pm	O
,	O
so	O
the	O
headings	O
of	O
the	O
data	O
frame	O
look	O
like	O
:	O

`	O
Index	O
([	O
u'id	O
opinion	O
']	O
,	O
dtype=	O
'	O
object	O
')`	O
Thanks	O
for	O
the	O
response	O

The	O
end	O
product	O
would	O
be	O
ten	O
timeseries	O
plots	O
with	O
charted	O
lines	O
over	O
time	O
for	O
each	O
TID	O
.	O

And	O
get	O
the	O
result	O
:	O
#CODE	O

However	O
,	O
I	O
still	O
don't	O
get	O
why	O
`	O
iconv	O
`	O
messes	O
it	O
up	O
.	O

If	O
you	O
have	O
huge	O
CSV	O
data	O
,	O
NYSOL's	O
mcmd	O
is	O
the	O
best	O
.	O

I	O
get	O
#CODE	O

If	O
I	O
use	O
a	O
tweaked	O
version	O
of	O
@USER	O
'	O
s	O
suggestion	O
below	O
,	O
I	O
get	O
this	O
error	O
:	O
#CODE	O

ValueError	O
:	O
Unknown	O
format	O
code	O
'	O
f	O
'	O
for	O
object	O
of	O
type	O
'	O
str	O
'	O
-	O
why	O
do	O
I	O
get	O
this	O
the	O
second	O
time	O
but	O
not	O
the	O
first	O
time	O
?	O

Any	O
suggestion	O
about	O
the	O
reason	O
?	O

I	O
have	O
a	O
data	O
set	O
which	O
has	O
multiple	O
columns	O
,	O
strings	O
and	O
integers	O

is	O
the	O
condition	O
,	O
returning	O
a	O
booleans	O
array	O
of	O
True	O
/	O
False	O
for	O
all	O
values	O
meeting	O
the	O
condition	O
or	O
not	O
,	O
and	O
then	O
the	O
corresponding	O
A	O
values	O
are	O
selected	O

I	O
fixed	O
this	O
bug	O
in	O
0.11-dev	O
in	O
any	O
event	O
,	O
see	O
here	O
:	O
#URL	O
thanks	O
!	O

To	O
split	O
`	O
my_data2	O
`	O
into	O
two	O
arrays	O
of	O
roughly	O
equal	O
size	O
:	O
#CODE	O

to	O
get	O
a	O
`	O
Series	O
`	O
of	O
`	O
list	O
`	O
s	O
of	O
strings	O
.	O

For	O
example	O
,	O
you	O
can't	O
sum	O
a	O
mix	O
of	O
strings	O
and	O
floats	O
in	O
pandas	O
but	O
Excel	O
would	O
silently	O
drop	O
the	O
string	O
value	O
and	O
sum	O
the	O
floats	O
.	O

Notice	O
how	O
the	O
values	O
in	O
the	O
second	O
column	O
are	O
no	O
longer	O
integers	O
,	O
as	O
they	O
were	O
originally	O
.	O

I	O
have	O
a	O
large	O
but	O
very	O
sparse	O
matrix	O
(	O
50,000	O
rows*	O
100,000	O
columns	O
,	O
only	O
10%	O
of	O
the	O
values	O
are	O
known	O
)	O
.	O

In	O
python	O
normally	O
you	O
don't	O
need	O
and	O
you	O
shouldn't	O
use	O
a	O
semicolon	O
at	O
the	O
end	O
of	O
the	O
line	O
.	O

That's	O
all	O
data	O
python	O
is	O
reading	O
in	O
,	O
apparently	O
:	O
the	O
16	O
first	O
lines	O
,	O
or	O
at	O
least	O
I	O
am	O
not	O
able	O
to	O
get	O
the	O
rest	O
of	O
data	O
in	O
.	O

The	O
problem	O
is	O
to	O
find	O
average	O
values	O
of	O
temp1	O
,	O
temp2	O
and	O
temp3	O
for	O
a	O
period	O
of	O
time	O
(	O
say	O
,	O
2	O
days	O
)	O
over	O
the	O
same	O
intervals	O
(	O
for	O
that	O
example	O
-	O
15	O
minutes	O
)	O
.	O

In	O
generally	O
I	O
wonder	O
if	O
pandas	O
should	O
not	O
at	O
least	O
throw	O
a	O
warning	O
,	O
afterall	O
broadcasting	O
the	O
result	O
to	O
both	O
columns	O
should	O
be	O
almost	O
never	O
what	O
is	O
wanted	O
.	O

I	O
get	O
pandas	O
error	O
when	O
I	O
try	O
to	O
read	O
HDF5	O
format	O
files	O
that	O
I	O
have	O
created	O
with	O
h5py	O
.	O

Additionally	O
you	O
can	O
use	O
numpys	O
matrix	O
#CODE	O

I	O
updated	O
pandas	O
'	O
sudo	O
pip	O
install	O
--	O
upgrade	O
pandas	O
'	O
,	O
between	O
both	O
of	O
these	O
fixes	O
,	O
everything	O
worked	O
.	O

Sorry	O
can't	O
reproduce	O
nor	O
understand	O
your	O
real	O
problem	O
,	O
please	O
post	O
what	O
you	O
see	O
in	O
your	O
question	O

When	O
I	O
used	O
'	O
ethnicity	O
'	O
or	O
'	O
veteran	O
'	O
as	O
a	O
value	O
my	O
results	O
came	O
out	O
really	O
strange	O
and	O
didn't	O
match	O
my	O
value	O
counts	O
numbers	O
.	O

`	O
post_start	O
`	O
is	O
the	O
date	O
that	O
the	O
employee	O
started	O
in	O
the	O
post	O
,	O
and	O
`	O
change_date	O
`	O
is	O
the	O
date	O
that	O
the	O
post	O
title	O
was	O
changed	O
.	O

How	O
do	O
I	O
replace	O
the	O
ints	O
with	O
the	O
float	O
values	O
from	O
another	O
column	O
(	O
by	O
same	O
row	O
)	O
,	O
but	O
leave	O
all	O
the	O
nulls	O
?	O

There	O
may	O
be	O
a	O
more	O
foolproof	O
,	O
cleaner	O
way	O
of	O
computing	O
date	O
time	O
differences	O
in	O
pandas	O
.	O

However	O
,	O
to	O
get	O
the	O
row	O
sum	O
,	O
one	O
needs	O
to	O
specify	O
axis=1	O
.	O

Using	O
the	O
second	O
method	O
I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Filter	O
data	O
to	O
get	O
only	O
first	O
day	O
of	O
the	O
month	O
rows	O

(	O
FYI	O
if	O
i	O
insert	O
a	O
print	O
print	O
(	O
vals	O
)	O
in	O
the	O
middle	O
of	O
that	O
loop	O
,	O
it	O
prints	O
#CODE	O

For	O
days	O
in	O
a	O
month	O
(	O
'	O
2015-07	O
'	O
say	O
)	O
You	O
could	O
change	O
#CODE	O

Doesnt	O
the	O
frame	O
variable	O
get	O
overwritten	O
during	O
each	O
iteration	O
in	O
the	O
loop	O
?	O

Any	O
other	O
advice	O
I	O
can	O
leverage	O
in	O
the	O
meantime	O
?	O

If	O
`	O
Change	O
Closing	O
Date	O
`	O
is	O
True	O
,	O
I	O
would	O
like	O
to	O
add	O
`	O
Closing	O
Date2	O
`	O
column	O
into	O
my	O
new	O
column	O
with	O
adding	O
1	O
year	O
.	O

If	O
you	O
REALLY	O
want	O
to	O
get	O
by	O
a	O
group	O
individually	O
#CODE	O

but	O
I	O
get	O
the	O
error	O
:	O
#CODE	O

I	O
am	O
new	O
to	O
pandas	O
for	O
data	O
analysis	O
and	O
I	O
just	O
installed	O
pandas	O
with	O
required	O
dependencies	O
(	O
NumPy	O
,	O
python-dateutil	O
,	O
pytz	O
,	O
numexpr	O
,	O
bottleneck	O
and	O
matplotlib	O
)	O
.	O

What	O
do	O
you	O
get	O
if	O
you	O
print	O
that	O
?	O

Can't	O
you	O
use	O
sets	O
and	O
intersections	O
?	O

is	O
there	O
a	O
way	O
to	O
insert	O
`	O
s	O
`	O
into	O
`	O
df	O
`	O
without	O
creating	O
a	O
reindexed	O
copy	O
of	O
`	O
df	O
`	O
first	O
?	O

I'm	O
using	O
python	O
2.7.5	O
(	O
with	O
all	O
the	O
packages	O
in	O
the	O
python	O
(	O
x	O
,	O
y	O
)	O
bundle	O
)	O
,	O
and	O
running	O
files	O
from	O
the	O
command	O
prompt	O
.	O

Any	O
suggestions	O
??	O

This	O
will	O
never	O
get	O
the	O
similar	O
graph	O
as	O
the	O
kernel	O
estimate	O
base	O
of	O
the	O
original	O
data	O
,	O
result	O
:	O

The	O
working	O
version	O
I	O
have	O
is	O
this	O
one	O
,	O
but	O
I	O
feel	O
there	O
is	O
potential	O
for	O
improvement	O
,	O
as	O
I	O
find	O
my	O
solution	O
unreadable	O
and	O
I	O
am	O
unsure	O
about	O
how	O
it	O
would	O
generalize	O
to	O
multiindexes	O
#CODE	O

Also	O
,	O
once	O
you	O
get	O
to	O
15	O
points	O
,	O
you'll	O
be	O
able	O
to	O
upvote	O
as	O
well	O
.	O

You	O
can	O
then	O
get	O
the	O
last	O
first	O
value	O
by	O
forward	O
filling	O
`	O
first_values	O
`	O
,	O
reindexing	O
like	O
`	O
second_values	O
`	O
,	O
stacking	O
again	O
and	O
indexing	O
into	O
the	O
result	O
using	O
the	O
original	O
`'	O
time	O
'	O
,	O
'	O
second	O
'`	O
pairs	O
:	O
#CODE	O

how	O
do	O
i	O
avoid	O
creating	O
so	O
many	O
variables	O
as	O
I	O
add	O
columns	O
together	O
?	O

Any	O
suggestion	O
on	O
how	O
to	O
efficiently	O
achieve	O
this	O
?	O

I	O
get	O
:	O
#CODE	O

For	O
instance	O
,	O
I	O
can	O
compute	O
the	O
value	O
for	O
data	O
record	O
3	O
by	O
taking	O
`	O
len	O
(	O
set	O
([	O
4	O
,	O
4	O
,	O
6	O
,	O
12	O
]))`	O
which	O
gives	O
3	O
.	O

@USER	O
That's	O
a	O
great	O
suggestion	O
(	O
for	O
some	O
use-cases	O
)	O
it	O
should	O
be	O
its	O
own	O
answer	O
(	O
so	O
I	O
can	O
upvote	O
it	O
)	O
Though	O
it	O
does	O
need	O
tweak	O
to	O
multiply	O
by	O
100	O
.	O

python	O
how	O
to	O
sum	O
together	O
all	O
values	O
within	O
a	O
time	O
interval	O
in	O
datetime64	O
?	O

was	O
trying	O
to	O
do	O
a	O
"	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
results	O
))"	O
before	O
the	O
"	O
for	O
item	O
in	O
results	O
[	O
i	O
]"	O
that	O
you	O
did	O
but	O
not	O
working	O
for	O
me	O
...	O

But	O
,	O
on	O
the	O
other	O
hand	O
,	O
if	O
your	O
columns	O
aren't	O
in	O
the	O
same	O
order	O
,	O
then	O
my	O
suggestion	O
won't	O
work	O
.	O

When	O
I	O
execute	O
the	O
program	O
for	O
the	O
data	O
of	O
the	O
same	O
day	O
,	O
processor	O
time	O
becomes	O
long	O
from	O
the	O
same	O
point	O
.	O

I'm	O
new	O
to	O
pandas	O
,	O
python	O
,	O
and	O
scripting	O
in	O
general	O
,	O
so	O
am	O
still	O
getting	O
my	O
head	O
around	O
the	O
basics	O
.	O

You	O
can	O
,	O
for	O
example	O
,	O
use	O
interpolation	O
to	O
get	O
equally	O
spaced	O
datapoints	O
out	O
off	O
your	O
timeseries	O
.	O

What	O
I	O
was	O
hoping	O
for	O
was	O
to	O
add	O
up	O
all	O
of	O
the	O
frequencies	O
across	O
the	O
websites	O
and	O
to	O
create	O
two	O
columns	O
:	O
Column	O
A	O
with	O
the	O
word	O
,	O
and	O
Column	O
B	O
with	O
all	O
of	O
the	O
frequencies	O
added	O
together	O
.	O

It	O
does	O
not	O
work	O
without	O
dropping	O
index	O
.	O

Now	O
I	O
was	O
wondering	O
how	O
I	O
could	O
subtract	O
my	O
multi-year	O
timeseries	O
from	O
this	O
standard	O
year	O
,	O
in	O
order	O
to	O
get	O
a	O
timeseries	O
that	O
show	O
which	O
days	O
were	O
below	O
or	O
above	O
it's	O
standard	O
.	O

I	O
may	O
try	O
installing	O
an	O
older	O
version	O
to	O
find	O
out	O
what	O
was	O
actually	O
getting	O
calculated	O
.	O

Is	O
there	O
any	O
disadvantage	O
?	O

The	O
length	O
of	O
the	O
frame	O
is	O
over	O
2	O
million	O
rows	O
and	O
looping	O
to	O
extract	O
the	O
elements	O
I	O
need	O
is	O
a	O
poor	O
choice	O
.	O

edit	O
I	O
believe	O
'	O
endog	O
'	O
as	O
defined	O
is	O
incorrect-I	O
should	O
be	O
passing	O
the	O
values	O
for	O
which	O
I	O
want	O
to	O
predict	O
;	O
therefore	O
I've	O
created	O
a	O
date	O
range	O
of	O
12	O
periods	O
past	O
the	O
last	O
recorded	O
value	O
.	O

@USER	O
It	O
should	O
be	O
a	O
little	O
quicker	O
with	O
a	O
boolean	O
index	O
like	O
that	O
,	O
but	O
it	O
does	O
do	O
a	O
cast	O
(	O
timedelta	O
)	O
so	O
I'm	O
not	O
100%	O
sure	O
on	O
that	O
.	O

I	O
still	O
get	O
the	O
same	O
TypeError	O
message	O
using	O
the	O
line	O
you	O
suggest	O
.	O

Use	O
regex	O
with	O
`	O
python	O
`	O
engine	O
#CODE	O

(	O
it's	O
pretty	O
clear	O
that	O
`	O
id	O
`	O
maps	O
to	O
`	O
individual	O
`	O
,	O
but	O
I	O
would	O
clean	O
that	O
up	O
too	O
)	O
.	O

Being	O
able	O
to	O
quickly	O
determine	O
the	O
time	O
difference	O
between	O
Order	O
1	O
and	O
Order	O
2	O
(	O
per	O
PersonID	O
)	O
would	O
be	O
great	O
too	O
.	O

Thus	O
,	O
if	O
there	O
is	O
an	O
update	O
to	O
some	O
value	O
on	O
a	O
memory	O
page	O
,	O
that	O
page	O
is	O

and	O
make	O
this	O
a	O
Series	O
,	O
mapping	O
names	O
to	O
their	O
respective	O
numbers	O
:	O
#CODE	O

That	O
is	O
,	O
for	O
each	O
second	O
there	O
is	O
a	O
value	O
and	O
they	O
should	O
not	O
be	O
averaged	O
,	O
just	O
grouped	O
together	O
to	O
a	O
new	O
series	O
..	O

Specifically	O
,	O
in	O
this	O
case	O
,	O
I'd	O
only	O
like	O
to	O
drop	O
row	O
with	O
Indices	O
'	O
1991-12-31	O
'	O
and	O
'	O
1992-01-31	O
'	O
.	O

Or	O
read	O
it	O
in	O
directly	O
as	O
a	O
csv	O
,	O
by	O
appending	O
'	O
na	O
'	O
to	O
the	O
list	O
of	O
values	O
to	O
be	O
considered	O
NaN	O
:	O
#CODE	O

I	O
fail	O
to	O
see	O
the	O
corelation	O
between	O
"	O
John	O
"	O
and	O
the	O
dates	O
in	O
the	O
target	O
.	O

I	O
get	O
:	O

The	O
question	O
is	O
,	O
how	O
can	O
I	O
remove	O
or	O
filter	O
out	O
all	O
entries	O
that	O
have	O
frequency	O
1	O
?	O

For	O
all	O
the	O
other	O
names	O
that	O
are	O
not	O
in	O
the	O
top	O
ten	O
frequencies	O
I	O
want	O
to	O
combine	O
their	O
number	O
of	O
occurences	O
together	O
under	O
say	O
the	O
name	O
"	O
other	O
"	O
.	O

You	O
should	O
get	O
the	O
following	O
result	O
:	O

Which	O
indeed	O
is	O
longer	O
(	O
50	O
)	O
than	O
my	O
number	O
of	O
columns	O
/	O
indices	O
(	O
25	O
)	O
.	O

I	O
am	O
new	O
to	O
Python	O
(	O
and	O
programming	O
in	O
general	O
!	O
)	O
,	O
trying	O
to	O
conduct	O
some	O
data	O
analysis	O
using	O
Pandas	O
.	O

I	O
would	O
like	O
to	O
combine	O
these	O
columns	O
into	O
start	O
time	O
(	O
index	O
)	O
and	O
length	O
in	O
actual	O
seconds	O
.	O

I'm	O
looking	O
to	O
find	O
,	O
for	O
each	O
Census	O
Block	O
centroid	O
,	O
the	O
distance	O
to	O
it's	O
closest	O
restaurant	O
.	O

You	O
will	O
get	O
the	O
exception	O
"	O
appended	O
items	O
do	O
not	O
match	O
existing	O
items	O
in	O
table	O
!	O

Honestly	O
-	O
we	O
were	O
going	O
to	O
originally	O
do	O
visualizations	O
with	O
it	O
(	O
heatmaps	O
)	O
-	O
but	O
for	O
a	O
lot	O
of	O
reasons	O
we're	O
now	O
going	O
to	O
use	O
D3	O
...	O

For	O
example	O
,	O
if	O
I	O
say	O
year	O
,	O
the	O
entire	O
column	O
needs	O
to	O
be	O
appended	O
into	O
a	O
list	O
like	O
[	O
1	O
year	O
,	O
3	O
minutes	O
,	O
2	O
hours	O
]	O
.	O

Anyone	O
have	O
any	O
suggestions	O
for	O
how	O
to	O
accomplish	O
this	O
?	O

Yeah	O
I	O
know	O
it	O
gives	O
NaN	O
padding	O
,	O
but	O
only	O
on	O
the	O
indices	O
the	O
joining	O
is	O
done	O
over	O
.	O

The	O
paired	O
measurements	O
should	O
have	O
the	O
same	O
month	O
,	O
just	O
different	O
years	O
.	O

You	O
can	O
get	O
started	O
on	O
debugging	O
this	O
by	O
just	O
adding	O
a	O
line	O
to	O
your	O
code	O
and	O
running	O
again	O
:	O
#CODE	O

When	O
I	O
run	O
the	O
solution	O
I	O
get	O
the	O
error	O
.	O

Then	O
let's	O
add	O
a	O
helper	O
column	O
,	O
called	O
Safe	O
,	O
that	O
will	O
be	O
a	O
concatenation	O
of	O
all	O
the	O
Safex	O
columns	O
.	O

product	O
1111	O
non-null	O
object	O

In	O
R	O
,	O
using	O
the	O
car	O
package	O
,	O
there	O
is	O
a	O
useful	O
function	O
`	O
some	O
(	O
x	O
,	O
n	O
)`	O
which	O
is	O
similar	O
to	O
head	O
but	O
selects	O
,	O
in	O
this	O
example	O
,	O
10	O
rows	O
at	O
random	O
from	O
x	O
.	O

The	O
separator	O
(	O
between	O
cells	O
)	O
is	O
defined	O
by	O
the	O
operating	O
system	O
(	O
at	O
least	O
under	O
Windows	O
)	O
,	O
and	O
when	O
the	O
system	O
wide	O
list	O
separator	O
differs	O
from	O
comma	O
,	O
pandas	O
(	O
or	O
anything	O
else	O
I	O
tried	O
)	O
cannot	O
determine	O
what	O
separator	O
should	O
be	O
used	O
.	O

Setting	O
up	O
a	O
histogram	O
with	O
a	O
range	O
and	O
an	O
appropriate	O
bin	O
size	O
is	O
an	O
unknown	O
.	O

Thanks	O
TravisJ	O
,	O
I	O
guess	O
I	O
was	O
just	O
struggling	O
to	O
get	O
the	O
(	O
...	O
something	O
involving	O
group	O
...	O
)	O
in	O
when	O
i	O
was	O
using	O
the	O
ax=fig1	O
....	O
method	O
.	O

I	O
am	O
optimising	O
the	O
span	O
of	O
an	O
exponential	O
moving	O
average	O
and	O
the	O
number	O
of	O
lagged	O
variables	O
that	O
I	O
use	O
in	O
the	O
regression	O
.	O

The	O
error	O
message	O
that	O
I	O
get	O
is	O
:	O
#CODE	O

It	O
doesn't	O
however	O
take	O
advantage	O
of	O
the	O
psql	O
package	O
in	O
Pandas	O
.	O

On	O
a	O
much	O
larger	O
data	O
set	O
,	O
this	O
runs	O
in	O
790	O
ms	O
compared	O
to	O
1345	O
ms	O
for	O
ajcr's	O
and	O
Primer's	O
solutions	O
.	O

I've	O
put	O
together	O
one	O
approach	O
to	O
that	O
solution	O
that	O
should	O
scale	O
relatively	O
well	O
.	O

I	O
was	O
hoping	O
there	O
was	O
an	O
easy	O
way	O
to	O
get	O
the	O
set	O
of	O
B	O
values	O
per	O
each	O
A	O
value	O
like	O
`	O
{	O
'	O
one	O
'	O
:[	O
'	O
A	O
'	O
,	O
'	O
B	O
']	O
,	O
'	O
two	O
'	O
:[	O
'	O
A	O
']	O
,	O
'	O
three	O
'	O
:[	O
'	O
B	O
']	O
}	O
`	O
but	O
I	O
don't	O
see	O
anything	O
like	O
that	O
in	O
the	O
pandas	O
documentation	O

To	O
avoid	O
chained	O
indexing	O
,	O
you	O
need	O
to	O
get	O
all	O
your	O
conditions	O
into	O
a	O
single	O
set	O
of	O
brackets	O
.	O

But	O
trying	O
to	O
parse	O
the	O
column	O
name	O
and	O
hierarchy	O
and	O
auto-generate	O
the	O
insertable	O
thing	O
with	O
matching	O
index	O
is	O
unpleasant	O
.	O

The	O
seaborn	O
package	O
will	O
allow	O
you	O
to	O
plot	O
long	O
form	O
data	O
like	O
you	O
have	O
without	O
pivoting	O
but	O
pandas	O
requires	O
shared	O
index	O
and	O
one	O
column	O
per	O
plotted	O
line	O
by	O
default	O
so	O
your	O
solution	O
is	O
the	O
correct	O
one	O
.	O

Unable	O
to	O
filter	O
out	O
missing	O
(	O
NaN	O
)	O
location	O
data	O
while	O
using	O
Pandas	O
and	O
Geocoder	O
modules	O
in	O
Python	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

How	O
could	O
I	O
sum	O
consecutive	O
day	O
values	O
here	O
,	O
so	O
I	O
would	O
get	O
something	O
like	O
this	O
?	O

I	O
changed	O
this	O
to	O
use	O
\t	O
as	O
the	O
separator	O
.	O

It's	O
possible	O
,	O
but	O
if	O
your	O
data	O
is	O
organized	O
it's	O
very	O
quick	O
with	O
shifting	O
it	O

@USER	O
fixed	O
,	O
was	O
a	O
typo	O
;	O
this	O
take	O
a	O
full	O
uri	O

Just	O
get	O
rid	O
of	O
it	O
and	O
reindent	O
the	O
loop	O
body	O
to	O
the	O
left	O
one	O
level	O
.	O

However	O
,	O
I	O
still	O
get	O
the	O
warning	O
.	O

For	O
any	O
x	O
in	O
dataset2	O
it	O
has	O
mapped	O
value	O
in	O
col2	O
.	O

but	O
you	O
then	O
need	O
to	O
store	O
these	O
dfs	O
somewhere	O
which	O
means	O
either	O
in	O
a	O
list	O
or	O
tuple	O
or	O
some	O
other	O
container	O
or	O
use	O
a	O
generator	O

and	O
so	O
on	O
for	O
the	O
remaining	O
location	O
categories	O
.	O

@USER	O
do	O
**	O
all	O
**	O
the	O
columns	O
in	O
the	O
DF	O
require	O
that	O
same	O
replacement	O
?	O

did	O
you	O
get	O
any	O
warnings	O
while	O
installing	O
numpy	O
or	O
pandas	O
?	O

This	O
`	O
df	O
`	O
consist	O
of	O
volume	O
observations	O
at	O
every	O
10	O
second	O
for	O
22	O
non-consecutive	O
days	O
.	O

I've	O
also	O
included	O
a	O
section	O
to	O
immediately	O
identify	O
any	O
redundant	O
genes	O
that	O
don't	O
have	O
any	O
SNPs	O
that	O
fall	O
within	O
their	O
range	O
.	O

Using	O
some	O
string	O
formatting	O
to	O
get	O
the	O
index	O
,	O
but	O
works	O
for	O
any	O
combination	O
of	O
months	O
(	O
as	O
long	O
as	O
the	O
first	O
month	O
is	O
explicitly	O
included	O
)	O
.	O

Any	O
suggestions	O
please	O
?	O

I	O
get	O
:	O
#CODE	O

I	O
understand	O
that	O
we	O
can	O
line	O
them	O
all	O
together	O
side	O
by	O
side	O
so	O
their	O
dates	O
match	O
and	O
loop	O
row	O
by	O
row	O
,	O
but	O
then	O
when	O
i	O
have	O
100k	O
different	O
securities	O
,	O
this	O
is	O
slow	O
in	O
memory	O
.	O

I	O
would	O
like	O
to	O
automate	O
this	O
table	O
so	O
If	O
I	O
change	O
my	O
parameters	O
in	O
my	O
code	O
,	O
I	O
get	O
a	O
new	O
table	O
with	O
that	O
new	O
data	O
.	O

But	O
I	O
get	O
,	O
which	O
I	O
cannot	O
understand	O
,	O
#CODE	O

Here's	O
the	O
product	O
:	O
#CODE	O

(	O
My	O
actual	O
problem	O
involves	O
parsing	O
strings	O
into	O
lists	O
,	O
then	O
checking	O
for	O
presents	O
of	O
a	O
1	O
or	O
0	O
in	O
one	O
list	O
and	O
if	O
so	O
marking	O
the	O
cosponsoring	O
element	O
in	O
the	O
other	O
list	O
with	O
a	O
asterix	O
,	O
but	O
I	O
didn't	O
want	O
to	O
put	O
that	O
in	O
my	O
example	O
and	O
it	O
is	O
long	O
and	O
harder	O
to	O
follow	O
.	O

What	O
is	O
the	O
error	O
you	O
get	O
?	O

So	O
first	O
chunk	O
is	O
stored	O
as	O
integer	O
and	O
in	O
second	O
chunk	O
gets	O
NaN	O
values	O
and	O
store	O
cannot	O
convert	O
NaN	O
to	O
integer	O

Just	O
play	O
around	O
with	O
it	O
to	O
get	O
it	O
right	O
.	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

Edit	O
:	O
Here's	O
an	O
example	O
of	O
generating	O
a	O
close-enough	O
data	O
set	O
,	O
so	O
you	O
can	O
get	O
some	O
idea	O
of	O
what	O
I	O
mean	O
:	O
#CODE	O

@USER	O
that	O
means	O
you	O
have	O
non	O
string	O
values	O
mixed	O
in	O
;	O
you	O
need	O
to	O
specify	O
`	O
na=True	O
`	O
or	O
`	O
na=False	O
`	O
depending	O
on	O
what	O
those	O
values	O
are	O
.	O
see	O
my	O
edits	O
.	O

Preferably	O
,	O
use	O
Pandas	O
for	O
the	O
data	O
structure	O
and	O
Python	O
for	O
the	O
language	O
.	O

Then	O
as	O
in	O
the	O
first	O
point	O
,	O
I	O
would	O
like	O
to	O
calculate	O
the	O
number	O
of	O
continuous	O
up	O
and	O
down	O
sequences	O
from	O
the	O
previous	O
point	O
.	O

0	O
can	O
be	O
changed	O
to	O
1	O
or	O
other	O
values	O
later	O
in	O
the	O
code	O

and	O
in	O
the	O
instance	O
when	O
i	O
am	O
able	O
to	O
set	O
the	O
index	O
of	O
the	O
df	O
to	O
the	O
range	O
,	O
the	O
data	O
in	O
the	O
4	O
columns	O
change	O
to	O
NaN	O
since	O
they	O
have	O
no	O
data	O
that	O
matches	O
the	O
new	O
index	O
.	O

(	O
Note	O
that	O
this	O
produces	O
an	O
unrealistically	O
high	O
number	O
of	O
flooding	O
events	O
,	O
but	O
that's	O
just	O
because	O
of	O
how	O
the	O
sample	O
data	O
is	O
set	O
up	O
and	O
not	O
reflective	O
of	O
a	O
typical	O
pond	O
,	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
pond	O
flooding	O
!	O
)	O
#CODE	O

What	O
output	O
do	O
you	O
get	O
from	O
this	O
?	O

`	O
Ideally	O
,	O
for	O
the	O
pages	O
that	O
have	O
multiple	O
groups	O
of	O
34	O
,	O
i'd	O
like	O
to	O
add	O
a	O
suffix	O
of	O
_1	O
,	O
_2	O
,	O
_3	O
,	O
etc	O
.	O

That	O
means	O
duplicating	O
values	O
from	O
cols	O
`	O
product_id	O
`	O
and	O
tem_name	O
`	O
as	O
long	O
as	O
there	O
are	O
items	O
in	O
list	O
`	O
prices	O
`	O
.	O

cool	O
,	O
but	O
I	O
get	O
a	O
syntax	O
error	O
for	O

problem	O
is	O
the	O
sum	O
i	O
now	O
get	O
is	O
lined	O
up	O
in	O
week	O
intervals	O
but	O
not	O
in	O
the	O
right	O
sequence	O
.	O
this	O
wouldn't	O
be	O
a	O
problem	O
but	O
i	O
need	O
to	O
get	O
to	O
the	O
month	O
of	O
each	O
date	O
in	O
order	O
to	O
do	O
the	O
next	O
step	O
i	O
guess	O
.	O

For	O
empty	O
date	O
cells	O
I	O
am	O
inserting	O
a	O
NaT	O
,	O
which	O
I	O
would	O
have	O
thought	O
would	O
be	O
fine	O
,	O
but	O
in	O
Oracle	O
that	O
is	O
becoming	O
some	O
weird	O
invalid	O
time	O
that	O
displays	O
as	O
"	O
0001-255-255	O
00:00	O
:	O
00	O
"	O
(	O
Something	O
like	O
MAXINT	O
or	O
0	O
being	O
converted	O
to	O
a	O
timestamp	O
I'm	O
guessing	O
?	O
)	O
#CODE	O

I	O
would	O
like	O
to	O
get	O
the	O
following	O
result	O
:	O
#CODE	O

so	O
in	O
all	O
both	O
suggestions	O
below	O
worked	O
for	O
me	O
:	O

I	O
get	O
the	O
following	O
error	O
message	O
:	O
#CODE	O

I	O
would	O
suggest	O
using	O
the	O
duplicated	O
method	O
on	O
the	O
Pandas	O
Index	O
itself	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
group	O
the	O
x	O
values	O
into	O
equal	O
size	O
bins	O
,	O
and	O
for	O
each	O
bin	O
take	O
the	O
average	O
value	O
of	O
both	O
x	O
and	O
y	O
.	O

For	O
this	O
data	O
set	O
the	O
two	O
numbers	O
are	O
always	O
equal	O
.	O

Do	O
you	O
want	O
to	O
check	O
if	O
the	O
value	O
is	O
in	O
the	O
provided	O
bounds	O
and	O
return	O
a	O
boolean	O
True	O
/	O
False	O
array	O
,	O
or	O
you	O
want	O
to	O
represent	O
your	O
values	O
in	O
categories	O
represented	O
by	O
those	O
bounds	O
?	O

The	O
series	O
I'd	O
like	O
to	O
get	O
would	O
contain	O
:	O
#CODE	O

The	O
error	O
I	O
get	O
:	O
#CODE	O

How	O
do	O
you	O
deal	O
with	O
apparently	O
overlapping	O
date	O
ranges	O
?	O

There	O
are	O
more	O
columns	O
in	O
the	O
data	O
that	O
are	O
not	O
shown	O
above	O
,	O
and	O
using	O
this	O
code	O
causes	O
the	O
non-numeric	O
columns	O
to	O
drop	O
off	O
.	O

Any	O
suggestions	O
?	O

python	O
-	O
trying	O
to	O
get	O
a	O
new	O
pandas	O
release	O

In	O
the	O
process	O
of	O
creating	O
an	O
example	O
with	O
code	O
,	O
I	O
managed	O
to	O
get	O
it	O
working	O
.	O

Can	O
you	O
post	O
raw	O
data	O
and	O
example	O
code	O
that	O
demonstrates	O
this	O
'	O
cutting	O
'	O
off	O

Running	O
your	O
code	O
on	O
the	O
sample	O
data	O
produces	O
the	O
same	O
result	O
.	O

I	O
want	O
to	O
take	O
advantage	O
of	O
sortedness	O
since	O
with	O
very	O
large	O
series	O
merging	O
when	O
we	O
know	O
it's	O
sorted	O
should	O
be	O
linear	O
in	O
total	O
length	O
of	O
the	O
arrays	O
,	O
whereas	O
any	O
sort	O
will	O
be	O
non-linear	O
.	O

What	O
if	O
you	O
just	O
changed	O
the	O
index	O
from	O
date	O
/	O
status	O
to	O
date	O
/	O
var1	O
/	O
status	O
?	O

which	O
when	O
imported	O
into	O
pandas	O
data	O
frames	O
and	O
each	O
joined	O
to	O
a	O
common	O
timestamp	O
,	O
with	O
a	O
day	O
of	O
year	O
field	O
added	O
,	O
so	O
looking	O
something	O
like	O
:	O
#CODE	O

This	O
works	O
only	O
if	O
your	O
object	O
is	O
table	O
format	O
(	O
rather	O
than	O
fixed	O
format	O
)	O
.	O

Drop	O
range	O
of	O
columns	O
by	O
labels	O

You	O
could	O
put	O
```	O
[	O
'	O
col1	O
']```	O
at	O
the	O
end	O
to	O
get	O
an	O
int	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

For	O
encoding	O
training	O
data	O
you	O
can	O
use	O
fit_transform	O
which	O
will	O
discover	O
the	O
category	O
labels	O
and	O
create	O
appropriate	O
dummy	O
variables	O
.	O

I	O
think	O
you're	O
confusing	O
how	O
to	O
filter	O
here	O
,	O
if	O
you're	O
looking	O
for	O
a	O
specific	O
value	O
then	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
VKO	O
']`	O
will	O
return	O
only	O
the	O
rows	O
where	O
that	O
condition	O
is	O
satisfied	O
,	O
for	O
your	O
last	O
part	O
the	O
reason	O
you	O
get	O
an	O
empty	O
row	O
is	O
that	O
you're	O
slicing	O
the	O
first	O
3	O
rows	O
and	O
then	O
comparing	O
the	O
first	O
value	O
with	O
your	O
string	O
but	O
the	O
first	O
string	O
value	O
is	O
'	O
VKO	O
'	O
and	O
not	O
'	O
ZZZ	O
'	O
,	O
you	O
should	O
do	O
this	O
:	O
`	O
stock	O
[	O
stock	O
[	O
'	O
Whs	O
']	O
==	O
'	O
ZZZ	O
']`	O
to	O
filter	O
the	O
resuls	O
first	O

I	O
had	O
changed	O
the	O
data	O
on	O
the	O
local	O
file	O
.	O

Note	O
that	O
this	O
is	O
slightly	O
different	O
as	O
we	O
are	O
returning	O
the	O
first	O
index	O
here	O
(	O
and	O
not	O
the	O
normally	O
returned	O
last	O
,	O
youy	O
could	O
do	O
either	O
)	O
.	O

I'm	O
not	O
sure	O
how	O
the	O
archive	O
block	O
reading	O
works	O
and	O
how	O
much	O
data	O
it	O
loads	O
into	O
memory	O
,	O
but	O
it's	O
clear	O
that	O
you	O
will	O
have	O
to	O
somehow	O
control	O
the	O
size	O
of	O
the	O
chunks	O
.	O

Regarding	O
nesting	O
of	O
functions	O
:	O
if	O
you	O
believe	O
a	O
function	O
has	O
a	O
general	O
purpose	O
or	O
is	O
reusable	O
,	O
then	O
it	O
should	O
be	O
defined	O
at	O
the	O
top	O
level	O
of	O
a	O
module	O
or	O
some	O
place	O
where	O
other	O
functions	O
can	O
call	O
it	O
.	O

I	O
get	O
an	O
error	O
saying	O
:	O
lambda	O
(	O
)	O
takes	O
exactly	O
1	O
argument	O
(	O
3	O
given	O
)	O

You	O
can	O
get	O
the	O
list	O
of	O
columns	O
with	O
:	O
#CODE	O

One	O
is	O
I	O
only	O
wanted	O
to	O
get	O
the	O
mean	O
of	O
the	O
next	O
rows	O
that	O
relate	O
to	O
the	O
same	O
group	O
.	O

By	O
default	O
this	O
will	O
add	O
a	O
column	O
of	O
integers	O
(	O
because	O
R	O
factors	O
are	O
encoded	O
as	O
integers	O
)	O
.	O

I	O
am	O
trying	O
to	O
calculate	O
the	O
percent	O
change	O
by	O
month	O
for	O
each	O
product	O
.	O

If	O
i	O
want	O
only	O
USA	O
Equities	O
vs	O
all	O
other	O
equity	O
and	O
not	O
the	O
enitre	O
89	O
columns	O
how	O
do	O
i	O
do	O
it	O
?	O

Pandas	O
:	O
Efficient	O
way	O
to	O
get	O
first	O
row	O
with	O
element	O
that	O
is	O
smaller	O
than	O
a	O
given	O
value	O

I	O
first	O
thought	O
this	O
was	O
a	O
spacing	O
issue	O
in	O
the	O
columns	O
values	O
,	O
so	O
I	O
replaced	O
them	O
with	O
underscores	O
,	O
but	O
it	O
also	O
doesn't	O
work	O
in	O
columns	O
which	O
only	O
contain	O
a	O
single	O
word	O
and	O
no	O
spaces	O
?	O

Are	O
you	O
getting	O
the	O
values	O
from	O
the	O
GUI	O
ok	O
,	O
but	O
your	O
calculations	O
are	O
returning	O
nothin	O
?	O

This	O
is	O
not	O
precisely	O
what	O
I'm	O
after	O
,	O
but	O
I	O
think	O
I'll	O
have	O
to	O
write	O
a	O
loop	O
top	O
get	O
that	O
.	O

Hence	O
,	O
the	O
width	O
of	O
each	O
bin	O
over	O
the	O
interval	O
from	O
[	O
-1	O
,	O
1	O
]	O
should	O
be	O
`	O
2	O
/	O
10=	O
0.20	O
`	O
;	O
however	O
,	O
the	O
graph	O
does	O
not	O
have	O
any	O
bins	O
with	O
a	O
width	O
of	O
0.20	O
.	O

I'm	O
having	O
a	O
problem	O
trying	O
to	O
get	O
a	O
character	O
count	O
column	O
of	O
the	O
string	O
values	O
in	O
another	O
column	O
,	O
and	O
haven't	O
figured	O
out	O
how	O
to	O
do	O
it	O
efficiently	O
.	O

I	O
think	O
I	O
get	O
the	O
idea	O
.	O

How	O
can	O
I	O
get	O
the	O
position	O
of	O
index	O
`	O
18	O
`	O
?	O

When	O
using	O
`	O
groupy-apply	O
`	O
,	O
instead	O
of	O
dropping	O
the	O
group	O
key	O
index	O
using	O
:	O
#CODE	O

No	O
real	O
advantage	O
if	O
there	O
are	O
just	O
two	O
categories	O
:	O
#CODE	O

@USER	O
I	O
suspect	O
something	O
else	O
is	O
going	O
on	O
then	O
,	O
because	O
when	O
I	O
memory	O
profile	O
this	O
way	O
(	O
with	O
`	O
drop	O
`)	O
using	O
the	O
snippet	O
that	O
Michael	O
Laszlo	O
posted	O
,	O
I	O
do	O
not	O
see	O
memory	O
growth	O
.	O

Of	O
course	O
you	O
may	O
not	O
like	O
the	O
index	O
as	O
tuples	O
;	O
you	O
could	O
reset	O
the	O
index	O
within	O
the	O
list	O
comprehension	O
to	O
get	O
the	O
following	O
if	O
you	O
prefer	O
(	O
for	O
example	O
,	O
this	O
if	O
for	O
part	O
1	O
):	O
#CODE	O

Here	O
is	O
how	O
I	O
am	O
trying	O
to	O
get	O
the	O
output	O
to	O
look	O
like	O
:	O
#CODE	O

memory	O
efficient	O
Python	O
(	O
pandas	O
)	O
aggregates	O
of	O
categories	O
from	O
one	O
csv	O
file	O
per	O
period	O

But	O
this	O
last	O
line	O
generate	O
the	O
error	O
message	O
:	O
`	O
no	O
item	O
named	O
timestamp	O
`	O
.	O

The	O
problem	O
here	O
,	O
well	O
the	O
biggest	O
one	O
,	O
is	O
that	O
your	O
`	O
data	O
`	O
is	O
string	O
,	O
not	O
valid	O
data	O
structure	O
,	O
same	O
thing	O
with	O
dictionary	O
inside	O
it	O
,	O
you	O
creating	O
strings	O
,	O
not	O
data	O
structures	O
.	O

I	O
also	O
want	O
to	O
create	O
a	O
new	O
column	O
that	O
shows	O
the	O
difference	O
in	O
days	O
between	O
the	O
end	O
and	O
begin	O
dates	O
.	O

In	O
the	O
next	O
column	O
(	O
B	O
)	O
,	O
I	O
want	O
to	O
create	O
an	O
indexed	O
series	O
that	O
begins	O
at	O
1000	O
based	O
on	O
the	O
percent	O
changes	O
.	O

I	O
can't	O
use	O
fixed	O
position	O
to	O
slice	O
it	O
.	O

This	O
gets	O
you	O
to	O
where	O
I	O
am	O
.	O

I	O
am	O
using	O
this	O
to	O
generate	O
nodes	O
in	O
a	O
graph	O
,	O
if	O
x1	O
,	O
x2	O
are	O
not	O
exactly	O
equal	O
,	O
networkx	O
recognizes	O
them	O
as	O
different	O
nodes	O
,	O
if	O
x1=x2	O
,	O
i	O
get	O
a	O
recombinant	O
tree	O
which	O
is	O
what	O
i	O
want	O
.	O

My	O
example	O
was	O
not	O
good	O
enough	O
,	O
as	O
your	O
script	O
smartly	O
took	O
the	O
size	O
from	O
the	O
length	O
of	O
the	O
df	O
.	O

iPython's	O
Rmagic	O
is	O
already	O
able	O
to	O
perform	O
an	O
automagic	O
conversion	O
between	O
the	O
two	O
in	O
a	O
number	O
of	O
situations	O
,	O
and	O
might	O
be	O
a	O
good	O
way	O
to	O
get	O
familiar	O
with	O
Python	O
.	O

So	O
if	O
your	O
dataset	O
is	O
really	O
large	O
,	O
perhaps	O
store	O
them	O
first	O
in	O
on-disk	O
database	O
/	O
HDF	O
rather	O
than	O
csv	O
file	O
and	O
sort	O
them	O
there	O
,	O
and	O
then	O
query	O
.	O

One	O
had	O
no	O
problem	O
at	O
all	O
(	O
the	O
xlsx	O
file	O
,	O
example	O
2	O
)	O
and	O
the	O
other	O
(	O
xls	O
,	O
example	O
1	O
)	O
had	O
a	O
difference	O
between	O
the	O
columns	O
.	O

then	O
with	O
the	O
`	O
sorted	O
`	O
function	O
and	O
`	O
datetime	O
`	O
module	O
(	O
remember	O
the	O
`	O
sorted	O
`	O
function	O
change	O
the	O
`	O
data	O
`	O
it	O
self	O
)	O
#CODE	O

BUT	O
,	O
I	O
get	O
the	O
"	O
SettingWithCopyWarning	O
"	O
:	O

This	O
is	O
machine	O
generated	O
data	O
.	O

You	O
can't	O
use	O
`	O
or	O
`	O
with	O
arrays	O
,	O
if	O
you	O
try	O
this	O
you	O
get	O
an	O
error	O
`	O
ValueError	O
:	O
The	O
truth	O
value	O
of	O
a	O
Series	O
is	O
ambiguous	O
.	O

Incidentally	O
,	O
this	O
is	O
the	O
same	O
result	O
that	O
you	O
would	O
get	O
with	O
the	O
Spearman	O
R	O
coefficient	O
as	O
well	O
.	O

How	O
could	O
I	O
do	O
to	O
have	O
exactly	O
one	O
calendar	O
year	O
between	O
dates	O
in	O
spite	O
of	O
leap	O
years	O
?	O

"	O
However	O
,	O
we	O
still	O
have	O
one	O
large	O
difference	O
.	O

All	O
that	O
remains	O
is	O
to	O
merged	O
the	O
contents	O
of	O
the	O
second-level	O
dictionaries	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
a	O
single	O
image	O
with	O
heatmaps	O
representing	O
the	O
correlation	O
of	O
features	O
of	O
data	O
points	O
for	O
each	O
label	O
separately	O
.	O

I	O
have	O
two	O
Series	O
which	O
have	O
a	O
format	O
equal	O
to	O
this	O
:	O
#CODE	O

However	O
,	O
when	O
I	O
do	O
the	O
following	O
,	O
the	O
error	O
does	O
not	O
show	O
up	O
and	O
I	O
get	O
the	O
expected	O
result	O
:	O
#CODE	O

As	O
brackets	O
are	O
part	O
of	O
the	O
regex	O
syntax	O
if	O
you're	O
trying	O
to	O
match	O
literal	O
brackets	O
you	O
need	O
to	O
escape	O
them	O
:	O
#CODE	O

Yeah	O
,	O
the	O
best	O
idea	O
I've	O
had	O
this	O
whole	O
time	O
was	O
to	O
actually	O
sign	O
up	O
to	O
SO	O
so	O
I	O
can	O
post	O
my	O
own	O
questions	O
instead	O
of	O
trying	O
to	O
funble	O
my	O
way	O
through	O
problems	O
by	O
patching	O
together	O
answers	O
to	O
other	O
peoples	O
questions	O
-	O
sometimes	O
what	O
I	O
need	O
just	O
isn't	O
covered	O
in	O
other	O
people's	O
questions	O
.	O

But	O
when	O
I	O
try	O
and	O
import	O
pandas	O
I	O
get	O
:	O
#CODE	O

However	O
,	O
I	O
can't	O
get	O
the	O
column	O
to	O
fill	O
up	O
with	O
the	O
appropriate	O
user	O
inputted	O
value	O
.	O

Also	O
I'm	O
not	O
getting	O
any	O
traceback	O
messages	O
but	O
I	O
think	O
I	O
have	O
an	O
idea	O
of	O
where	O
my	O
problem	O
may	O
be	O
.	O

Basically	O
I'm	O
trying	O
to	O
get	O
at	O
this	O
:	O
#CODE	O

So	O
this	O
is	O
not	O
a	O
fully	O
working	O
answer	O
,	O
but	O
maybe	O
it	O
can	O
be	O
extended	O
to	O
ultimatively	O
get	O
you	O
there	O
.	O

I	O
have	O
a	O
pandas	O
Series	O
holding	O
one	O
numpy	O
array	O
per	O
entry	O
(	O
same	O
length	O
for	O
all	O
entries	O
)	O
and	O
I	O
would	O
like	O
to	O
convert	O
this	O
to	O
a	O
2D	O
numpy	O
array	O
.	O

For	O
example	O
,	O
let's	O
say	O
I	O
want	O
to	O
select	O
50%	O
(	O
but	O
this	O
could	O
change	O
)	O
.	O

For	O
this	O
purpose	O
I	O
would	O
like	O
to	O
find	O
the	O
soonest	O
date	O
(	O
month	O
)	O
and	O
from	O
there	O
start	O
counting	O
months	O
and	O
their	O
averages	O
.	O

You've	O
changed	O
your	O
data	O
,	O
so	O
the	O
script	O
as	O
written	O
doesn't	O
work	O
.	O

Hopefully	O
you'll	O
get	O
an	O
answer	O
soon	O
.	O

This	O
should	O
get	O
you	O
to	O
the	O
point	O
in	O
your	O
code	O
where	O
you	O
start	O
dropping	O
columns	O
and	O
start	O
concatenating	O
.	O

So	O
,	O
if	O
k	O
were	O
equal	O
to	O
2	O
,	O
and	O
this	O
were	O
my	O
data	O
frame	O
:	O
#CODE	O

It	O
seems	O
as	O
though	O
the	O
second	O
approach	O
,	O
using	O
"	O
where	O
"	O
is	O
only	O
returning	O
data	O
from	O
the	O
last	O
few	O
appended	O
files	O
,	O
while	O
the	O
first	O
approach	O
is	O
returning	O
much	O
more	O
data	O
.	O

I	O
have	O
to	O
improve	O
this	O
to	O
get	O
rid	O
of	O
redundancy	O
,	O
and	O
I	O
am	O
not	O
sure	O
how	O
to	O
go	O
about	O
this	O
.	O

Are	O
you	O
able	O
to	O
post	O
raw	O
data	O
and	O
code	O
to	O
reproduce	O
this	O
issue	O
?	O

The	O
relative	O
size	O
between	O
consecutive	O
levels	O
.	O

This	O
will	O
potentially	O
cater	O
the	O
corner	O
cases	O
if	O
you	O
happen	O
to	O
have	O
conditions	O
like	O
:	O
"	O
value	O
"	O
360	O
then	O
+360	O
else	O
-360	O
but	O
the	O
sequence	O
of	O
the	O
update	O
will	O
cause	O
the	O
results	O
reapply	O
,	O
ie	O
.	O

I'll	O
put	O
an	O
example	O
of	O
what	O
I'm	O
suggesting	O
in	O
my	O
answer	O
.	O

But	O
you	O
said	O
you	O
want	O
only	O
the	O
time	O
points	O
from	O
the	O
longest	O
`	O
csv	O
`	O
.	O

replacing	O
this	O
in	O
code	O
just	O
drop	O
those	O
whole	O
rows	O
...	O

Thank	O
you	O
for	O
response	O
and	O
for	O
helping	O
me	O
get	O
to	O
next	O
level	O
of	O
pyhton	O
,	O
great	O
stuff	O
!	O

For	O
instance	O
,	O
you	O
can	O
insert	O
new	O
values	O
into	O
the	O
index	O
(	O
and	O
even	O
choose	O
what	O
value	O
it	O
should	O
have	O
):	O
#CODE	O

What	O
I	O
need	O
to	O
do	O
is	O
to	O
compute	O
the	O
average	O
temperature	O
for	O
every	O
run	O
,	O
averaging	O
all	O
the	O
temperature	O
measurements	O
belonging	O
to	O
a	O
run	O
.	O

@USER	O
I	O
get	O
`	O
Type	O
Error	O
:	O
'	O
bool	O
'	O
object	O
is	O
not	O
callable	O
`	O
when	O
I	O
do	O
that	O

For	O
your	O
specific	O
request	O
of	O
entries	O
between	O
12:00	O
to	O
13:00	O
for	O
every	O
single	O
day	O
,	O
you	O
can	O
fetch	O
the	O
rows	O
with	O
:	O
#CODE	O

Also	O
,	O
their	O
order	O
matters	O
(	O
they	O
are	O
sorted	O
by	O
decreasing	O
standard	O
deviation	O
across	O
rows	O
and	O
should	O
appear	O
in	O
this	O
order	O
in	O
the	O
heatmap	O
.	O
)	O

I'm	O
not	O
averse	O
the	O
reformatting	O
the	O
data	O
in	O
Pandas	O
-->	O
dumping	O
to	O
CSV	O
-->	O
importing	O
to	O
NetworkX	O
,	O
but	O
it	O
seems	O
as	O
if	O
I	O
should	O
be	O
able	O
to	O
generate	O
the	O
edges	O
from	O
the	O
index	O
and	O
the	O
nodes	O
from	O
the	O
values	O
.	O

What	O
is	O
the	O
simplest	O
way	O
to	O
get	O
a	O
sum	O
of	O
only	O
numbers	O
across	O
the	O
entire	O
frame	O
?	O

I	O
think	O
you	O
mean	O
a	O
Lorenz	O
plot	O
:	O
#URL	O
This	O
would	O
make	O
sense	O
then	O
as	O
it	O
requires	O
a	O
specific	O
preordering	O
of	O
the	O
data	O
.	O

Any	O
suggestions	O
?	O

Despite	O
the	O
title	O
,	O
similar	O
problems	O
can	O
occur	O
with	O
other	O
operating	O
systems	O
if	O
you	O
mix	O
32-bit	O
and	O
64-fit	O
versions	O
.	O

Use	O
`	O
select_as_coordinates	O
`	O
to	O
actually	O
execute	O
your	O
query	O
;	O
this	O
returns	O
an	O
`	O
Int64Index	O
`	O
of	O
the	O
row	O
number	O
(	O
the	O
coordinates	O
)	O
.	O

Somehow	O
create	O
a	O
mapping	O
so	O
that	O
instead	O
of	O
the	O
labels	O
being	O
29	O
,	O
30	O
etc	O
,	O
they	O
say	O
"	O
week	O
29	O
"	O
,	O
"	O
Week	O
30	O
"	O
etc	O
.	O

The	O
fix	O
you	O
describe	O
would	O
work	O
,	O
of	O
course	O
,	O
but	O
then	O
I	O
could	O
skip	O
pandas	O
all-together	O
and	O
directly	O
plot	O
the	O
results	O
of	O
my	O
individual	O
simulations	O
.	O

These	O
two	O
timezones	O
have	O
different	O
names	O
but	O
represent	O
the	O
same	O
thing	O
,	O
however	O

which	O
would	O
just	O
change	O
the	O
last	O
data	O
point	O
.	O

What	O
do	O
you	O
mean	O
by	O
reproducible	O
example	O
?	O

But	O
thought	O
i'd	O
make	O
it	O
clear	O
what	O
my	O
next	O
objective	O
was	O
,	O
in	O
case	O
someone	O
could	O
illuminate	O
a	O
better	O
method	O
to	O
get	O
there	O
.	O

What	O
output	O
do	O
you	O
get	O
when	O
you	O
just	O
enter	O
`	O
pd	O
`	O
in	O
the	O
console	O
?	O

When	O
I	O
train	O
on	O
each	O
label	O
I	O
get	O
et	O
better	O
than	O
73%	O
on	O
each	O
label	O
.	O

but	O
what	O
value	O
does	O
it	O
grab	O
when	O
indexing	O
?	O
in	O
other	O
words	O
,	O
if	O
i'm	O
just	O
testing	O
one	O
side	O
i'll	O
get	O
the	O
value	O
corresponding	O
to	O
that	O
row	O
if	O
true	O
.	O
since	O
both	O
sides	O
could	O
be	O
true	O
and	O
one	O
of	O
them	O
is	O
always	O
true	O
,	O
which	O
row's	O
values	O
will	O
be	O
selected	O
?	O

However	O
,	O
this	O
is	O
a	O
bug	O
as	O
you	O
should	O
get	O
an	O
error	O
.	O

guys	O
at	O
least	O
tell	O
me	O
why	O
i	O
am	O
getting	O
downvoted	O
?	O

In	O
some	O
cases	O
the	O
data	O
might	O
be	O
out	O
of	O
sync	O
which	O
makes	O
direct	O
comparisons	O
difficult	O
.	O

I	O
would	O
like	O
to	O
generate	O
a	O
matrix	O
which	O
contains	O
the	O
output	O
of	O
the	O
function	O
for	O
every	O
combination	O
of	O
X	O
and	O
Y	O
.	O

I'm	O
trying	O
to	O
get	O
the	O
data	O
to	O
in	O
the	O
column	O
"	O
Structure	O
"	O
to	O
repeat	O
the	O
row	O
labels	O
so	O
it	O
look	O
like	O
this	O
:	O
#CODE	O

decision	O
for	O
single	O
rows	O
to	O
get	O
converted	O
into	O
a	O
series	O
-	O
why	O
not	O
a	O

I	O
get	O
#CODE	O

More	O
info	O
as	O
requested	O
#CODE	O

[	O
Their	O
product	O
page	O
]	O
(	O
#URL	O
)	O
holds	O
the	O
answer	O
.	O

Next	O
,	O
you	O
wish	O
to	O
get	O
the	O
specific	O
groups	O
from	O
this	O
`	O
grouped	O
`	O
object	O
.	O

To	O
get	O
datetime64	O
that	O
uses	O
seconds	O
directly	O
:	O
#CODE	O

The	O
only	O
option	O
you	O
may	O
have	O
is	O
to	O
setup	O
your	O
data	O
structures	O
to	O
be	O
light	O
weight	O
so	O
each	O
worker	O
isn't	O
boated	O
by	O
redundant	O
copies	O
of	O
the	O
same	O
data	O
or	O
excessive	O
amounts	O
of	O
data	O
which	O
might	O
be	O
better	O
off	O
split	O
across	O
different	O
workers	O
.	O

Inplace	O
dropping	O
seems	O
more	O
like	O
idiomatic	O
pandas	O
to	O
me	O
than	O
making	O
a	O
copy	O
only	O
to	O
instantly	O
garbage	O
collect	O
the	O
now-defunct	O
original	O
.	O

And	O
fortunately	O
,	O
these	O
days	O
,	O
`	O
-pylab	O
`	O
has	O
been	O
deprecated	O
and	O
using	O
`	O
--	O
matplotlib	O
`	O
and	O
importing	O
`	O
pylab	O
`	O
manually	O
is	O
encouraged	O
.	O

Are	O
you	O
sure	O
you	O
don't	O
mean	O
`	O
range	O
(	O
1	O
,	O
len	O
(	O
DF	O
)):	O
`	O
?	O

Where	O
"	O
timeblock	O
"	O
#1	O
will	O
include	O
the	O
first	O
4:59	O
minutes	O
of	O
observation	O
period	O
#1	O
,	O
#2	O
will	O
include	O
5:00	O
to	O
9:59	O
minutes	O
...	O
through	O
to	O
25:00	O
and	O
over	O
,	O
for	O
each	O
observation	O
period	O
.	O

It	O
seems	O
like	O
I'm	O
maybe	O
getting	O
confused	O
between	O
the	O
underlying	O
data	O
and	O
views	O
on	O
it	O
.	O

When	O
I	O
try	O
specifying	O
index_col=0	O
,	O
as	O
some	O
examples	O
in	O
the	O
documentation	O
do	O
,	O
I	O
get	O
a	O
"	O
IndexError	O
:	O
list	O
index	O
out	O
of	O
range	O
"	O
error	O
,	O
which	O
was	O
a	O
solution	O
to	O
several	O
related	O
questions	O
but	O
for	O
some	O
reason	O
isn't	O
working	O
for	O
me	O
.	O

That's	O
what	O
I	O
thought	O
about	O
my	O
original	O
code	O
but	O
for	O
some	O
reason	O
when	O
I	O
check	O
len	O
(	O
Sframe	O
)	O
at	O
the	O
end	O
in	O
the	O
main	O
code	O
,	O
it	O
still	O
has	O
the	O
duplicate	O
values	O
even	O
if	O
the	O
conditional	O
statement	O
applies	O
option	O
2	O
to	O
remove	O
duplicates	O
.	O

The	O
main	O
thing	O
I	O
need	O
to	O
do	O
is	O
to	O
group	O
the	O
days	O
by	O
week	O
such	O
that	O
I	O
can	O
get	O
sum	O
of	O
the	O
data	O
to	O
be	O
by	O
week	O
.	O

To	O
move	O
the	O
third	O
row	O
to	O
the	O
first	O
,	O
you	O
can	O
create	O
an	O
index	O
moving	O
the	O
target	O
row	O
to	O
the	O
first	O
element	O
.	O

So	O
using	O
your	O
approach	O
,	O
how	O
can	O
I	O
:	O
1	O
)	O
plot	O
the	O
scores	O
as	O
a	O
histogram	O
in	O
a	O
memory-conscious	O
way	O
,	O
and	O
2	O
)	O
extract	O
the	O
scores	O
belonging	O
to	O
the	O
certain	O
cell	O
types	O
to	O
plot	O
those	O
as	O
well	O
?	O

I	O
have	O
a	O
CSV	O
file	O
,	O
I	O
wanted	O
to	O
filter	O
it	O
where	O
I	O
keep	O
just	O
rows	O
where	O
I	O
have	O
values	O
in	O
row	O
"	O
d	O
"	O
bigger	O
then	O
0	O
.	O

Simulations	O
can	O
be	O
repeated	O
for	O
different	O
scenarios	O
and	O
each	O
one	O
of	O
these	O
scenarios	O
will	O
produce	O
a	O
different	O
hourly	O
set	O
of	O
data	O
for	O
each	O
room	O
and	O
each	O
variable	O
.	O

To	O
get	O
around	O
this	O
,	O
I'm	O
passing	O
in	O
a	O
large	O
number	O
for	O
the	O
max_results	O
parameter	O
and	O
specifying	O
a	O
chunksize	O
.	O

You	O
get	O
back	O
a	O
float	O
because	O
each	O
row	O
contains	O
a	O
mix	O
of	O
`	O
float	O
`	O
and	O
`	O
int	O
`	O
types	O
.	O

Should	O
I	O
use	O
something	O
different	O
.	O

The	O
best	O
would	O
be	O
to	O
convert	O
that	O
one	O
file	O
to	O
a	O
an	O
actual	O
comma	O
(	O
semicolon	O
or	O
other	O
)	O
separated	O
file	O
or	O
make	O
sure	O
that	O
compound	O
values	O
are	O
quoted	O
(	O
"	O
Alabama	O
County	O
")	O
and	O
then	O
specify	O
the	O
quotechar	O
:	O
#CODE	O

I	O
would	O
like	O
to	O
calculate	O
the	O
average	O
of	O
time	O
per	O
org	O
per	O
cluster	O
.	O

I	O
found	O
a	O
way	O
which	O
seems	O
very	O
inefficient	O
(	O
stacking	O
and	O
unstacking	O
which	O
will	O
create	O
many	O
many	O
columns	O
in	O
case	O
of	O
millions	O
of	O
categories	O
)	O
.	O

I	O
was	O
going	O
to	O
suggest	O
cumcount	O
and	O
tail	O
(	O
1	O
)	O
,	O
but	O
you're	O
after	O
something	O
else	O
(	O
these	O
would	O
be	O
much	O
faster	O
)	O
.	O

This	O
is	O
because	O
working	O
with	O
dictionaries	O
is	O
so	O
easy	O
and	O
thinking	O
of	O
them	O
like	O
simple	O
dicts	O
often	O
means	O
you	O
can	O
find	O
a	O
solution	O
to	O
an	O
issue	O
without	O
having	O
to	O
get	O
too	O
deep	O
into	O
pandas	O
.	O

If	O
you're	O
trying	O
to	O
slice	O
each	O
string	O
to	O
get	O
the	O
substring	O
from	O
5	O
to	O
7	O
,	O
you	O
need	O
a	O
`	O
:	O
`	O
,	O
not	O
a	O
`	O
,	O
`	O
:	O
#CODE	O

However	O
,	O
since	O
each	O
of	O
your	O
new	O
`	O
DataFrames	O
`	O
is	O
a	O
summary	O
of	O
a	O
single	O
customer	O
,	O
I	O
would	O
suggest	O
writing	O
one	O
function	O
that	O
can	O
return	O
all	O
of	O
your	O
desired	O
results	O
in	O
a	O
single	O
`	O
Series	O
`	O
.	O

I	O
want	O
to	O
get	O
statistics	O
of	O
debt_ratio	O
based	O
on	O
subgroups	O
of	O
market_capitalization	O
.	O

Any	O
ideas	O
why	O
this	O
error	O
might	O
be	O
showing	O
up	O
so	O
I	O
can	O
know	O
what	O
to	O
go	O
after	O
to	O
fix	O
?	O

I	O
believe	O
it	O
is	O
getting	O
at	O
what	O
I	O
want	O
.	O

I	O
haven't	O
done	O
any	O
stress	O
testing	O
but	O
I'd	O
imagine	O
this	O
could	O
get	O
slow	O
on	O
very	O
large	O
DataFrames	O
.	O

Is	O
there	O
a	O
quick	O
way	O
to	O
sort	O
my	O
data	O
by	O
a	O
given	O
column	O
that	O
only	O
takes	O
chunks	O
into	O
account	O
and	O
doesn't	O
require	O
loading	O
entire	O
datasets	O
in	O
memory	O
?	O

Take	O
a	O
look	O
at	O
the	O
regex	O
docs	O
.	O

Any	O
logic	O
requirements	O
(	O
like	O
comparing	O
elem+1	O
to	O
elem	O
)	O
should	O
be	O
in	O
your	O
question	O
so	O
there	O
is	O
no	O
confusion	O
.	O

I'd	O
like	O
to	O
get	O
a	O
list	O
as	O
`	O
[	O
'	O
abcd	O
'	O
,	O
'	O
ddse	O
'	O
,	O
'	O
123d	O
'	O
,	O
'	O
aaaaa*	O
']`	O
.	O

but	O
get	O
the	O
following	O
error	O
:	O

I	O
get	O
#CODE	O

Ideally	O
the	O
question	O
would	O
provide	O
a	O
self-contained	O
piece	O
of	O
code	O
generating	O
the	O
data	O
structure	O
,	O
or	O
even	O
just	O
something	O
like	O
`	O
df	O
=	O
[[	O
1	O
,	O
2	O
]	O
,	O
[	O
2	O
,	O
3	O
]	O
,	O
[	O
4	O
,	O
5	O
]]`	O
,	O
enough	O
to	O
try	O
to	O
get	O
an	O
answer	O
without	O
diving	O
into	O
panda	O
.	O

I'm	O
running	O
daily	O
simulations	O
in	O
a	O
batch	O
:	O
I	O
do	O
365	O
simluations	O
to	O
get	O
results	O
for	O
a	O
full	O
year	O
.	O

I	O
edited	O
my	O
answer	O
to	O
use	O
capwords	O
per	O
your	O
suggestion	O
,	O
that	O
fixed	O
the	O
problem	O
I	O
missed	O
where	O
it	O
capitalized	O
the	O
'	O
s	O
'	O
in	O
Guy's	O
Name	O
.	O

Does	O
not	O
work	O
exactly	O
right	O
since	O
categories	O
can	O
be	O
mixed	O
like	O
that	O
so	O
it	O
will	O
produce	O
more	O
duplications	O

Essentially	O
,	O
I	O
want	O
to	O
look	O
at	O
quintiles	O
(	O
since	O
there	O
are	O
5	O
days	O
in	O
a	O
business	O
week	O
)	O
rank	O
1	O
and	O
5	O
and	O
see	O
how	O
they	O
change	O
from	O
week	O
to	O
week	O
.	O

Doing	O
this	O
transformation	O
for	O
500,000	O
file	O
takes	O
time	O
.	O

The	O
requirement	O
isn't	O
easy	O
to	O
wrap	O
once	O
mind	O
around	O
,	O
so	O
sorry	O
If	O
I	O
am	O
not	O
being	O
clear	O
.	O

My	O
question	O
is	O
,	O
from	O
`	O
result	O
`	O
how	O
can	O
I	O
get	O
the	O
column	O
index	O
of	O
the	O
first	O
level	O
as	O
list	O
:	O
#CODE	O

This	O
function	O
will	O
then	O
work	O
out	O
the	O
maximum	O
,	O
minimum	O
,	O
and	O
return	O
rages	O
of	O
values	O
based	O
on	O
the	O
fact	O
I	O
want	O
5	O
categories	O
:	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
3	O
,	O
4	O
)	O
,	O
(	O
5	O
,	O
6	O
)	O
,	O
(	O
7	O
,	O
8)	O
,	O
(	O
9	O
,	O
10	O
)	O
.	O

Any	O
suggestions	O
of	O
a	O
better	O
way	O
?	O

In	O
addition	O
this	O
is	O
unlikely	O
to	O
be	O
only	O
time	O
I	O
have	O
to	O
do	O
this	O
so	O
being	O
able	O
to	O
change	O
the	O
numbers	O
and	O
columns	O
i'm	O
interested	O
in	O
would	O
be	O
good	O
.	O

And	O
at	O
that	O
time	O
i	O
think	O
the	O
state	O
becomes	O
bad	O
,	O
so	O
subsequent	O
calls	O
will	O
lead	O
to	O
exceptions	O
like	O
these	O
:	O
#CODE	O

Let	O
me	O
fix	O
that	O
and	O
return	O
to	O
this	O
issue	O
,	O
as	O
I	O
have	O
read	O
something	O
about	O
format	O
changes	O
between	O
10	O
and	O
12	O
.	O

All	O
the	O
data	O
,	O
columns	O
222	O
and	O
333	O
are	O
offset	O
as	O
required	O
,	O
but	O
it	O
isn't	O
even	O
the	O
same	O
size	O
as	O
the	O
output	O
in	O
the	O
first	O
order	O
.	O

But	O
I	O
can	O
only	O
get	O
this	O
:	O
#CODE	O

I	O
have	O
summary-level	O
data	O
of	O
the	O
count	O
of	O
people	O
by	O
age	O
group	O
,	O
city	O
,	O
income	O
and	O
the	O
industry	O
in	O
which	O
they	O
work	O
,	O
or	O
in	O
this	O
case	O
four	O
dimensions	O
.	O

@USER	O
:	O
Yes	O
,	O
it	O
can	O
as	O
I	O
said	O
,	O
"	O
They	O
all	O
suck	O
in	O
different	O
ways	O
"	O
.	O

Error	O
:	O
list	O
indices	O
must	O
be	O
integers	O
,	O
not	O
Series	O

Just	O
so	O
that	O
it	O
does	O
not	O
fail	O
silently	O
if	O
the	O
wrong	O
kind	O
of	O
data	O
is	O
passed	O
in	O
.	O

If	O
you	O
want	O
to	O
take	O
advantage	O
of	O
NumPy	O
/	O
Pandas	O
to	O
perform	O
fast	O
(	O
er	O
)	O
calculations	O
you	O
must	O
keep	O
the	O
data	O
in	O
a	O
NumPy	O
array	O
or	O
Pandas	O
NDFrame	O
.	O

Your	O
benchmark	O
is	O
actually	O
too	O
small	O
to	O
show	O
the	O
real	O
difference	O
.	O

I	O
was	O
able	O
to	O
resolve	O
this	O
by	O
opening	O
/	O
closing	O
a	O
connection	O
each	O
time	O
i	O
need	O
to	O
execute	O
a	O
query	O
.	O

I	O
used	O
a	O
chunksize	O
of	O
4	O
to	O
make	O
the	O
grouping	O
noticeable	O
on	O
the	O
small	O
dataset	O
;	O
you'll	O
want	O
to	O
change	O
it	O
to	O
90000	O
for	O
your	O
real	O
dataset	O
.	O

It's	O
mostly	O
trial	O
and	O
error	O
for	O
me	O
,	O
plus	O
knowledge	O
that	O
you	O
can	O
do	O
a	O
lot	O
just	O
with	O
```	O
rank	O
```	O
and	O
```	O
count	O
```	O
,	O
which	O
are	O
both	O
pretty	O
fast	O
.	O

And	O
those	O
columns	O
which	O
have	O
differing	O
values	O
:	O
#CODE	O

(	O
I	O
will	O
use	O
logistic	O
regression	O
and	O
random	O
forest	O
to	O
do	O
the	O
prediction	O
,	O
which	O
support	O
sparse	O
matrix	O
.	O
)	O
Is	O
there	O
anyway	O
to	O
efficiently	O
slicing	O
a	O
sparseDataFrame	O
or	O
for	O
the	O
whole	O
process	O
I	O
am	O
doing	O
,	O
it	O
should	O
be	O
improved	O
in	O
anyway	O
?	O

I	O
looked	O
here	O
,	O
but	O
when	O
I	O
ran	O
that	O
in	O
iPython	O
Notebook	O
,	O
I	O
don't	O
get	O
anything	O
.	O

does	O
not	O
produce	O
any	O
difference	O
in	O
terms	O
of	O
file	O
size	O
than	O
...	O

You	O
will	O
either	O
have	O
to	O
split	O
the	O
table	O
up	O
or	O
choose	O
another	O
storage	O
format	O
.	O

To	O
get	O
the	O
number	O
of	O
groups	O
you	O
can	O
use	O
the	O
ngroups	O
attribute	O
:	O
#CODE	O

this	O
does	O
not	O
seem	O
to	O
work	O
and	O
I	O
am	O
not	O
sure	O
if	O
it	O
just	O
is	O
not	O
possible	O
,	O
I	O
can	O
always	O
generate	O
separate	O
dictionaries	O
from	O
a	O
master	O
dictionary	O
to	O
get	O
around	O
having	O
to	O
update	O
data	O
in	O
multiple	O
locations	O

I	O
want	O
to	O
compare	O
these	O
two	O
data	O
frames	O
by	O
row	O
based	O
on	O
column	O
Value	O
and	O
keep	O
the	O
row	O
from	O
first	O
or	O
second	O
depending	O
where	O
the	O
Value	O
is	O
bigger	O
.	O

Then	O
,	O
you	O
can	O
run	O
`	O
sudo	O
port	O
install	O
py27-pandas	O
`	O
to	O
get	O
Python	O
and	O
all	O
of	O
the	O
dependencies	O
installed	O
.	O

See	O
the	O
shape	O
method	O
of	O
the	O
input	O
array	O
,	O
and	O
you	O
should	O
get	O
something	O
like	O
`	O
(	O
N	O
,	O
)`	O
and	O
not	O
`	O
(	O
N	O
,	O
1	O
)`	O
.	O

So	O
,	O
will	O
the	O
lower	O
value	O
always	O
come	O
first	O
,	O
or	O
could	O
that	O
change	O
?	O

and	O
get	O
the	O
following	O
error	O
:	O

In	O
order	O
to	O
have	O
the	O
index	O
the	O
exact	O
same	O
as	O
the	O
first	O
example	O
you'd	O
need	O
to	O
change	O
to	O
int	O
from	O
float	O
.	O

The	O
graph	O
bit	O
is	O
sorted	O
but	O
the	O
part	O
i'm	O
finding	O
hard	O
is	O
the	O
fact	O
the	O
column	O
headers	O
can	O
change	O
so	O
picking	O
up	O
their	O
data	O
without	O
manually	O
calling	O
them	O
is	O
something	O
I'm	O
unable	O
to	O
do	O
.	O

If	O
pytables	O
used	O
msgpack	O
it	O
would	O
be	O
easier	O
for	O
other	O
languages	O
to	O
read	O
the	O
data	O
but	O
obviously	O
their	O
target	O
is	O
python	O
.	O

The	O
company	O
name	O
may	O
be	O
variable	O
length	O
,	O
it	O
will	O
however	O
always	O
be	O
after	O
the	O
first	O
`	O
\s	O
`	O

To	O
get	O
the	O
same	O
form	O
of	O
broadcasting	O
to	O
occur	O
like	O
the	O
diagram	O
above	O
shows	O
we	O
have	O
to	O
decompose	O
to	O
numpy	O
arrays	O
which	O
then	O
become	O
anonymous	O
data	O
:	O
#CODE	O

I	O
want	O
to	O
generate	O
a	O
plot	O
showing	O
these	O
dates	O
graphically	O
.	O

How	O
is	O
it	O
possible	O
to	O
get	O
the	O
label	O
of	O
value	O
'	O
12	O
'	O
?	O

You	O
shall	O
note	O
that	O
`	O
and	O
`	O
and	O
`	O
or	O
`	O
are	O
not	O
appropriate	O
for	O
a	O
vector	O
of	O
booleans	O
,	O
use	O
`	O
`	O
and	O
`	O
|	O
`	O
instead	O
.	O

No	O
,	O
this	O
table	O
is	O
used	O
by	O
a	O
lot	O
of	O
other	O
code	O
(	O
mostly	O
C#	O
)	O
,	O
I	O
am	O
just	O
doing	O
some	O
data	O
analysis	O
on	O
it	O
from	O
Python	O
,	O
so	O
I'm	O
not	O
in	O
a	O
position	O
to	O
change	O
the	O
semantics	O
/	O
data	O
structure	O
.	O

#	O
Valid	O
positions	O
in	O
output	O
array	O
to	O
be	O
changed	O

Is	O
there	O
a	O
bug	O
in	O
my	O
code	O
or	O
is	O
there	O
another	O
reason	O
for	O
the	O
huge	O
computation	O
speed	O
difference	O
between	O
those	O
two	O
lines	O
of	O
code	O
?	O

I	O
also	O
used	O
a	O
longer	O
window	O
because	O
there	O
were	O
only	O
15	O
values	O
per	O
array	O
but	O
you	O
seemed	O
to	O
be	O
planning	O
on	O
using	O
the	O
last	O
50	O
days	O
.	O

I	O
am	O
trying	O
to	O
get	O
the	O
data	O
into	O
the	O
following	O
shape	O
:	O
#CODE	O

I	O
then	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
want	O
to	O
:	O
plot	O
a	O
heatmap	O
of	O
x	O
,	O
y	O
and	O
the	O
colour	O
is	O
the	O
z	O
value	O
..	O

I'm	O
also	O
running	O
python	O
3.4	O
and	O
I	O
didn't	O
get	O
any	O
warning	O
when	O
I	O
ran	O
your	O
code	O
exactly	O
as	O
is	O
.	O

This	O
does	O
it	O
in	O
a	O
one	O
liner	O
but	O
is	O
not	O
so	O
readable	O
,	O
basically	O
it	O
tests	O
where	O
the	O
value	O
counts	O
for	O
each	O
column	O
is	O
equal	O
to	O
1	O
,	O
filters	O
the	O
resultant	O
list	O
out	O
and	O
uses	O
the	O
index	O
as	O
a	O
boolean	O
indec	O
:	O
#CODE	O

I	O
still	O
think	O
pandas	O
is	O
not	O
correctly	O
handling	O
your	O
empty	O
column	O
and	O
you	O
end	O
up	O
with	O
either	O
with	O
5	O
columns	O
,	O
or	O
with	O
6	O
columns	O
,	O
but	O
shifted	O
one	O
to	O
the	O
left	O
.	O

However	O
,	O
I	O
am	O
receiving	O
this	O
warning	O
`	O
/	O
usr	O
/	O
local	O
/	O
lib	O
/	O
python2.7	O
/	O
dist-packages	O
/	O
pandas	O
/	O
core	O
/	O
index	O
.	O

It	O
looks	O
like	O
this	O
changed	O
at	O
some	O
point	O
;	O
maybe	O
he	O
has	O
an	O
old	O
version	O
of	O
pandas	O
where	O
S	O
and	O
Sec	O
are	O
no	O
good	O
.	O

I	O
have	O
also	O
heard	O
of	O
Orange	O
library	O
for	O
imputation	O
,	O
but	O
haven't	O
had	O
a	O
chance	O
to	O
use	O
it	O
yet	O
.	O

I	O
managed	O
to	O
find	O
how	O
this	O
is	O
almost	O
done	O
:	O
#CODE	O

For	O
this	O
purpose	O
you	O
need	O
pandas	O
-	O
most	O
popular	O
python	O
package	O
for	O
working	O
with	O
timeseries	O
and	O
another	O
analytic	O
data	O
.	O

well	O
,	O
what	O
I	O
am	O
trying	O
to	O
do	O
is	O
to	O
have	O
all	O
my	O
results	O
ready	O
out	O
of	O
mysql	O
,	O
and	O
then	O
do	O
different	O
types	O
of	O
merging	O
to	O
get	O
my	O
plots	O
.	O

I	O
agree	O
with	O
Chang	O
that	O
it	O
would	O
help	O
to	O
have	O
a	O
very	O
clear	O
example	O
of	O
how	O
the	O
exact	O
alignment	O
should	O
be	O
.	O

You	O
mention	O
in	O
update	O
2	O
above	O
that	O
you	O
want	O
to	O
get	O
the	O
columns	O
and	O
the	O
only	O
way	O
is	O
opening	O
the	O
hdf	O
.	O

So	O
where	O
those	O
indices	O
don't	O
match	O
up	O
(	O
50	O
,	O
and	O
51	O
)	O
,	O
you	O
get	O
`	O
NaN	O
`	O
as	O
I	O
would	O
hope	O
.	O

However	O
,	O
if	O
I	O
save	O
it	O
as	O
a	O
csv	O
file	O
and	O
reload	O
it	O
again	O
,	O
I	O
got	O
error	O
message	O
and	O
the	O
plot	O
is	O
not	O
quite	O
right	O
either	O
,	O
#CODE	O

My	O
main	O
goal	O
is	O
to	O
match	O
the	O
index	O
value	O
from	O
`	O
ds2	O
`	O
into	O
`	O
ds1	O
`	O
and	O
replace	O
it	O
with	O
corresponding	O
value	O
,	O
so	O
the	O
output	O
would	O
look	O
like	O
#CODE	O

That	O
would	O
be	O
a	O
possibility	O
but	O
the	O
problem	O
is	O
that	O
each	O
frame5	O
has	O
a	O
different	O
index	O
.	O

First	O
,	O
you	O
need	O
some	O
kind	O
of	O
mapping	O
of	O
what	O
makes	O
up	O
each	O
level	O
.	O

PyTables	O
3.1	O
was	O
just	O
released	O
that	O
changes	O
the	O
file	O
caching	O
mechanism	O
at	O
least	O
on	O
a	O
lower	O
HDF5	O
version	O
,	O
do	O
to	O
see	O
your	O
version	O
:	O
#CODE	O

backfilling	O
data	O
from	O
one	O
column	O
into	O
another	O

Also	O
,	O
I	O
know	O
I	O
am	O
missing	O
patterns	O
that	O
may	O
be	O
useful	O
because	O
if	O
a	O
pattern	O
exists	O
between	O
Variable_1	O
and	O
Variable_2	O
and	O
Variable_3	O
and	O
Variable_4	O
are	O
missing	O
completely	O
at	O
random	O
,	O
then	O
concatenating	O
them	O
as	O
strings	O
will	O
not	O
capture	O
the	O
pattern	O
between	O
Variable_1	O
and	O
Variable_2	O
.	O

Do	O
you	O
know	O
what	O
is	O
the	O
difference	O
in	O
this	O
case	O
between	O
both	O
?	O

The	O
goal	O
is	O
to	O
take	O
the	O
2x2	O
piece	O
of	O
df	O
with	O
index	O
(	O
4	O
,	O
5	O
)	O
and	O
columns	O
(	O
'	O
date	O
'	O
,	O
'	O
val	O
')	O
and	O
replace	O
it	O
with	O
a	O
same-shaped	O
,	O
same-typed	O
2x2	O
block	O
.	O

Convert	O
Matrix	O
format	O
to	O
Column	O
in	O
Pandas	O

`	O
nan	O
`	O
is	O
commonly	O
used	O
for	O
this	O
purpose	O
,	O
but	O
here	O
I'm	O
actually	O
just	O
using	O
the	O
time	O
that	O
was	O
already	O
there	O
if	O
there	O
isn't	O
a	O
new	O
one	O
defined	O
for	O
it	O
in	O
the	O
`	O
time_map	O
`	O
`	O
dict	O
`	O
.	O

I	O
want	O
to	O
drop	O
all	O
values	O
after	O
index	O
`	O
5	O
`	O
because	O
it	O
has	O
no	O
values	O
,	O
but	O
not	O
index	O
`	O
2	O
`	O
,	O
`	O
3	O
`	O
.	O

You	O
should	O
be	O
able	O
to	O
uninstall	O
Anaconda	O
(	O
it	O
is	O
only	O
a	O
directory	O
)	O
to	O
reverse	O
any	O
changes	O
.	O

Since	O
it	O
is	O
a	O
very	O
large	O
data	O
frame	O
,	O
I	O
think	O
it	O
might	O
be	O
inefficient	O
to	O
do	O
a	O
loop	O
and	O
row	O
by	O
row	O
extraction	O
.	O

Since	O
the	O
NumPy	O
array	O
has	O
no	O
index	O
,	O
there	O
should	O
be	O
no	O
"	O
Unalignable	O
boolean	O
Series	O
"	O
problem	O
.	O

My	O
guess	O
is	O
that	O
I	O
am	O
either	O
not	O
applying	O
the	O
functions	O
correctly	O
for	O
a	O
column	O
or	O
the	O
values	O
I	O
am	O
getting	O
arent	O
integers	O
.	O

which	O
gives	O
you	O
your	O
date	O
as	O
a	O
list	O
arranged	O
in	O
the	O
order	O
of	O
importance	O
...	O

I'm	O
not	O
sure	O
what	O
the	O
difference	O
was	O
.	O

The	O
mongodb	O
collection	O
contains	O
sensor	O
values	O
tagged	O
with	O
date	O
and	O
time	O
.	O

This	O
should	O
not	O
make	O
any	O
difference	O
.	O

I	O
would	O
like	O
to	O
split	O
that	O
file	O
into	O
files	O
of	O
len	O
(	O
index	O
)	O
=	O
2	O
,	O
using	O
linux	O
:	O
#CODE	O

You	O
can	O
coerce	O
the	O
response	O
into	O
a	O
data	O
frame	O
after	O
you	O
get	O
it	O
.	O

Oh	O
wait	O
,	O
your	O
matrix	O
must	O
already	O
be	O
in	O
the	O
form	O
of	O
differences	O
from	O
the	O
mean	O
(	O
by	O
column	O
)	O
?	O

Any	O
suggestions	O
?	O

I	O
get	O
the	O
following	O
error	O
:	O
#CODE	O

I	O
am	O
curious	O
why	O
doing	O
`	O
unique_df	O
[	O
i	O
]	O
=	O
"	O
AAA	O
"`	O
no	O
longer	O
modifies	O
the	O
data	O
frame	O
values	O
.	O

Unfortunately	O
I	O
get	O
an	O
error	O
,	O
and	O
the	O
shading	O
doesn't	O
work	O
.	O

The	O
range	O
of	O
the	O
values	O
in	O
`	O
megaball	O
`	O
are	O
from	O
1	O
to	O
25	O
,	O
and	O
this	O
line	O
of	O
code	O
:	O
#CODE	O

yes	O
it	O
works	O
fine	O
but	O
I	O
need	O
to	O
drop	O
'	O
2014-07-16	O
14:24	O
'	O
thnx	O

If	O
you	O
have	O
more	O
pressing	O
things	O
to	O
do	O
,	O
you	O
could	O
temporarily	O
rename	O
it	O
out	O
of	O
the	O
way	O
to	O
get	O
through	O
your	O
installs	O
,	O
then	O
rename	O
it	O
back	O
.	O

As	O
you	O
can	O
see	O
,	O
the	O
lines	O
overlap	O
perfectly	O
for	O
the	O
days	O
where	O
there	O
is	O
data	O
:	O
no	O
original	O
data	O
is	O
'	O
changed	O
'	O
.	O

I	O
have	O
a	O
massive	O
file	O
with	O
per	O
timestamp	O
survey	O
data	O
from	O
about	O
thousands	O
of	O
different	O
people	O
and	O
over	O
20	O
different	O
locations	O
.	O

So	O
if	O
you	O
have	O
a	O
million	O
items	O
that	O
,	O
on	O
average	O
,	O
belong	O
to	O
three	O
categories	O
each	O
,	O
then	O
you	O
need	O
storage	O
for	O
the	O
million	O
items	O
plus	O
three	O
million	O
references	O
.	O

Given	O
how	O
the	O
sample	O
was	O
built	O
,	O
there	O
was	O
a	O
need	O
to	O
weight	O
adjust	O
the	O
respondent	O
data	O
so	O
that	O
not	O
every	O
one	O
is	O
deemed	O
as	O
"	O
equal	O
"	O
when	O
performing	O
the	O
analysis	O
.	O

What	O
I'm	O
hoping	O
to	O
achieve	O
is	O
knowing	O
where	O
the	O
first	O
/	O
last	O
row	O
of	O
trimmed	O
data	O
is	O
located	O
so	O
I	O
can	O
set	O
up	O
a	O
for-loop	O
to	O
go	O
through	O
the	O
data	O
and	O
perform	O
mathematical	O
calculations	O
with	O
the	O
values	O
and	O
then	O
send	O
those	O
results	O
back	O
into	O
new	O
columns	O
attached	O
directly	O
to	O
the	O
same	O
date	O
as	O
the	O
date	O
in	O
question	O
.	O

Ultimately	O
I	O
want	O
to	O
be	O
able	O
to	O
loop	O
through	O
the	O
json	O
to	O
display	O
the	O
dates	O
and	O
corresponding	O
values	O
,	O
but	O
I	O
cant	O
do	O
that	O
until	O
this	O
error	O
is	O
no	O
longer	O
happening	O
.	O

They	O
are	O
however	O
extremely	O
useful	O
once	O
you	O
get	O
to	O
grips	O
with	O
them	O
.	O

I	O
know	O
the	O
values	O
within	O
the	O
CSV	O
are	O
not	O
all	O
"	O
NaN	O
"	O
so	O
why	O
does	O
the	O
output	O
looks	O
like	O
this	O
and	O
how	O
can	O
I	O
get	O
the	O
correct	O
output	O
with	O
the	O
numbers	O
in	O
reach	O
of	O
the	O
rows	O
?	O

I've	O
filtered	O
my	O
data	O
as	O
suggested	O
here	O
:	O
With	O
Pandas	O
in	O
Python	O
,	O
select	O
the	O
highest	O
value	O
row	O
for	O
each	O
group	O
#CODE	O

I	O
was	O
able	O
to	O
get	O
the	O
more	O
precise	O
value	O
in	O
my	O
previous	O
environment	O
by	O
doing	O
the	O
incremental	O
update	O
to	O
cumulative	O
mean	O
instead	O
of	O
taking	O
a	O
batch	O
sum	O
and	O
divide	O
.	O

Then	O
use	O
the	O
`	O
~	O
`	O
to	O
flip	O
the	O
bools	O
.	O

The	O
above	O
entire	O
expression	O
is	O
therefore	O
evaluating	O
to	O
an	O
array	O
of	O
truth	O
values	O
,	O
rather	O
than	O
a	O
single	O
`	O
True	O
`	O
/	O
`	O
False	O
`	O
.	O

For	O
the	O
multiprocessing	O
:	O
You	O
can	O
distribute	O
the	O
data	O
sets	O
across	O
cores	O
,	O
do	O
`	O
partial_fit	O
`	O
,	O
get	O
the	O
weight	O
vectors	O
,	O
average	O
them	O
,	O
distribute	O
them	O
to	O
the	O
estimators	O
,	O
do	O
partial	O
fit	O
again	O
.	O

the	O
size	O
of	O
Y	O
is	O
100e6	O
x	O
1	O

Put	O
I	O
think	O
that	O
following	O
this	O
route	O
would	O
lead	O
to	O
an	O
inefficient	O
solution	O
.	O

Your	O
example	O
come	O
at	O
a	O
good	O
time	O
for	O
me	O
,	O
so	O
I	O
now	O
have	O
something	O
concrete	O
to	O
train	O
with	O
.	O

One	O
thing	O
I	O
find	O
very	O
confortable	O
with	O
Numpy	O
is	O
the	O
vectorization	O
of	O
operations	O
with	O
arrays	O
(	O
ie	O
.	O
the	O
absence	O
of	O
any	O
explicit	O
looping	O
)	O
,	O
and	O
the	O
implicit	O
element-by-element	O
behavior	O
of	O
operations	O
.	O

I	O
suggest	O
to	O
set	O
it	O
to	O
some	O
reasonable	O
upper	O
limit	O
,	O
though	O
.	O

Gonna	O
try	O
to	O
find	O
another	O
solution	O
.	O

Is	O
it	O
possible	O
to	O
construct	O
a	O
`	O
numpy	O
`	O
matrix	O
from	O
a	O
function	O
?	O

I	O
have	O
2D	O
numpy	O
array	O
,	O
with	O
example	O
shape	O
:	O
#CODE	O

`	O
grid	O
[	O
0	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
i	O
`	O
,	O
and	O

Thank	O
you	O
Martijn	O
:)	O
-	O
your	O
are	O
BIG	O
help	O
,	O
and	O
just	O
one	O
thing	O
confuses	O
me	O
,	O
how	O
do	O
I	O
tell	O
python	O
to	O
read	O
all	O
CDR	O
records	O
if	O
record	O
is	O
907	O
bytes	O
long	O
.	O

I	O
want	O
to	O
get	O
the	O
elements	O
of	O
a	O
`	O
numpy	O
`	O
array	O
using	O
an	O
index	O
array	O
like	O
so	O
#CODE	O

I	O
can	O
weight	O
them	O
how	O
I	O
want	O
to	O
as	O
long	O
as	O
sum	O
of	O
their	O
weights	O
adds	O
to	O
1	O
.	O

I	O
wanted	O
to	O
try	O
to	O
duplicate	O
those	O
performance	O
gains	O
when	O
solving	O
the	O
distance	O
between	O
two	O
equal	O
sized	O
arrays	O
.	O

Even	O
if	O
it	O
worked	O
,	O
I	O
would	O
not	O
expect	O
any	O
speed-up	O
from	O
this	O
compared	O
to	O
an	O
ordinary	O
loop	O
,	O
since	O
it	O
needs	O
to	O
call	O
a	O
Python	O
function	O
for	O
every	O
entry	O
.	O

@USER	O
,	O
you're	O
right	O
,	O
if	O
you	O
have	O
to	O
convert	O
everything	O
to	O
ndarrays	O
it's	O
often	O
not	O
worth	O
it	O
.	O

An	O
example	O
implementation	O
without	O
recalculating	O
the	O
distance	O
array	O
would	O
be	O
this	O
:	O
#CODE	O

I	O
need	O
to	O
return	O
all	O
of	O
the	O
points	O
within	O
a	O
distance	O
of	O
X	O
units	O
from	O
every	O
point	O
.	O

EDIT	O
:	O
Actually	O
renaming	O
my	O
package	O
does	O
not	O
fix	O
it	O
.	O

2	O
)	O
look	O
at	O
the	O
lengths	O
distance	O
(	O
point	O
,	O
centre	O
,	O
metric=	O
...	O
)	O
of	O
all	O
the	O
rays	O
.	O

Sorry	O
,	O
all	O
are	O
positive	O
values	O
greater	O
than	O
0	O
.	O

After	O
that	O
I	O
convert	O
the	O
image	O
to	O
BGR	O
model	O
:	O
#CODE	O

How	O
do	O
I	O
standardize	O
a	O
matrix	O
?	O

Speed	O
can	O
probably	O
be	O
increased	O
by	O
ensuring	O
that	O
the	O
record	O
array	O
you	O
pass	O
to	O
Cython	O
is	O
contiguous	O
.	O

fid	O
is	O
the	O
file	O
currently	O
being	O
looked	O
at	O

I'm	O
guessing	O
it's	O
opening	O
TWO	O
filehandles	O
per	O
iteration	O
,	O
just	O
based	O
on	O
the	O
498	O
(	O
a	O
bit	O
less	O
than	O
half	O
1024	O
,	O
and	O
Python	O
would	O
have	O
some	O
files	O
open	O
itself	O
(	O
maybe	O
25-odd	O
?	O
)	O
.	O

The	O
idea	O
is	O
to	O
count	O
the	O
number	O
of	O
occurrences	O
of	O
each	O
transition	O
,	O
and	O
use	O
the	O
counts	O
in	O
a	O
vectorized	O
update	O
of	O
the	O
matrix	O
.	O

I	O
kept	O
them	O
in	O
to	O
distinguish	O
them	O
from	O
the	O
`	O
math	O
`	O
ones	O
,	O
which	O
won't	O
work	O
for	O
this	O
approach	O
.	O

Powers	O
of	O
two	O
are	O
simple	O
to	O
compute	O
,	O
but	O
mixed	O
radix	O
sizes	O
can	O
be	O
faster	O
and	O
use	O
less	O
memory	O
.	O

The	O
stars	O
/	O
dots	O
are	O
the	O
`	O
X	O
`	O
and	O
`	O
Y	O
`	O
plotted	O
with	O
two	O
modifications	O
,	O
I	O
removed	O
the	O
first	O
position	O
and	O
added	O
a	O
false	O
one	O
to	O
make	O
this	O
a	O
full	O
example	O
of	O
the	O
sought	O
algorithm	O
.	O

Please	O
look	O
at	O
my	O
EDIT	O
2	O
,	O
where	O
I	O
described	O
my	O
problem	O
with	O
input	O
data	O
...	O
and	O
why	O
I	O
can't	O
get	O
matrix	O
..	O

pyqt	O
:	O
Convert	O
numpy	O
array	O
to	O
QImage	O

To	O
find	O
the	O
difference	O
between	O
your	O
data	O
and	O
a	O
point	O
,	O
you'd	O
just	O
do	O
`	O
data	O
-	O
point	O
`	O
.	O

Unfortunately	O
when	O
numpy	O
reads	O
the	O
19-digit	O
number	O
as	O
a	O
floating	O
point	O
number	O
,	O
there	O
is	O
not	O
enough	O
precision	O
to	O
get	O
all	O
the	O
significant	O
digits	O
,	O
so	O
there	O
is	O
a	O
rounding	O
error	O
.	O

The	O
exceptions	O
are	O
very	O
rare	O
,	O
if	O
any	O
.	O

I	O
can't	O
reproduce	O
your	O
problem	O
on	O
Linux	O
using	O
the	O
same	O
versions	O
of	O
numpy	O
and	O
python	O
and	O
a	O
quickly	O
made	O
test	O
file	O
(	O
with	O
dos	O
line	O
endings	O
,	O
even	O
)	O
...	O

I	O
imagine	O
I	O
would	O
have	O
to	O
use	O
the	O
uncompiled	O
source	O
provided	O
from	O
each	O
of	O
these	O
three	O
projects	O
.	O

However	O
,	O
I	O
am	O
checking	O
optimization	O
routine	O
result	O
,	O
and	O
sometimes	O
power	O
is	O
negative	O
,	O
sometimes	O
it	O
is	O
positive	O
.	O

What	O
about	O
array	O
of	O
arrays	O
that	O
contains	O
some	O
structures	O
?	O

The	O
y	O
data	O
takes	O
the	O
shape	O
of	O
the	O
triangle	O
wave	O
below	O
.	O

There	O
are	O
some	O
algorithm	O
to	O
calculate	O
faster	O
the	O
results	O
for	O
low	O
valued	O
matrix	O
,	O
but	O
just	O
google	O
for	O
this	O
.	O

Those	O
are	O
not	O
random	O
replacements	O
by	O
any	O
means	O
.	O

I	O
would	O
suggest	O
to	O
make	O
the	O
library	O
use	O
an	O
(	O
NumPy-	O
)	O
array	O
you	O
allocate	O
in	O
Python	O
and	O
pass	O
on	O
to	O
the	O
library	O
.	O

For	O
the	O
simple	O
case	O
of	O
"	O
remove	O
column	O
3	O
"	O
,	O
`	O
delete	O
`	O
makes	O
more	O
sense	O
;	O
for	O
a	O
more	O
complicated	O
case	O
,	O
`	O
take	O
`	O
probably	O
makes	O
more	O
sense	O
.	O

I	O
have	O
an	O
array	O
of	O
points	O
in	O
numpy	O
:	O
#CODE	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

How	O
do	O
I	O
find	O
the	O
length	O
(	O
or	O
dimensions	O
,	O
size	O
)	O
of	O
a	O
numpy	O
matrix	O
in	O
python	O
?	O

It's	O
longer	O
than	O
the	O
other	O
answer	O
but	O
is	O
more	O
generic	O
(	O
can	O
be	O
used	O
with	O
values	O
that	O
are	O
not	O
strings	O
)	O
.	O

I	O
coded	O
my	O
own	O
routine	O
with	O
Python	O
/	O
Numpy	O
,	O
and	O
it	O
is	O
giving	O
me	O
a	O
little	O
bit	O
different	O
results	O
from	O
the	O
MATLAB	O
code	O
somebody	O
else	O
did	O
,	O
and	O
I	O
am	O
having	O
hard	O
time	O
finding	O
out	O
where	O
it	O
is	O
coming	O
from	O
because	O
of	O
different	O
random	O
draws	O
.	O

How	O
can	O
1,000,000	O
4-byte	O
ints	O
be	O
compressed	O
any	O
smaller	O
?	O

If	O
this	O
number	O
is	O
less	O
than	O
a	O
third	O
of	O
the	O
total	O
I'll	O
use	O
my	O
answer	O
above	O
.	O

I	O
have	O
done	O
7	O
of	O
the	O
problems	O
on	O
Project	O
Euler	O
(	O
nothing	O
to	O
brag	O
about	O
,	O
but	O
it	O
might	O
give	O
you	O
a	O
better	O
idea	O
of	O
where	O
I	O
stand	O
in	O
skills	O
)	O
.	O

Apologies	O
if	O
this	O
is	O
a	O
wrongly	O
framed	O
question	O
or	O
if	O
this	O
question	O
was	O
already	O
asked	O
earlier	O
(	O
I	O
couldn't	O
find	O
it	O
)	O

If	O
you	O
can	O
choose	O
,	O
I	O
strongly	O
recommend	O
pandas	O
:	O
it	O
has	O
"	O
column	O
indexing	O
"	O
built-in	O
plus	O
a	O
lot	O
of	O
other	O
features	O
.	O

But	O
this	O
will	O
iterate	O
through	O
the	O
entire	O
array	O
and	O
allocate	O
a	O
new	O
array	O
in	O
memory	O
containing	O
the	O
all	O
the	O
results	O
,	O
and	O
only	O
then	O
check	O
to	O
see	O
if	O
it	O
is	O
empty	O
.	O

Since	O
some	O
askers	O
and	O
some	O
answers	O
both	O
avoid	O
that	O
constraint	O
,	O
I	O
encourage	O
anyone	O
who's	O
here	O
and	O
doesn't	O
mind	O
having	O
PIL	O
to	O
look	O
below	O
,	O
and	O
any	O
non-PIL	O
answers	O
(	O
new	O
or	O
old	O
)	O
to	O
mention	O
that	O
they're	O
a	O
PIL-is-used	O
type	O
of	O
answer	O
,	O
to	O
distinguish	O
themselves	O
from	O
answers	O
meeting	O
the	O
original	O
constraint	O
.	O

As	O
I	O
understand	O
your	O
question	O
,	O
you	O
have	O
a	O
2D	O
array	O
of	O
"	O
z	O
"	O
values	O
that	O
ranges	O
from	O
some	O
xmin	O
to	O
xmax	O
,	O
and	O
ymin	O
to	O
ymax	O
in	O
each	O
direction	O
.	O

The	O
covariance	O
matrix	O
of	O
a	O
dataset	O
A	O
is	O
:	O
1	O
/(	O
N-1	O
)	O
*	O
AA^T	O

I	O
have	O
a	O
large	O
(	O
500k	O
by	O
500k	O
)	O
,	O
sparse	O
matrix	O
.	O

I	O
can't	O
comment	O
on	O
a	O
numpy	O
array	O
as	O
I	O
haven't	O
used	O
one	O
before	O
,	O
but	O
for	O
using	O
a	O
list	O
of	O
lists	O
Python	O
already	O
has	O
built	O
in	O
support	O
.	O

If	O
`	O
finer_fxy	O
`	O
is	O
stored	O
in	O
the	O
probably-default	O
`	O
float64	O
`	O
s	O
,	O
this	O
would	O
take	O
about	O
64	O
GiB	O
of	O
memory	O
;	O
not	O
surprising	O
that	O
you're	O
running	O
out	O
.	O

Sebastian's	O
solution	O
for	O
a	O
way	O
around	O
the	O
integer-values-only	O
restriction	O
and	O
big-values	O
problem	O
.	O

This	O
allows	O
the	O
column	O
to	O
hold	O
float	O
values	O
at	O
first	O
,	O
and	O
strings	O
later	O
.	O

Efficient	O
slicing	O
of	O
matrices	O
using	O
matrix	O
multiplication	O
,	O
with	O
Python	O
,	O
NumPy	O
,	O
SciPy	O

Is	O
there	O
a	O
more	O
compact	O
way	O
to	O
operate	O
on	O
array	O
elements	O
,	O
without	O
having	O
to	O
use	O
the	O
standard	O
for	O
loop	O
.?	O

Please	O
look	O
at	O
this	O
answer	O
:	O
#URL	O

When	O
I	O
tried	O
this	O
,	O
I	O
got	O
sort	O
of	O
similar	O
shaped	O
"	O
tiles	O
"	O
of	O
different	O
colors	O
rather	O
than	O
3	O
Gaussian	O
humps	O
.	O

I	O
created	O
the	O
first	O
array	O
like	O
this	O
:	O

(	O
Note	O
that	O
I	O
can't	O
imagine	O
any	O
reason	O
why	O
this	O
should	O
be	O
necessary	O
.	O
)	O

SOLUTION	O
:	O
i	O
have	O
some	O
scattered	O
points	O
(	O
i	O
don't	O
know	O
how	O
many	O
)	O
and	O
i	O
want	O
to	O
reduce	O
it	O
to	O
a	O
8	O
meaning	O
point	O
.	O
one	O
of	O
the	O
technique	O
i	O
can	O
use	O
is	O
to	O
clusterize	O
them	O
with	O
some	O
cluster	O
algorithms	O
.	O

that	O
blas	O
is	O
reference	O
blas	O
from	O
netlib	O
-	O
the	O
slowest	O
blas	O
around	O
.	O
install	O
atlas	O
or	O
mkl	O
instead	O
.	O

EDIT	O
:	O
Answer	O
updated	O
for	O
a	O
2D	O
array	O
.	O

But	O
you	O
lose	O
a	O
lot	O
of	O
NumPy	O
power	O
that	O
way	O
.	O

Because	O
I	O
view	O
doesn't	O
really	O
have	O
to	O
do	O
with	O
filtering	O
,	O
but	O
rather	O
with	O
different	O
representation	O
of	O
the	O
same	O
data	O
.	O

@USER	O
,	O
not	O
sure	O
what	O
you	O
mean	O
by	O
"	O
changing	O
original	O
values	O
"	O
.	O

this	O
could	O
also	O
be	O
achieved	O
elegantly	O
with	O
numpy's	O
`	O
where	O
`	O
function	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

In	O
the	O
following	O
trivial	O
function	O
,	O
I	O
have	O
declared	O
the	O
numpy	O
array	O
argument	O
`	O
arr	O
`	O
using	O
the	O
buffer	O
syntax	O
.	O

I	O
remember	O
that	O
there	O
was	O
a	O
smart	O
trick	O
about	O
turning	O
on	O
and	O
off	O
the	O
right	O
intersections	O
of	O
rows	O
and	O
columns	O
to	O
turn	O
off	O
one-by-one	O
all	O
the	O
lightbulbs	O
,	O
but	O
it	O
wont	O
come	O
back	O
to	O
my	O
mind	O
...	O

However	O
this	O
code	O
is	O
to	O
slow	O
in	O
the	O
current	O
version	O
,	O
and	O
I	O
am	O
wondering	O
wheater	O
there	O
is	O
a	O
faster	O
solution	O
.	O
thanks	O
!	O

This	O
would	O
probably	O
be	O
the	O
most	O
efficient	O
way	O
to	O
access	O
a	O
numpy	O
array	O
stored	O
on	O
disk	O
.	O

hmmmmm	O
,	O
probably	O
it	O
will	O
help	O
some	O
others	O
to	O
sort	O
dictionarys	O
or	O
to	O
prevent	O
from	O
using	O
commands	O
like	O
sorted	O
=	O
sorted	O
(	O
...	O
)	O
.	O

The	O
ticket	O
simply	O
spoke	O
of	O
random	O
number	O
seeding	O
with	O
64-bit	O
,	O
perhaps	O
its	O
referring	O
to	O
a	O
different	O
random	O
number	O
generator	O
.	O

Not	O
really	O
elegant	O
at	O
all	O
but	O
you	O
can	O
get	O
close	O
to	O
what	O
you	O
want	O
using	O
a	O
tuple	O
to	O
store	O
pointers	O
to	O
the	O
arrays	O
.	O

For	O
example	O
I	O
am	O
looking	O
for	O
4.2	O
but	O
I	O
know	O
in	O
the	O
array	O
there	O
is	O
no	O
4.2	O
but	O
I	O
want	O
to	O
return	O
the	O
index	O
of	O
the	O
value	O
4.1	O
instead	O
of	O
4.4	O
.	O

Print	O
'	O
Length	O
of	O
together	O
'	O
goes	O
just	O
before	O
the	O
matrix	O
line	O
.	O

solve	O
a	O
nonlinear	O
equation	O
at	O
several	O
intermediate	O
points	O
of	O
a	O
calculation	O
,	O
not	O
just	O
as	O
the	O
final	O
result	O
.	O

Find	O
where	O
they're	O
located	O
at	O
(	O
assumes	O
the	O
data	O
is	O
sorted	O
!!	O
):	O
#CODE	O

You	O
need	O
Python	O
to	O
keep	O
track	O
of	O
your	O
vector	O
so	O
that	O
it	O
can	O
be	O
deleted	O
*	O
after	O
*	O
the	O
numpy	O
array	O
.	O

I	O
find	O
that	O
I	O
have	O
to	O
first	O
build	O
a	O
list	O
and	O
then	O
cast	O
it	O
(	O
using	O
"	O
array	O
")	O
to	O
an	O
array	O
.	O

I	O
have	O
an	O
numpy	O
one	O
dimensional	O
array	O
c	O
that	O
is	O
supposed	O
to	O
be	O
filled	O
with	O
the	O
contents	O
of	O

but	O
the	O
issue	O
now	O
,	O
when	O
I	O
am	O
trying	O
to	O
save	O
the	O
name	O
of	O
the	O
file	O
as	O
well	O
in	O
the	O
csv	O
file	O
like	O
this	O
:	O
#CODE	O

After	O
you	O
do	O
this	O
no	O
matter	O
where	O
the	O
template	O
object	O
is	O
in	O
a	O
calculation	O
.	O

So	O
the	O
easiest	O
thing	O
to	O
do	O
would	O
be	O
to	O
take	O
a	O
sample	O
of	O
say	O
,	O
1000	O
points	O
,	O
from	O
your	O
data	O
:	O
#CODE	O

Your	O
array	O
consists	O
of	O
:	O
#CODE	O

The	O
final	O
DF	O
should	O
have	O
as	O
many	O
columns	O
as	O
all	O
the	O
df	O
columns	O
added	O
together	O
,	O
so	O
it	O
grow	O
additively	O
and	O
not	O
be	O
combinatorial	O
.	O

I'm	O
sorting	O
the	O
cells	O
of	O
the	O
matrix	O
by	O
the	O
float	O
value	O
,	O
producing	O
a	O
list	O
of	O
`	O
(	O
row	O
,	O
col	O
,	O
value	O
)`	O
tuples	O
.	O

Is	O
it	O
essential	O
that	O
you	O
need	O
a	O
numpy	O
array	O
?	O

Mh	O
.	O
but	O
look	O
at	O
this	O
:	O

All	O
variables	O
are	O
dependent	O
on	O
each	O
other	O
and	O
I	O
am	O
only	O
looking	O
for	O
local	O
minima	O
from	O
the	O
initial	O
guess	O
.	O

The	O
basic	O
idea	O
is	O
to	O
simply	O
run	O
all	O
the	O
usual	O
steps	O
of	O
a	O
root	O
finder	O
in	O
parallel	O
on	O
a	O
vector	O
of	O
variables	O
,	O
using	O
a	O
function	O
that	O
can	O
be	O
evaluated	O
on	O
a	O
vector	O
of	O
variables	O
and	O
equivalent	O
vector	O
(	O
s	O
)	O
of	O
parameters	O
that	O
define	O
the	O
individual	O
component	O
functions	O
.	O

Hence	O
,	O
with	O
NetworkX	O
,	O
you	O
can	O
put	O
in	O
an	O
adjacency	O
matrix	O
and	O
find	O
out	O
which	O
authors	O
are	O
clustered	O
together	O
.	O

The	O
issue	O
your	O
having	O
more	O
likely	O
is	O
a	O
python	O
mmap	O
issue	O
,	O
since	O
python	O
mmaps	O
handle	O
all	O
the	O
memory	O
mapping	O
and	O
file	O
closing	O
for	O
numpy	O
memmaps	O
.	O

So	O
far	O
,	O
I'm	O
sticking	O
with	O
C++	O
-	O
on	O
my	O
tests	O
,	O
at	O
least	O
2	O
orders	O
of	O
magnitude	O
faster	O
!	O

Sorting	O
ends	O
up	O
being	O
the	O
slowest	O
step	O
but	O
it's	O
still	O
faster	O
if	O
m	O
is	O
large	O
because	O
the	O
n*log	O
(	O
n	O
)	O
sort	O
is	O
faster	O
than	O
(	O
n*m	O
)	O
.	O

Basically	O
,	O
I	O
am	O
getting	O
a	O
memory	O
error	O
in	O
python	O
when	O
trying	O
to	O
perform	O
an	O
algebraic	O
operation	O
on	O
a	O
numpy	O
matrix	O
.	O

Surely	O
there	O
must	O
be	O
a	O
way	O
to	O
populate	O
a	O
boost	O
::	O
python	O
::	O
numeric	O
::	O
array	O
with	O
data	O
from	O
a	O
simple	O
std	O
::	O
vector	O
without	O
having	O
to	O
get	O
some	O
3rd	O
party	O
library	O
.	O

Here	O
again	O
a	O
if	O
statement	O
could	O
do	O
,	O
but	O
I	O
am	O
wondering	O
if	O
there	O
is	O
a	O
workarouns	O
and	O
a	O
Python	O
library	O
where	O
negative	O
exposant	O
is	O
allowed	O
.	O

The	O
key	O
point	O
here	O
is	O
that	O
Tabular	O
and	O
NumPy	O
set	O
certain	O
standards	O
for	O
what	O
counts	O
as	O
"	O
fast	O
"	O
or	O
"	O
slow	O
"	O
--	O
and	O
then	O
,	O
force	O
you	O
to	O
be	O
explicit	O
about	O
operations	O
that	O
are	O
going	O
to	O
be	O
slow	O
.	O

Asume	O
that	O
your	O
numpy	O
module	O
is	O
located	O
at	O
/	O
Users	O
/	O
Me	O
/	O
python	O
/	O
modules	O
directory	O
.	O

I	O
am	O
not	O
responsible	O
from	O
any	O
brain	O
damage	O
resulting	O
from	O
attempting	O
to	O
understand	O
this	O
code	O
.	O

There	O
a	O
plenty	O
of	O
places	O
where	O
you're	O
inadvertently	O
creating	O
additional	O
temporary	O
arrays	O
,	O
but	O
they're	O
mostly	O
irrelevant	O
,	O
as	O
they're	O
overwhelmed	O
by	O
what	O
goes	O
on	O
during	O
the	O
call	O
to	O
`	O
select	O
`	O
.	O

The	O
fact	O
that	O
you	O
are	O
using	O
`	O
object	O
`	O
arrays	O
(	O
not	O
very	O
common	O
and	O
not	O
very	O
memory-efficient	O
)	O
presents	O
a	O
particular	O
problem	O
when	O
trying	O
to	O
determine	O
the	O
index	O
of	O
non-None	O
array	O
items	O
.	O

where	O
things	O
improve	O
as	O
the	O
number	O
of	O
bits	O
increases	O
.	O

Really	O
,	O
4D	O
arrays	O
are	O
just	O
1D	O
arrays	O
in	O
memory	O
anyway	O
(	O
Unless	O
you	O
really	O
have	O
view	O
objects	O
,	O
but	O
it	O
should	O
still	O
work	O
with	O
those	O
as	O
well	O
)	O

I'll	O
add	O
comments	O
to	O
explain	O
things	O
in	O
a	O
bit	O
.	O

I	O
was	O
assuming	O
that	O
the	O
rgb	O
and	O
ycc	O
matrices	O
were	O
just	O
a	O
matrix	O
that	O
had	O
as	O
many	O
rows	O
as	O
pixels	O
and	O
a	O
column	O
per	O
colour	O
component	O
.	O

For	O
example	O
,	O
suppose	O
`	O
a	O
=	O
ones	O
((	O
3	O
,	O
3	O
))`	O
.	O

Therefore	O
,	O
n	O
and	O
m	O
correspond	O
to	O
indices	O
in	O
the	O
array	O
,	O
but	O
I'm	O
not	O
sure	O
how	O
?	O

Update	O
:	O
As	O
mentioned	O
in	O
my	O
comment	O
below	O
,	O
I	O
should	O
have	O
stated	O
that	O
I'm	O
trying	O
to	O
do	O
this	O
on	O
2D	O
arrays	O
,	O
and	O
therefore	O
get	O
a	O
set	O
of	O
2D	O
indices	O
back	O
.	O

Need	O
to	O
add	O
a	O
check	O
for	O
that	O
,	O
but	O
otherwise	O
thanks	O
!	O

I	O
think	O
you	O
just	O
want	O
`	O
label	O
==	O
num	O
`	O
where	O
`	O
num	O
`	O
is	O
the	O
number	O
of	O
the	O
object	O
in	O
`	O
label	O
`	O
(	O
the	O
labeled	O
array	O
)	O
.	O

My	O
question	O
is	O
how	O
can	O
I	O
go	O
thru	O
the	O
array	O
to	O
access	O
the	O
object	O
in	O
the	O
array	O
?	O

The	O
matrix	O
in	O
the	O
example	O
above	O
is	O
singular	O
(	O
determinant	O
~	O
0	O
)	O
.	O

See	O
the	O
note	O
at	O
#URL	O

would	O
turn	O
into	O
either	O
this	O
array	O
:	O
#CODE	O

Note	O
that	O
this	O
is	O
a	O
bit	O
more	O
sophisticated	O
than	O
the	O
simple	O
do-it-yourself	O
convolve-method	O
,	O
since	O
it	O
tries	O
to	O
handle	O
the	O
problems	O
at	O
the	O
beginning	O
and	O
the	O
end	O
of	O
the	O
data	O
by	O
reflecting	O
it	O
(	O
which	O
may	O
or	O
may	O
not	O
work	O
in	O
your	O
case	O
...	O
)	O
.	O

Usually	O
,	O
in	O
numpy	O
,	O
you	O
keep	O
the	O
string	O
data	O
in	O
a	O
separate	O
array	O
.	O

Any	O
idea	O
what	O
might	O
be	O
happening	O
?	O

but	O
the	O
size	O
is	O
wrong	O
because	O
i've	O
assigned	O
1000	O
as	O
the	O
period	O
size	O
.	O

This	O
may	O
not	O
be	O
perfectly	O
pythonic	O
(	O
perhaps	O
someone	O
can	O
think	O
of	O
a	O
nicer	O
implementation	O
using	O
generators	O
or	O
itertools	O
?	O
)	O
but	O
it	O
is	O
hard	O
to	O
imagine	O
any	O
method	O
that	O
relies	O
on	O
searching	O
one	O
point	O
at	O
a	O
time	O
beating	O
this	O
in	O
speed	O
.	O

Thanks	O
,	O
your	O
post	O
helped	O
me	O
solve	O
this	O
problem	O
.	O

Now	O
imagine	O
that	O
the	O
next	O
time	O
step	O
some	O
values	O
change	O
,	O
so	O
should	O
this	O
picture	O
.	O

Since	O
get_probability	O
is	O
a	O
function	O
,	O
so	O
what	O
value	O
is	O
being	O
passed	O
to	O
count	O
parameter	O
here	O
???	O

taking	O
the	O
sum	O
for	O
each	O
column	O
.	O

You	O
should	O
be	O
able	O
to	O
just	O
load	O
the	O
entire	O
thing	O
into	O
memory	O
on	O
a	O
modern	O
machine	O
.	O

What	O
I	O
want	O
to	O
do	O
is	O
to	O
calculate	O
the	O
geographic	O
distances	O
between	O
rows	O
(	O
with	O
the	O
special	O
condition	O
that	O
the	O
first	O
element	O
is	O
always	O
zero	O
,	O
at	O
the	O
starting	O
point	O
)	O
.	O

We	O
can	O
simply	O
use	O
the	O
leastsq	O
function	O
to	O
find	O
the	O
best	O
coefficients	O
.	O

If	O
the	O
list	O
of	O
python	O
objects	O
doesn't	O
grow	O
at	O
all	O
from	O
frame	O
to	O
frame	O
,	O
the	O
leak	O
is	O
probably	O
in	O
the	O
C	O
code	O
or	O
the	O
python-to-C	O
link	O

Any	O
and	O
all	O
advice	O
is	O
greatly	O
appreciated	O
.	O

Numpy	O
:	O
Is	O
there	O
an	O
array	O
size	O
limit	O
?	O

Then	O
do	O
this	O
after	O
each	O
calculation	O
:	O
for	O
i	O
in	O
range	O
(	O
len	O
(	O
array	O
)):	O
array	O
[	O
i	O
]	O
[	O
i	O
]=	O
0	O

I	O
know	O
the	O
random	O
functions	O
and	O
numbers	O
seem	O
odd	O
,	O
but	O
conceptually	O
this	O
still	O
should	O
work	O
,	O
as	O
it	O
worked	O
when	O
both	O
were	O
set	O
to	O
variables	O
individually	O
.	O

@USER	O
are	O
your	O
numbers	O
in	O
the	O
range	O
of	O
-128	O
to	O
127	O
before	O
you	O
convert	O
them	O
to	O
8b	O
it	O
?	O

In	O
the	O
future	O
,	O
how	O
should	O
I	O
go	O
about	O
trying	O
to	O
find	O
routines	O
like	O
this	O
?	O

At	O
20,000	O
elements	O
,	O
your	O
method	O
is	O
about	O
25%	O
faster	O
.	O

I'll	O
fix	O
it	O
just	O
for	O
you	O
:P	O

Then	O
I	O
convert	O
it	O
to	O
a	O
numpy	O
array	O
:	O
#CODE	O

Just	O
throwing	O
in	O
my	O
two	O
cents	O
you	O
could	O
do	O
this	O
pretty	O
simply	O
using	O
list	O
comprehension	O
if	O
it's	O
always	O
a	O
2d	O
array	O
like	O
that	O
#CODE	O

While	O
its	O
expected	O
value	O
here	O
is	O
zero	O
,	O
the	O
particular	O
realizations	O
will	O
fluctuate	O
around	O
that	O
expected	O
value	O
.	O

Then	O
if	O
each	O
item	O
is	O
weighted	O
with	O
weight	O
w_i	O
,	O
the	O
"	O
summed	O
histogram	O
"	O
would	O
have	O
weight	O
sum	O
(	O
i	O
in	O
items	O
)	O
w_i	O
D_ij	O
.	O

This	O
approach	O
will	O
take	O
an	O
overhead	O
because	O
of	O
crating	O
a	O
new	O
array	O
in	O
memory	O
.	O

"	O
Eric's	O
suggestion	O
for	O
revising	O
this	O
question	O
is	O
a	O
good	O
start	O
,	O
but	O
I	O
think	O
the	O
question	O
"	O
Given	O
a	O
Cartesian	O
plane	O
,	O
how	O
to	O
discretize	O
it	O
in	O
a	O
matrix	O
form	O
?	O

it	O
is	O
the	O
same	O
as	O
long	O
as	O
you	O
ignore	O
precision	O
issue	O
-	O
which	O
matters	O
quite	O
often	O
when	O
you	O
start	O
taking	O
exponential	O
of	O
numbers	O
.	O

Google	O
Protocol	O
Buffers	O
support	O
self-describing	O
too	O
,	O
are	O
pretty	O
fast	O
(	O
but	O
Python	O
support	O
is	O
poor	O
at	O
present	O
time	O
,	O
slow	O
and	O
buggy	O
)	O
.	O

Not	O
all	O
people	O
can	O
install	O
NumPy	O
(	O
or	O
even	O
Python	O
:D	O
)	O
as	O
many	O
Blender	O
users	O
are	O
just	O
artists	O
.	O

All	O
possible	O
solutions	O
are	O
mentioned	O
in	O
the	O
comments	O
.	O

I've	O
also	O
refined	O
your	O
approach	O
to	O
allow	O
zooming	O
in	O
over	O
a	O
section	O
of	O
the	O
data	O
and	O
to	O
produce	O
better	O
results	O
at	O
the	O
borders	O
.	O

I	O
need	O
to	O
specify	O
datatypes	O
for	O
all	O
numerical	O
types	O
since	O
I	O
care	O
about	O
int	O
8/	O
16	O
/	O
32	O
,	O
etc	O
,	O
but	O
I	O
would	O
like	O
to	O
benefit	O
from	O
the	O
auto	O
string	O
length	O
detection	O
that	O
works	O
if	O
I	O
don't	O
specify	O
datatypes	O
.	O

I	O
would	O
appreciate	O
any	O
assistance	O
you	O
can	O
offer	O
.	O

Let's	O
say	O
for	O
example	O
I	O
have	O
a	O
matrix	O
X	O
which	O
is	O
my	O
input	O
.	O

@USER	O
-	O
By	O
the	O
way	O
,	O
indexing	O
returns	O
a	O
view	O
(	O
essentially	O
a	O
pointer	O
)	O
into	O
the	O
array	O
.	O

Note	O
that	O
`	O
view	O
`	O
holds	O
the	O
same	O
data	O
as	O
the	O
original	O
array	O
!	O

EDIT	O
:	O
What	O
sort	O
of	O
sequence	O
is	O
it	O
you're	O
making	O
?	O

The	O
relative	O
error	O
is	O
less	O
than	O
2	O
-24	O
,	O
which	O
is	O
1	O
/	O
2	O
ULP	O
divided	O
by	O
the	O
smallest	O
the	O
value	O
could	O
be	O
(	O
the	O
smallest	O
value	O
in	O
the	O
interval	O
for	O
a	O
particular	O
ULP	O
,	O
so	O
the	O
power	O
of	O
two	O
that	O
bounds	O
it	O
)	O
.	O

This	O
is	O
called	O
matrix	O
transposition	O
.	O

@USER	O
The	O
solutions	O
there	O
all	O
make	O
use	O
of	O
the	O
fact	O
that	O
only	O
a	O
3x3	O
sliding	O
window	O
is	O
needed	O
,	O
but	O
I	O
need	O
something	O
that	O
works	O
for	O
all	O
sizes	O
of	O
templates	O
.	O

(	O
0008	O
,	O
103e	O
)	O
Series	O
Description	O
LO	O
:	O
'	O
Screen	O
Save	O
'	O

@USER	O
khanSever	O
20k	O
wouldn't	O
be	O
a	O
problem	O
for	O
modern	O
computers	O
,	O
if	O
you	O
are	O
really	O
thresholded	O
by	O
speed	O
in	O
this	O
kind	O
of	O
computation	O
,	O
I	O
would	O
say	O
that	O
you	O
shouldn't	O
have	O
had	O
an	O
inhomogenous	O
data	O
array	O
to	O
begin	O
with	O
.	O

@USER	O
Eweiwi	O
:	O
Did	O
you	O
find	O
my	O
answer	O
anyway	O
useful	O
?	O

You	O
can	O
now	O
compute	O
the	O
function	O
`	O
f	O
(	O
x	O
)`	O
at	O
any	O
point	O
`	O
x	O
`	O
.	O

BTW	O
:	O
this	O
is	O
a	O
neat	O
workaround	O
,	O
but	O
if	O
it	O
were	O
possible	O
to	O
use	O
the	O
`	O
in	O
`	O
operator	O
would	O
have	O
preferred	O
,	O
as	O
in	O
my	O
"	O
real	O
case	O
"	O
I	O
have	O
a	O
pool	O
of	O
roughly	O
10	O
values	O
,	O
non	O
only	O
`	O
(	O
6	O
,	O
8)	O
`	O
.	O

In	O
this	O
example	O
I	O
want	O
to	O
return	O
an	O
array	O
of	O
[	O
202	O
203	O
206	O
210	O
]	O

So	O
f	O
(	O
x	O
,	O
y	O
)	O
=	O
0	O

I	O
present	O
below	O
a	O
sample	O
silhouette	O
implementation	O
in	O
both	O
MATLAB	O
and	O
Python	O
/	O
Numpy	O
(	O
keep	O
in	O
mind	O
that	O
I	O
am	O
more	O
fluent	O
in	O
MATLAB	O
):	O

Python	O
import	O
Column	O
Data	O
from	O
MySQL	O
as	O
Array	O

This	O
is	O
just	O
the	O
partial	O
count	O
due	O
to	O
the	O
34	O
1-chips	O
.	O

I	O
want	O
to	O
know	O
how	O
I	O
should	O
index	O
/	O
access	O
some	O
data	O
programmatically	O
in	O
python	O
.	O

There	O
is	O
a	O
short	O
comment	O
at	O
the	O
end	O
of	O
the	O
introduction	O
to	O
SciPy	O
documentation	O
:	O

What	O
about	O
the	O
maximum	O
value	O
in	O
the	O
array	O
?	O

If	O
you	O
use	O
a	O
list	O
of	O
`	O
True	O
/	O
False	O
`	O
,	O
NumPy	O
will	O
interpret	O
that	O
as	O
a	O
list	O
of	O
`	O
1	O
/	O
0	O
`	O
as	O
integers	O
,	O
that	O
is	O
,	O
indices	O
,	O
meaning	O
that	O
you	O
'	O
either	O
get	O
the	O
second	O
or	O
first	O
element	O
of	O
your	O
array	O
.	O

But	O
it's	O
still	O
an	O
array	O
and	O
there	O
is	O
no	O
difference	O
in	O
asymptotic	O
complexity	O
.	O

Here's	O
one	O
way	O
(	O
same	O
matrix	O
as	O
before	O
):	O
#CODE	O

assume	O
i	O
have	O
100	O
points	O
whose	O
coordinates	O
are	O
random	O
,	O

If	O
you	O
just	O
want	O
the	O
first	O
one	O
,	O
use	O
next	O
with	O
the	O
list	O
comprehension	O
as	O
a	O
generator	O
expression	O
.	O

So	O
I	O
am	O
able	O
to	O
plot	O
what	O
I	O
want	O
onto	O
my	O
matrix	O

By	O
X3D	O
,	O
are	O
you	O
referring	O
to	O
the	O
x3d	O
standard	O
for	O
3d	O
content	O
,	O
as	O
at	O
#URL	O
If	O
so	O
,	O
I	O
would	O
very	O
much	O
like	O
to	O
learn	O
more	O
of	O
what	O
you	O
are	O
doing	O
--	O
thanks	O

Would	O
it	O
be	O
prohibitvely	O
wasteful	O
to	O
save	O
them	O
with	O
a	O
fixed	O
width	O
?	O

BSD-licensed	O
Python	O
source	O
code	O
for	O
surface	O
fits	O
can	O
be	O
found	O
at	O

...	O
which	O
returned	O
`	O
True	O
`	O
on	O
each	O
value	O
of	O
the	O
array	O
.	O

I	O
have	O
two	O
ordered	O
numpy	O
arrays	O
and	O
I	O
want	O
to	O
interleave	O
them	O
so	O
that	O
I	O
take	O
one	O
item	O
from	O
the	O
first	O
array	O
,	O
then	O
another	O
from	O
the	O
second	O
,	O
then	O
back	O
to	O
the	O
first	O
-	O
taking	O
the	O
next	O
item	O
that	O
is	O
larger	O
than	O
the	O
one	O
I	O
just	O
took	O
from	O
the	O
second	O
and	O
so	O
on	O
.	O

Did	O
you	O
look	O
at	O
the	O
link	O
in	O
my	O
answer	O
to	O
the	O
SciPy	O
page	O
on	O
Performance	O
Python	O
.	O

If	O
you	O
want	O
the	O
column	O
indices	O
instead	O
of	O
the	O
resulting	O
square	O
matrix	O
,	O
just	O
replace	O
`	O
return	O
B	O
`	O
with	O
`	O
return	O
colset	O
`	O
.	O

At	O
the	O
end	O
of	O
it	O
all	O
:	O
#CODE	O

Is	O
there	O
no	O
equivalent	O
function	O
that	O
gets	O
the	O
index	O
of	O
the	O
last	O
occurrence	O
?	O

I	O
want	O
to	O
get	O
a	O
cartesian	O
product	O
of	O
a	O
[:	O
:	O
i	O
]	O
and	O
b	O
[:	O
:	O
j	O
]	O
from	O
c	O
.	O

python	O
/	O
numpy	O
:	O
how	O
to	O
get	O
2D	O
array	O
column	O
length	O
?	O

Your	O
example	O
works	O
for	O
me	O
if	O
I	O
sample	O
around	O
2**6	O
points	O
.	O

NumPy's	O
main	O
object	O
is	O
the	O
homogeneous	O
multidimensional	O
array	O
.	O

Pythonic	O
way	O
to	O
import	O
data	O
from	O
multiple	O
files	O
into	O
an	O
array	O

The	O
only	O
thing	O
I	O
was	O
going	O
to	O
add	O
was	O
this	O
:	O
#URL	O
Indicated	O
that	O
this	O
is	O
not	O
likely	O
to	O
change	O
.	O

i	O
have	O
a	O
numpy	O
array	O
like	O
the	O
following	O
#CODE	O

I	O
want	O
to	O
write	O
a	O
Boost-Python	O
program	O
to	O
take	O
a	O
symbolic	O
python	O
function	O
from	O
user	O
and	O
evaluate	O
its	O
derivative	O
in	O
my	O
program	O
.	O

Is	O
there	O
a	O
way	O
around	O
this	O
?	O

Not	O
sure	O
if	O
I	O
explained	O
this	O
all	O
really	O
well	O
,	O
but	O
just	O
print	O
out	O
a_strided	O
and	O
you'll	O
see	O
what	O
the	O
result	O
is	O
and	O
how	O
easy	O
this	O
makes	O
the	O
operation	O
.	O

But	O
when	O
I	O
start	O
calling	O
columns	O
by	O
their	O
field	O
names	O
,	O
screwy	O
things	O
happen	O
.	O

all	O
I	O
get	O
is	O
very	O
high	O
or	O
inf	O
numbers	O
.	O

If	O
you're	O
iterating	O
through	O
,	O
and	O
applying	O
the	O
function	O
to	O
_each_	O
item	O
,	O
then	O
,	O
yeah	O
,	O
the	O
numpy	O
functions	O
will	O
be	O
slower	O
.	O

Slicing	O
does	O
not	O
copy	O
the	O
array	O
into	O
new	O
memory	O
(	O
unlike	O
delete	O
)	O
.	O

And	O
here's	O
the	O
filled	O
version	O
:	O
#CODE	O

This	O
is	O
a	O
little	O
bit	O
annoying	O
to	O
do	O
,	O
but	O
at	O
least	O
you	O
can	O
remove	O
that	O
annoying	O
`	O
==	O
`	O
easily	O
,	O
using	O
sorting	O
(	O
and	O
thats	O
probably	O
your	O
speed	O
killer	O
)	O
.	O

I	O
still	O
haven't	O
found	O
an	O
entirely	O
satisfactory	O
solution	O
,	O
but	O
nevertheless	O
there	O
is	O
something	O
one	O
can	O
do	O
to	O
obtain	O
the	O
pointer	O
with	O
a	O
lot	O
less	O
overhead	O
in	O
CPython	O
.	O

I	O
also	O
tried	O
using	O
NumPy	O
masked	O
arrays	O
,	O
with	O
NaN	O
fill_value	O
,	O
which	O
also	O
did	O
not	O
work	O
.	O

cartesian	O
(	O
split	O
(	O
a	O
,	O
3	O
))`	O
.	O

I	O
did	O
a	O
little	O
further	O
experimenting	O
and	O
found	O
a	O
numpy	O
specific	O
way	O
to	O
solve	O
this	O
:	O
#CODE	O

When	O
you	O
need	O
to	O
deal	O
with	O
exponential	O
,	O
you	O
quickly	O
go	O
into	O
under	O
/	O
over	O
flow	O
since	O
the	O
function	O
grows	O
so	O
quickly	O
.	O

Long	O
story	O
short	O
,	O
not	O
only	O
does	O
tabular	O
not	O
act	O
like	O
a	O
spreadsheet	O
out	O
of	O
the	O
box	O
,	O
I	O
can't	O
find	O
a	O
way	O
to	O
make	O
it	O
work	O
.	O

What	O
do	O
you	O
mean	O
"	O
two	O
significant	O
figures	O
"	O
?	O

We	O
put	O
it	O
in	O
a	O
list	O
and	O
double	O
it	O
.	O

For	O
example	O
for	O
value	O
255	O
the	O
coordinates	O
of	O
the	O
box	O
around	O
the	O
value	O
255	O
will	O
be	O
upper	O
left	O
(	O
0	O
,	O
0	O
)	O
and	O
lower	O
right	O
(	O
4	O
,	O
6	O
)	O
.	O

Like	O
in	O
a	O
java	O
program	O
,	O
you	O
can	O
choose	O
to	O
start	O
it	O
up	O
with	O
,	O
say	O
,	O
5GB	O
of	O
memory	O
.	O

However	O
,	O
due	O
to	O
the	O
way	O
the	O
data	O
points	O
lie	O
it	O
does	O
not	O
give	O
me	O
a	O
y-axis	O
interception	O
at	O
0	O
.	O

I'd	O
like	O
to	O
sort	O
it	O
such	O
that	O
my	O
points	O
are	O
ordered	O
by	O
x-coordinate	O
,	O
and	O
then	O
by	O
y	O
in	O
cases	O
where	O
the	O
x	O
coordinate	O
is	O
the	O
same	O
.	O

Of	O
course	O
this	O
will	O
slow	O
the	O
program	O
down	O
,	O
but	O
at	O
least	O
it'll	O
finish	O
.	O

Im	O
writing	O
it	O
here	O
because	O
i	O
cant	O
put	O
image	O
in	O
comment	O
.	O

In	O
looking	O
at	O
`	O
fill	O
`	O
,	O
I	O
saw	O
that	O
`	O
repeat	O
`	O
suits	O
my	O
needs	O
even	O
better	O
.	O

Note	O
that	O
an	O
array's	O
base	O
will	O
be	O
another	O
array	O
,	O
even	O
if	O
it	O
is	O
a	O
subset	O
:	O
#CODE	O

If	O
you	O
have	O
float	O
data	O
,	O
or	O
data	O
spread	O
over	O
a	O
huge	O
range	O
you	O
can	O
convert	O
it	O
to	O
integers	O
by	O
doing	O
:	O
#CODE	O

@USER	O
,	O
plaes	O
recommend	O
using	O
a	O
generator	O
(	O
parenthesis	O
)	O
instead	O
of	O
a	O
list	O
(	O
brackets	O
)	O
in	O
order	O
to	O
save	O
memory	O
and	O
gain	O
speed	O
when	O
managing	O
high	O
amounts	O
of	O
data	O
.	O

I	O
want	O
to	O
divide	O
this	O
array	O
into	O
3	O
blocks	O
of	O
size	O
2x4	O
,	O
and	O
then	O
find	O
the	O
mean	O
of	O
all	O
three	O
blocks	O
(	O
so	O
that	O
the	O
shape	O
of	O
the	O
mean	O
is	O
2x4	O
.	O

(	O
Have	O
a	O
look	O
at	O
the	O
comments	O
above	O
the	O
code	O
for	O
that	O
portion	O
.	O
)	O

That	O
is	O
because	O
`	O
fsolve	O
`	O
thinks	O
it	O
is	O
looking	O
for	O
an	O
array	O
of	O
length	O
17	O
that	O
solves	O
`	O
p	O
`	O
.	O

When	O
there's	O
a	O
choice	O
between	O
working	O
with	O
NumPy	O
array	O
and	O
numeric	O
lists	O
,	O
the	O
former	O
are	O
typically	O
faster	O
.	O

Wait	O
...	O
why	O
do	O
you	O
need	O
the	O
negative	O
?	O

But	O
if	O
a	O
dense	O
3d	O
array	O
representation	O
isn't	O
that	O
much	O
bigger	O
,	O
storing	O
it	O
as	O
a	O
chuncked	O
and	O
compressed	O
hdf5	O
array	O
is	O
probably	O
the	O
way	O
to	O
go	O
.	O

Index	O
datetime	O
in	O
numpy	O
array	O

Is	O
there	O
an	O
"	O
expandable	O
"	O
matrix	O
data	O
structure	O
available	O
in	O
a	O
well	O
tested	O
module	O
?	O

You	O
can	O
make	O
this	O
one-liner	O
reusable	O
if	O
you	O
are	O
going	O
to	O
repeat	O
it	O
a	O
lot	O
:	O
#CODE	O

Here's	O
my	O
array	O
(	O
rather	O
,	O
a	O
method	O
of	O
generating	O
representative	O
test	O
arrays	O
):	O
#CODE	O

We	O
need	O
more	O
information	O
on	O
your	O
array	O
.	O

@USER	O
:	O
If	O
the	O
code	O
all	O
F77	O
,	O
why	O
is	O
the	O
question	O
tagged	O
Python	O
?	O

It	O
does	O
that	O
without	O
densifying	O
the	O
matrix	O
right	O
?	O

To	O
speed	O
up	O
the	O
program	O
,	O
I	O
want	O
to	O
pass	O
the	O
index	O
through	O
a	O
subroutine	O
,	O
but	O
I	O
cannot	O
pass	O
`	O
[	O
index	O
[	O
0	O
]	O
,	O
:	O
,	O
index	O
[	O
1	O
]	O
,	O
index	O
[	O
2	O
]]`	O
through	O
a	O
subroutine	O
because	O
I	O
cannot	O
pass	O
the	O
colon	O
'	O
:	O
'	O
.	O

Any	O
thoughts	O
on	O
what	O
I'm	O
doing	O
wrong	O
?	O

This	O
will	O
be	O
far	O
,	O
far	O
faster	O
than	O
constantly	O
reallocating	O
the	O
array	O
inside	O
the	O
loop	O
.	O

How	O
can	O
I	O
get	O
a	O
new	O
array	O
containing	O
the	O
values	O
of	O
specific	O
attributes	O
of	O
those	O
objects	O
?	O

Seriously	O
,	O
at	O
least	O
leave	O
a	O
note	O
,	O
but	O
given	O
the	O
"	O
complexity	O
"	O
of	O
your	O
actual	O
request	O
I'd	O
say	O
that	O
you'll	O
have	O
better	O
chances	O
with	O
a	O
new	O
question	O
.	O

and	O
find	O
the	O
roots	O
with	O
numpy	O
:	O
#CODE	O

I	O
need	O
to	O
create	O
a	O
numpy	O
array	O
of	O
N	O
elements	O
,	O
but	O
I	O
want	O
to	O
access	O
the	O

I	O
have	O
allocated	O
a	O
chunk	O
of	O
double	O
in	O
a	O
C	O
library	O
and	O
I	O
would	O
like	O
to	O
create	O
a	O
numpy	O
1D	O
array	O
based	O
on	O
that	O
data	O
;	O
ideally	O
I	O
would	O
like	O
two	O
versions	O
one	O
which	O
only	O
wraps	O
the	O
c_ptr	O
readonly	O
-	O
letting	O
the	O
C	O
layer	O
retain	O
ownership	O
of	O
the	O
data	O
,	O
and	O
one	O
which	O
copies	O
the	O
data	O
.	O

The	O
code	O
included	O
in	O
pypy	O
is	O
a	O
new	O
array	O
class	O
which	O
tries	O
to	O
be	O
compatible	O
with	O
numpy	O
,	O
IOW	O
,	O
it	O
is	O
a	O
reimplementation	O
from	O
scratch	O
,	O
without	O
many	O
features	O
from	O
numpy	O
.	O

Like	O
I	O
say	O
,	O
I'm	O
honestly	O
struggling	O
,	O
any	O
help	O
would	O
be	O
much	O
appreciated	O
.	O

with	O
array	O
.	O

I'm	O
not	O
sure	O
that	O
I	O
understand	O
the	O
difference	O
between	O
copying	O
the	O
matrix	O
(	O
example	O
1	O
)	O
and	O
copying	O
the	O
data	O
(	O
example	O
2	O
)	O
.	O

Does	O
anybody	O
know	O
of	O
a	O
(	O
common	O
case	O
)	O
faster-than-linear	O
way	O
to	O
find	O
the	O
endpoints	O
of	O
a	O
boolean	O
property	O
of	O
an	O
array	O
.	O

Any	O
unrecognized	O
type	O
will	O
work	O
this	O
way	O
,	O
so	O
you	O
might	O
want	O
to	O
use	O
`	O
myclass	O
`	O
instead	O
of	O
`	O
object	O
`	O
.	O

Iterate	O
over	O
vectors	O
in	O
a	O
multidimensional	O
numpy	O
array	O

This	O
way	O
you	O
can	O
load	O
a	O
large	O
dataset	O
from	O
a	O
textfile	O
memory-efficiently	O
while	O
retaining	O
all	O
the	O
convenient	O
parsing	O
features	O
of	O
the	O
two	O
functions	O
.	O

you	O
may	O
win	O
few	O
cycles	O
if	O
you	O
multiply	O
by	O
inverse	O
instead	O
of	O
dividing	O
in	O
floating-point	O
performance	O
.	O

Without	O
knowing	O
the	O
size	O
or	O
quantity	O
of	O
the	O
images	O
or	O
the	O
application	O
of	O
the	O
algorithm	O
(	O
computer	O
vision	O
?	O
)	O
,	O
I	O
can't	O
say	O
how	O
big	O
a	O
deal	O
that	O
kind	O
of	O
speedup	O
is	O
.	O

Is	O
there	O
an	O
easy	O
way	O
to	O
sort	O
these	O
eigenvalues	O
(	O
and	O
associated	O
vectors	O
)	O
in	O
order	O
?	O

You	O
can	O
pass	O
a	O
numpy	O
array	O
or	O
matrix	O
as	O
an	O
argument	O
when	O
initializing	O
a	O
sparse	O
matrix	O
.	O

(	O
For	O
most	O
common	O
applications	O
of	O
quadratic	O
forms	O
q	O
A	O
,	O
the	O
matrix	O
A	O
is	O
symmetric	O
,	O
or	O
even	O
symmetric	O
positive	O
definite	O
,	O
so	O
feel	O
free	O
to	O
assume	O
that	O
either	O
one	O
of	O
these	O
is	O
the	O
case	O
,	O
if	O
it	O
matters	O
for	O
your	O
answer	O
.	O
)	O

I	O
think	O
you	O
might	O
find	O
the	O
`	O
flat	O
`	O
method	O
useful	O
.	O

Now	O
that	O
we	O
have	O
both	O
the	O
starting	O
and	O
ending	O
values	O
,	O
we	O
can	O
use	O
the	O
indices	O
function	O
from	O
this	O
question	O
to	O
get	O
an	O
array	O
of	O
selector	O
indices	O
:	O
#CODE	O

10	O
(	O
i	O
?	O
1	O
)	O
K	O
,	O
where	O
K	O
=	O
k	O
/	O
(	O
n	O
?	O
1	O
)	O
.	O

This	O
identifies	O
which	O
rows	O
have	O
any	O
element	O
which	O
are	O
True	O
#CODE	O

Broadcasting	O
is	O
a	O
more	O
general	O
way	O
to	O
fill	O
an	O
array	O
and	O
I	O
would	O
guess	O
is	O
slower	O
or	O
equal	O
to	O
the	O
very	O
narrow	O
use	O
case	O
of	O
`	O
fill	O
`	O
.	O

By	O
"	O
not	O
replicating	O
data	O
"	O
I	O
am	O
assuming	O
you	O
mean	O
"	O
not	O
allocating	O
more	O
memory	O
"	O
.	O

Can	O
you	O
post	O
all	O
/	O
more	O
of	O
the	O
data	O
?	O

The	O
scoring	O
matrix	O
would	O
be	O
trivial	O
,	O
as	O
the	O
"	O
distance	O
"	O
between	O
two	O
numbers	O
is	O
just	O
their	O
difference	O
.	O

Contours	O
around	O
scipy	O
labeled	O
regions	O
in	O
a	O
2D	O
grid	O

Why	O
doesn't	O
the	O
shape	O
of	O
my	O
numpy	O
array	O
change	O
?	O

Mind	O
also	O
the	O
indexing	O
starts	O
at	O
`	O
0	O
`	O

I	O
have	O
a	O
vague	O
feeling	O
that	O
I	O
might	O
have	O
seen	O
a	O
question	O
addressing	O
this	O
problem	O
,	O
but	O
I	O
can't	O
find	O
it	O
now	O
.	O

If	O
you	O
know	O
which	O
rows	O
are	O
to	O
be	O
deleted	O
,	O
just	O
extract	O
the	O
other	O
rows	O
(	O
you	O
need	O
)	O
and	O
create	O
a	O
new	O
array	O
.	O

If	O
there	O
is	O
any	O
other	O
way	O
I	O
guess	O
I	O
have	O
to	O
do	O
that	O
.	O

I	O
have	O
a	O
matrix	O
,	O
say	O
#CODE	O

You	O
might	O
find	O
out	O
the	O
distribution	O
information	O
using	O
`	O
cat	O
/	O
etc	O
/	O
*-release	O
`	O
;)	O

I	O
was	O
wondering	O
if	O
anyone	O
found	O
a	O
good	O
workaround	O
,	O
as	O
my	O
real-world	O
problem	O
of	O
iterating	O
over	O
the	O
Cartesian-product	O
of	O
the	O
rows	O
in	O
very	O
large	O
arrays	O
is	O
so	O
slow	O
it's	O
impeding	O
progress	O
.	O

I	O
have	O
an	O
array	O
of	O
x	O
,	O
y	O
,	O
z	O
distances	O
and	O
I	O
need	O
to	O
find	O
the	O
differences	O
between	O
each	O
vector	O
from	O
one	O
another	O
.	O

The	O
code	O
above	O
finds	O
parts	O
where	O
there	O
are	O
at	O
least	O
MIN_SILENCE	O
consecutive	O
elements	O
smaller	O
than	O
SILENCE_THRESHOLD	O
.	O

The	O
list	O
of	O
indices	O
will	O
always	O
be	O
ascending	O
,	O
never	O
have	O
duplicates	O
,	O
but	O
may	O
have	O
gaps	O
like	O
the	O
example	O
.	O

Any	O
ideas	O
?	O

sum	O
function	O
in	O
python	O

All	O
globals	O
hold	O
either	O
values	O
referenced	O
by	O
those	O
tuples	O
or	O
are	O
lists	O
of	O
tuples	O
.	O

You	O
can	O
pass	O
a	O
list	O
or	O
an	O
array	O
as	O
indexes	O
to	O
any	O
np	O
array	O
.	O

The	O
array	O
I'm	O
using	O
is	O
quite	O
large	O
(	O
3500x3500	O
)	O
,	O
so	O
I'm	O
wondering	O
where	O
the	O
best	O
place	O
to	O
load	O
it	O
is	O
for	O
repeated	O
use	O
.	O

Basically	O
,	O
it	O
comes	O
down	O
to	O
checking	O
before	O
you	O
add	O
.	O

I	O
have	O
a	O
simple	O
function	O
called	O
get_gradient	O
which	O
takes	O
a	O
numpy	O
array	O
of	O
[[	O
x	O
,	O
y	O
,	O
Vx	O
,	O
Vy	O
]]	O
and	O
returns	O
(	O
should	O
return	O
)	O
an	O
array	O
of	O
[[	O
Vx	O
,	O
Vy	O
,	O
Ax	O
,	O
Ay	O
]]	O
.	O

I	O
found	O
this	O
post	O
:	O
Python	O
:	O
finding	O
an	O
element	O
in	O
an	O
array	O

So	O
,	O
are	O
VBOs	O
simply	O
not	O
meant	O
to	O
be	O
that	O
big	O
(	O
I	O
somehow	O
doubt	O
that	O
VBOs	O
could	O
only	O
have	O
around	O
17k	O
triangles	O
each	O
)	O
?	O

`	O
flags	O
`	O
parameter	O
leads	O
to	O
`	O
TypeError	O
`	O
if	O
input	O
array	O
is	O
not	O
contiguous	O
.	O

I	O
then	O
have	O
a	O
2nd	O
array	O
similar	O
to	O
#CODE	O

Convert	O
a	O
list	O
of	O
2D	O
numpy	O
arrays	O
to	O
one	O
3D	O
numpy	O
array	O
?	O

I'm	O
currently	O
a	O
grad	O
student	O
at	O
Harvard	O
and	O
a	O
good	O
friend	O
of	O
mine	O
went	O
there	O
(	O
he	O
would	O
have	O
graduated	O
two	O
or	O
three	O
years	O
ago	O
,	O
as	O
he	O
is	O
currently	O
a	O
second-year	O
grad	O
student	O
here	O
at	O
Harvard	O
with	O
me	O
)	O
.	O

I'm	O
not	O
clear	O
on	O
how	O
you	O
are	O
wanting	O
to	O
plot	O
it	O
,	O
but	O
it	O
sound	O
like	O
you'll	O
need	O
to	O
select	O
some	O
values	O
of	O
a	O
column	O
.	O

The	O
issue	O
I	O
am	O
running	O
in	O
to	O
is	O
that	O
the	O
array	O
can	O
be	O
larger	O
than	O
3gb	O
in	O
size	O
(	O
these	O
are	O
huge	O
images	O
)	O
and	O
I	O
need	O
to	O
segment	O
them	O
prior	O
to	O
ingesting	O
them	O
.	O

The	O
latter	O
might	O
be	O
faster	O
because	O
it	O
doesn't	O
produce	O
the	O
intermediate	O
`	O
x**2	O
`	O
array	O
.	O

Any	O
suggestions	O
?	O

"	O
A	O
copy	O
of	O
arr	O
with	O
the	O
elements	O
specified	O
by	O
obj	O
removed	O
.	O

is	O
not	O
it	O
another	O
copy	O
?	O

NOTE	O
:	O
the	O
row	O
has	O
"	O
:	O
"	O
,	O
but	O
the	O
"	O
:	O
"	O
does	O
mean	O
the	O
dict	O
'	O
:	O
'	O
.	O

If	O
,	O
for	O
some	O
reason	O
,	O
I	O
would	O
only	O
save	O
one	O
dictionary	O
then	O
every	O
script	O
loading	O
this	O
file	O
with	O
pickle	O
would	O
mess	O
up	O
the	O
order	O
of	O
the	O
stored	O
variables	O
.	O

You	O
might	O
also	O
want	O
to	O
take	O
a	O
look	O
at	O
Anvil	O
,	O
announcement	O
here	O
.	O

The	O
other	O
way	O
that	O
I	O
know	O
is	O
to	O
convert	O
Y	O
to	O
list	O
iteratively	O
.	O

This	O
is	O
especially	O
helpful	O
since	O
it	O
includes	O
the	O
import	O
commands	O
and	O
info	O
on	O
how	O
to	O
write	O
to	O
file	O
.	O

But	O
actually	O
I	O
am	O
not	O
so	O
sure	O
that	O
from	O
where	O
you	O
are	O
now	O
,	O
using	O
sparse	O
matrices	O
will	O
gain	O
you	O
any	O
speed-up	O
.	O

Upon	O
deeper	O
examination	O
of	O
the	O
relationship	O
between	O
the	O
python	O
printout	O
and	O
the	O
structure	O
of	O
my	O
underlying	O
data	O
,	O
I	O
see	O
that	O
the	O
python	O
print	O
command	O
is	O
saying	O
that	O
there	O
are	O
two	O
empty	O
columns	O
at	O
the	O
end	O
of	O
the	O
array	O
.	O

How	O
to	O
convert	O
a	O
simple	O
list	O
of	O
lists	O
into	O
a	O
numppy	O
array	O
?	O

Django	O
has	O
a	O
library	O
for	O
encapsulating	O
all	O
the	O
database	O
work	O
into	O
Python	O
classes	O
,	O
so	O
you	O
don't	O
have	O
to	O
mess	O
with	O
raw	O
SQL	O
until	O
you	O
have	O
to	O
do	O
something	O
really	O
clever	O
.	O

So	O
I	O
got	O
numpy	O
,	O
scipy	O
,	O
IPython	O
,	O
and	O
matplotlib	O
working	O
(	O
I	O
can	O
import	O
all	O
four	O
with	O
"	O
import	O
_	O
)"	O
.	O

@USER	O
`	O
new	O
type	O
not	O
compatible	O
with	O
array	O
.	O

Is	O
there	O
any	O
way	O
to	O
do	O
this	O
in	O
Python	O
?	O

Then	O
you	O
can	O
choose	O
many	O
methods	O
to	O
visualize	O
it	O
.	O

Numpy	O
Array	O
to	O
base64	O
and	O
back	O
to	O
Numpy	O
Array	O
-	O
Python	O

In	O
each	O
iteration	O
of	O
Gibbs	O
sampling	O
,	O
we	O
remove	O
one	O
(	O
current	O
)	O
word	O
,	O
sample	O
a	O
new	O
topic	O
for	O
that	O
word	O
according	O
to	O
a	O
posterior	O
conditional	O
probability	O
distribution	O
inferred	O
from	O
the	O
LDA	O
model	O
,	O
and	O
update	O
word-topic	O
counts	O
,	O
as	O
follows	O
:	O
#CODE	O

I	O
am	O
getting	O
weird	O
errors	O
when	O
I	O
try	O
to	O
convert	O
a	O
black	O
and	O
white	O
PIL	O
image	O
to	O
a	O
numpy	O
array	O
.	O

Numpy	O
arrays	O
have	O
a	O
`	O
copy	O
`	O
method	O
which	O
you	O
can	O
use	O
for	O
just	O
this	O
purpose	O
.	O

Actually	O
I	O
could	O
not	O
test	O
with	O
big	O
K	O
,	O
d	O
and	O
N	O
as	O
I	O
was	O
going	O
out	O
of	O
memory	O
.	O

With	O
all	O
of	O
these	O
options	O
you	O
have	O
to	O
pay	O
a	O
JNA	O
tax	O
...	O
all	O
of	O
your	O
data	O
has	O
to	O
be	O
copied	O
before	O
it	O
can	O
be	O
processed	O
.	O

Useless	O
because	O
it	O
ignores	O
the	O
"	O
cross	O
platform	O
issues	O
,	O
proprietary	O
tool	O
chains	O
,	O
certification	O
gates	O
,	O
licensed	O
technologies	O
,	O
and	O
stringent	O
performance	O
requirements	O
on	O
top	O
of	O
the	O
issues	O
with	O
legacy	O
codebases	O
and	O
workforce	O
availability	O
"	O
(	O
John	O
Carmack	O
)	O
that	O
op	O
is	O
probably	O
facing	O
.	O

And	O
that	O
the	O
values	O
of	O
all	O
(	O
x	O
,	O
y	O
)	O
pairs	O
are	O
given	O
.	O

Is	O
is	O
possible	O
to	O
have	O
a	O
3-D	O
record	O
array	O
in	O
numpy	O
?	O

However	O
,	O
the	O
evidence	O
suggests	O
that	O
you've	O
encountered	O
an	O
issue	O
of	O
this	O
sort	O
.	O

There's	O
_way_	O
less	O
overhead	O
this	O
way	O
.	O

I'm	O
having	O
trouble	O
figuring	O
out	O
what	O
kind	O
of	O
test	O
I	O
need	O
here	O
,	O
and	O
the	O
best	O
numpy	O
/	O
scipy	O
/	O
R	O
function	O
to	O
use	O
for	O
these	O
kinds	O
of	O
issues	O
.	O

I	O
have	O
see	O
people	O
using	O
dictionaries	O
,	O
but	O
the	O
arrays	O
are	O
large	O
and	O
filled	O
with	O
both	O
positive	O
and	O
negative	O
floats	O
.	O

How	O
can	O
I	O
speed	O
up	O
iteration	O
through	O
this	O
transformed	O
numpy	O
array	O
?	O

This	O
is	O
may	O
not	O
be	O
the	O
best	O
way	O
to	O
solve	O
this	O
but	O
have	O
a	O
look	O
at	O
the	O
following	O
...	O

All	O
in	O
all	O
,	O
I	O
would	O
go	O
with	O
the	O
#CODE	O

This	O
is	O
not	O
a	O
matter	O
of	O
style	O
.	O
without	O
the	O
list	O
(	O
_	O
)	O
it	O
does	O
not	O
even	O
work	O
at	O
last	O
for	O
the	O
case	O
i	O
have	O
that	O
y	O
is	O
an	O
array	O
itself	O

(	O
at	O
least	O
it	O
gives	O
me	O
an	O
error	O
stating	O
that	O
the	O
'	O
as	O
'	O
is	O
reserved	O
in	O
python	O
2.6	O
)	O
Am	O
I	O
correct	O
?	O

Did	O
you	O
try	O
looking	O
at	O
numpy	O
for	O
matlab	O
users	O
manuals	O
,	O
like	O
:	O
#URL	O

I	O
would	O
not	O
try	O
to	O
process	O
`	O
arr	O
`	O
in	O
place	O
-	O
it	O
seems	O
that	O
a	O
new	O
array	O
is	O
created	O
under	O
the	O
hood	O
in	O
most	O
cases	O
anyway	O
.	O

Now	O
you	O
must	O
initialize	O
each	O
element	O
of	O
the	O
numpy	O
array	O
to	O
be	O
an	O
1-d	O
numpy	O
array	O
:	O
#CODE	O

The	O
easiest	O
way	O
around	O
this	O
is	O
to	O
just	O
use	O
a	O
numpy	O
array	O
,	O
instead	O
of	O
a	O
numpy	O
matrix	O
:	O
#CODE	O

I	O
am	O
trying	O
to	O
create	O
an	O
affinity	O
matrix	O
for	O
an	O
image	O
.	O

to	O
handle	O
the	O
error	O
cases	O
and	O
the	O
return	O
value	O
,	O
they	O
are	O
not	O
related	O
to	O
the	O
array	O
assignment	O
.	O

Saving	O
a	O
Numpy	O
array	O
as	O
an	O
image	O
(	O
instructions	O
)	O

Using	O
this	O
,	O
I	O
know	O
I	O
am	O
calculating	O
r-squared	O
correctly	O
for	O
linear	O
best-fit	O
(	O
degree	O
equals	O
1	O
)	O
.	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

about	O
15	O
times	O
faster	O
using	O
broadcast	O

Arrays	O
to	O
Matrix	O
numpy	O

but	O
it	O
appears	O
to	O
only	O
take	O
square	O
matrices	O
.	O

Any	O
idea	O
how	O
that	O
can	O
be	O
done	O
?	O

In	O
your	O
code	O
,	O
`	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
returns	O
the	O
value	O
in	O
a	O
,	O
but	O
I	O
want	O
the	O
INDEX	O
in	O
a	O
,	O
so	O
that	O
`	O
a	O
[	O
INDEX	O
]	O
=	O
a	O
[	O
condition	O
]	O
[	O
index	O
]`	O
.	O

Any	O
database	O
that	O
can	O
create	O
an	O
index	O
will	O
provide	O
relatively	O
fast	O
look-ups	O
(	O
depending	O
on	O
how	O
many	O
millions	O
of	O
records	O
you're	O
storing	O
)	O
.	O

Actually	O
,	O
the	O
best	O
way	O
to	O
manage	O
packages	O
on	O
OS	O
X	O
is	O
[	O
Homebrew	O
]	O
(	O
#URL	O
)	O
(	O
not	O
Fink	O
or	O
MacPorts	O
:))	O
-	O
which	O
unfortunately	O
lists	O
neither	O
NumPy	O
now	O
SciPy	O
at	O
the	O
current	O
time	O
.	O

I	O
would	O
like	O
to	O
keep	O
`	O
xcoords	O
`	O
a	O
numpy	O
array	O
if	O
possible	O
.	O
what	O
do	O
you	O
mean	O
'	O
adding	O
them	O
to	O
the	O
object	O
before	O
it	O
is	O
returned	O
'	O
?	O

But	O
I	O
just	O
need	O
to	O
sort	O
out	O
which	O
points	O
to	O
send	O
for	O
a	O
complete	O
graph	O
.	O

how	O
do	O
I	O
calculate	O
that	O
an	O
array	O
of	O
python	O
numpy	O
or	O
me	O
of	O
all	O
the	O
calculate	O
decimals	O
and	O
not	O
skip	O
like	O
.	O

It	O
will	O
support	O
it	O
on	O
the	O
next	O
release	O
.	O

Python	O
lists	O
are	O
defined	O
with	O
square	O
brackets	O
,	O
and	O
we	O
want	O
to	O
generate	O
a	O
list	O
of	O
lists	O
(	O
where	O
each	O
piece	O
contains	O
one	O
of	O
your	O
defined	O
segments	O
)	O
.	O

The	O
biggest	O
gotcha	O
for	O
me	O
was	O
that	O
almost	O
every	O
standard	O
operator	O
is	O
overloaded	O
to	O
distribute	O
across	O
the	O
array	O
.	O

I	O
want	O
to	O
combine	O
the	O
two	O
into	O
a	O
mutli-dimensional	O
numpy	O
array	O
.	O

where	O
`	O
nlooks	O
`	O
and	O
`	O
dfactor	O
`	O
are	O
scalars	O
and	O
`	O
Ic	O
`	O
is	O
the	O
unfiltered	O
array	O
.	O

In	O
a	O
10x5x5	O
matrix	O
with	O
`	O
x	O
[	O
0	O
,	O
:	O
,	O
:]	O
=	O
0	O
`	O
I	O
would	O
expect	O
a	O
result	O
of	O
:	O
#CODE	O

For	O
example	O
:	O
I	O
have	O
a	O
=	O
array	O
([	O
123	O
,	O
412	O
,	O
444	O
])	O

While	O
it	O
often	O
results	O
in	O
a	O
massive	O
speedup	O
to	O
eliminate	O
for	O
loops	O
and	O
take	O
advantage	O
of	O
numpy	O
built-ins	O
/	O
vectorization	O
.	O

If	O
I	O
understand	O
correctly	O
you	O
have	O
a	O
three	O
dimensional	O
array	O
,	O
something	O
like	O
:	O
#CODE	O

@USER	O
:	O
where	O
is	O
a	O
new	O
array	O
created	O
?	O

array	O
([	O
41	O
,	O
32	O
,	O
41	O
,	O
33	O
,	O
42	O
,	O
32	O
,	O
42	O
,	O
33	O
])	O

Any	O
idea	O
when	O
it	O
will	O
be	O
ready	O
?	O

I	O
see	O
you've	O
taken	O
care	O
of	O
my	O
edge	O
issues	O
,	O
although	O
your	O
filter	O
size	O
is	O
hardcoded	O
;)	O
.	O

If	O
you	O
open	O
idle	O
and	O
type	O
`	O
import	O
matplotlib	O
`	O
it	O
shouldn't	O
return	O
an	O
error	O

No	O
expert	O
on	O
the	O
topic	O
,	O
but	O
this	O
is	O
some	O
kind	O
of	O
adjency	O
matrix	O
(	O
#URL	O
)	O
.	O

Edit	O
:	O
If	O
it's	O
a	O
floating	O
point	O
issue	O
,	O
what	O
sort	O
of	O
floating	O
point	O
error	O
mistakes	O
a	O
number	O
much	O
less	O
than	O
1	O
as	O
one	O
around	O
8	O
?	O

The	O
question	O
was	O
about	O
how	O
to	O
slice	O
if	O
the	O
rank	O
is	O
not	O
known	O
at	O
the	O
time	O
I	O
write	O
the	O
code	O
.	O

I	O
think	O
a	O
typical	O
method	O
is	O
to	O
always	O
double	O
the	O
size	O
,	O
when	O
you	O
really	O
don't	O
know	O
how	O
large	O
things	O
will	O
be	O
.	O

This	O
script	O
is	O
mainly	O
intended	O
to	O
demonstrate	O
building	O
an	O
independent	O
python	O
in	O
your	O
home	O
directory	O
,	O
and	O
assumes	O
the	O
system	O
you're	O
building	O
on	O
has	O
the	O
proper	O
dependencies	O
already	O
installed	O
,	O
but	O
it	O
at	O
least	O
points	O
you	O
in	O
the	O
right	O
direction	O
.	O

and	O
use	O
the	O
information	O
on	O
the	O
size	O
inclued	O
in	O
the	O
filename	O
to	O
restore	O
the	O
initial	O
shape	O

Hmm	O
I	O
added	O
for	O
first	O
example	O
,	O
did	O
you	O
know	O
how	O
to	O
copy	O
from	O
IDE	O
exactly	O
with	O
commas	O
and	O
everything	O
..?	O

@USER	O
:	O
Your	O
answer	O
will	O
give	O
false	O
positives	O
in	O
the	O
event	O
that	O
one	O
or	O
more	O
(	O
but	O
not	O
all	O
)	O
of	O
the	O
elements	O
in	O
B	O
matches	O
with	O
one	O
of	O
the	O
rows	O
in	O
A	O
.	O

I	O
would	O
like	O
to	O
average	O
the	O
2	O
different	O
arrays	O
contained	O
within	O
`	O
record	O
`	O
.	O

I	O
need	O
to	O
constrained	O
minimization	O
of	O
some	O
data	O
(	O
ie	O
so	O
that	O
I	O
get	O
the	O
minimum	O
value	O
within	O
a	O
certain	O
range	O
)	O
.	O

In	O
this	O
case	O
,	O
I	O
would	O
like	O
to	O
return	O
the	O
index	O
2	O
(	O
2nd	O
row	O
)	O
.	O

a	O
32	O
bits	O
process	O
can	O
only	O
access	O
around	O
4	O
GB	O
of	O
memory	O
.	O

How	O
do	O
I	O
find	O
out	O
,	O
if	O
the	O
numpy	O
BLAS	O
libraries	O
are	O
availalbe	O
as	O
dynamically-loadable	O
?	O

(	O
they	O
are	O
at	O
same	O
scale	O
)	O

Now	O
simply	O
create	O
a	O
new	O
array	O
and	O
multiply	O
:	O
#CODE	O

Take	O
a	O
look	O
at	O
this	O
Project	O
Euler	O
problem	O
:	O
#URL	O

Python	O
:	O
how	O
to	O
store	O
a	O
numpy	O
multidimensional	O
array	O
in	O
PyTables	O
?	O

How	O
can	O
i	O
load	O
all	O
24	O
joblib	O
files	O
in	O
one	O
program	O
without	O
any	O
errors	O
?	O

Where	O
I'm	O
stuck	O
is	O
what	O
the	O
wrapper	O
code	O
should	O
then	O
look	O
like	O
to	O
pass	O
a	O
MxN	O
numpy	O
array	O
to	O
the	O
**	O
coords1	O
and	O
**	O
coords2	O
arguments	O
.	O

I	O
have	O
created	O
a	O
numpy	O
2d	O
array	O
of	O
type	O
string	O
called	O
'	O
minutes_array	O
'	O
with	O
the	O
first	O
column	O
as	O
unix	O
timestamps	O
rounded	O
to	O
the	O
nearest	O
minute	O
covering	O
every	O
minute	O
from	O
the	O
start	O
of	O
the	O
sensor	O
timeseries	O
to	O
the	O
end	O
with	O
three	O
empty	O
columns	O
to	O
be	O
filled	O
with	O
data	O
from	O
each	O
of	O
the	O
3	O
sensors	O
where	O
available	O
.	O

Which	O
can	O
be	O
done	O
in	O
O	O
(	O
n	O
)	O
,	O
but	O
your	O
answer	O
requires	O
O	O
(	O
mn	O
)	O
,	O
where	O
m	O
is	O
size	O
of	O
window	O
.	O

Somehow	O
I	O
always	O
thought	O
you	O
can	O
load	O
the	O
shared	O
library	O
compiled	O
with	O
any	O
compiler	O
.	O

`	O
array	O
=[	O
'	O
NaN	O
'	O
,	O
'	O
20	O
'	O
,	O
'	O
383.333	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
5	O
'	O
,	O
'	O
100	O
'	O
,	O
'	O
129	O
'	O
,	O
'	O
122.5	O
'	O
,	O
'	O
NaN	O
'	O
,	O
'	O
NaN	O
']`	O

array	O
,	O
and	O
then	O
use	O
`	O
view	O
`	O
to	O
turn	O
it	O
into	O
a	O
structured	O
array	O
,	O
and	O
then	O
use	O

and	O
so	O
all	O
we	O
need	O
to	O
do	O
is	O
:	O
#CODE	O

Any	O
clue	O
to	O
why	O
this	O
is	O
happening	O
?	O

I	O
think	O
the	O
definition	O
used	O
in	O
the	O
field	O
of	O
statistics	O
is	O
the	O
value	O
in	O
the	O
middle	O
of	O
your	O
data	O
array	O
after	O
it	O
has	O
been	O
sorted	O
.	O

Dense	O
covariance	O
matrices	O
of	O
that	O
size	O
suggest	O
operations	O
that	O
run	O
forever	O
!	O

In	O
this	O
case	O
,	O
I'd	O
like	O
it	O
to	O
return	O
a	O
density	O
that's	O
essentially	O
peaked	O
completely	O
at	O
a	O
difference	O
of	O
0	O
,	O
with	O
no	O
mass	O
everywhere	O
else	O
.	O

If	O
the	O
array	O
is	O
doubles	O
(	O
remember	O
python	O
floats	O
are	O
C	O
doubles	O
by	O
default	O
)	O
then	O
you	O
have	O
to	O
think	O
a	O
bit	O
harder	O
as	O
==	O
is	O
not	O
really	O
safe	O
or	O
what	O
you	O
want	O
for	O
floating	O
point	O
values	O
.	O

They	O
all	O
have	O
their	O
strengths	O
and	O
weaknesses	O
.	O

numpy	O
array	O
of	O
chars	O
to	O
string	O

matrix	O
rank	O
:	O
#CODE	O

This	O
slows	O
down	O
for	O
large	O
sigma	O
,	O
at	O
which	O
point	O
using	O
FFT-based	O
smoothing	O
might	O
be	O
faster	O
.	O

What	O
is	O
the	O
fastest	O
way	O
to	O
iterate	O
through	O
all	O
one	O
dimensional	O
sub-arrays	O
of	O
an	O
n	O
dimensional	O
array	O
in	O
python	O
.	O

This	O
works	O
,	O
but	O
it's	O
really	O
slow	O
.	O

If	O
I	O
create	O
a	O
simple	O
array	O
like	O
this	O
in	O
Python	O
I'm	O
able	O
to	O
read	O
the	O
values	O
in	O
the	O
C	O
code	O
:	O

In	O
an	O
ideal	O
world	O
,	O
the	O
function	O
or	O
class	O
would	O
support	O
overlap	O
between	O
the	O
divisions	O
in	O
the	O
input	O
matrix	O
too	O
.	O

My	O
problem	O
is	O
different	O
because	O
I	O
need	O
to	O
find	O
**	O
all	O
**	O
the	O
roots	O
of	O
my	O
function	O
,	O
on	O
a	O
given	O
interval	O
.	O

How	O
can	O
I	O
create	O
a	O
PyArrayObject	O
from	O
this	O
structure	O
,	O
specially	O
how	O
I	O
can	O
create	O
a	O
numpy	O
array	O
that	O
hold	O
3	O
object	O
(	O
off	O
course	O
3	O
is	O
an	O
example	O
here	O
)	O
(	O
each	O
of	O
them	O
is	O
an	O
array	O
)	O

x	O
:	O
a	O
numpy	O
2d	O
array	O

Thanks	O
for	O
the	O
info	O
.	O

How	O
would	O
you	O
avoid	O
the	O
loop	O
in	O
the	O
case	O
that	O
all	O
entries	O
in	O
`	O
repl	O
`	O
are	O
the	O
same	O
?	O

Pulling	O
data	O
from	O
a	O
numpy	O
array	O

There's	O
no	O
effective	O
difference	O
(	O
they	O
both	O
return	O
views	O
into	O
the	O
original	O
array	O
)	O
.	O

Thanks	O
for	O
all	O
the	O
tips	O
!	O

remove	O
zero	O
lines	O
2-D	O
numpy	O
array	O

Instead	O
of	O
using	O
`	O
PyInt_AsLong	O
`	O
,	O
use	O
the	O
`	O
PyArray_*	O
`	O
functions	O
provided	O
by	O
Numpy's	O
C	O
API	O
to	O
access	O
the	O
data	O
;	O
in	O
particular	O
,	O
see	O
section	O
Array	O
API	O
.	O

Well	O
,	O
I	O
tried	O
dividing	O
by	O
the	O
largest	O
place	O
value	O
.	O

All	O
of	O
those	O
numpys	O
are	O
linked	O
to	O
the	O
system	O
Accelerate	O
framework	O
:	O
#CODE	O

and	O
I	O
wish	O
to	O
create	O
a	O
third	O
array	O
with	O
each	O
element	O
from	O
`	O
b	O
`	O
appearing	O
`	O
a	O
`	O
times	O
in	O
the	O
new	O
array	O
,	O
as	O
:	O
#CODE	O

I	O
can	O
imagine	O
a	O
number	O
of	O
approaches	O
to	O
storing	O
both	O
of	O
these	O
data	O
formats	O
,	O
ranging	O
from	O
storing	O
the	O
metadata	O
with	O
the	O
`	O
AttributeSet	O
`	O
class	O
for	O
each	O
`	O
Array	O
`	O
/	O
`	O
CArray	O
`	O
to	O
using	O
a	O
`	O
Table	O
`	O
for	O
all	O
of	O
the	O
metadata	O
.	O

I	O
want	O
to	O
calculate	O
the	O
average	O
of	O
four	O
neighbors	O
in	O
a	O
huge	O
array	O
.	O

Suppress	O
Scientific	O
Notation	O
in	O
Numpy	O
When	O
Creating	O
Array	O
From	O
Nested	O
List	O

I	O
want	O
to	O
find	O
the	O
vector	O
x	O
'	O
such	O
that	O
Ax	O
'	O
is	O
as	O
close	O
as	O
possible	O
to	O

And	O
the	O
dataset	O
in	O
question	O
is	O
beyond	O
doubt	O
particular	O
:	O
There	O
certainly	O
is	O
an	O
upper	O
bound	O
and	O
a	O
precision	O
.	O

Only	O
integers	O
can	O
be	O
used	O
as	O
array	O
or	O
matrix	O
indices	O
.	O

I	O
can't	O
find	O
it	O
online	O
anywhere	O
.	O

I	O
will	O
try	O
your	O
code	O
,	O
but	O
I	O
am	O
also	O
going	O
to	O
try	O
writing	O
a	O
simple	O
C	O
extension	O
to	O
simply	O
do	O
the	O
reading	O
,	O
math	O
,	O
and	O
drawing	O
all	O
in	O
one	O
place	O
.	O

Are	O
there	O
any	O
good	O
greedy	O
implementations	O
to	O
solve	O
this	O
or	O
am	O
I	O
on	O
my	O
own	O
to	O
implement	O
this	O
?	O

The	O
problem	O
is	O
that	O
for	O
the	O
array	O
input	O
,	O
SWIG	O
complains	O
that	O
there	O
is	O
no	O
typemap	O
.	O

Is	O
`	O
column_array_to_add	O
`	O
another	O
2D	O
array	O
,	O
or	O
is	O
it	O
a	O
1D	O
column	O
array	O
,	O
as	O
the	O
name	O
implies	O
?	O

the	O
sum	O
of	O
a	O
triple-product	O
(	O
element-wise	O
)	O
.	O

I	O
ran	O
a	O
simple	O
speed	O
test	O
comparing	O
numpy	O
and	O
python	O
list	O
comprehension	O
,	O
and	O
apparently	O
list	O
comprehension	O
was	O
faster	O
.	O

That	O
is	O
why	O
your	O
sample	O
loop	O
has	O
been	O
collapsed	O
to	O
read	O
in	O
the	O
full	O
sample	O
for	O
the	O
receiver	O
and	O
channel	O
in	O
one	O
large	O
read	O
.	O

Something	O
like	O
the	O
following	O
iterator	O
should	O
get	O
around	O
both	O
of	O
these	O
problems	O
:	O
#CODE	O

I	O
appreciate	O
any	O
input	O
on	O
this	O
...	O

Do	O
you	O
really	O
need	O
to	O
find	O
such	O
a	O
weird	O
thing	O
?	O

Any	O
particular	O
reason	O
you	O
don't	O
want	O
to	O
use	O
a	O
straightforward	O
approach	O
?	O

The	O
advantage	O
of	O
numpy	O
is	O
the	O
support	O
of	O
slicing	O
at	O
different	O
levels	O
.	O

An	O
implementation	O
,	O
however	O
,	O
is	O
not	O
really	O
open	O
to	O
interpretation	O
.	O

Python	O
numpy	O
masked	O
array	O
initialization	O

You	O
can	O
further	O
optimize	O
by	O
exploiting	O
array-order	O
alignment	O
to	O
reduce	O
excess	O
memory	O
consumption	O
caused	O
by	O
copying	O
the	O
original	O
arrays	O
.	O

For	O
example	O
,	O
any	O
vector	O
(	O
of	O
the	O
appropriate	O
dimension	O
)	O
can	O
be	O
an	O
eigenvector	O
of	O
the	O
identity	O
matrix	O
.	O

The	O
normal	O
64-bit	O
double-precision	O
floating	O
point	O
has	O
least	O
positive	O
normal	O
value	O
2.2E-308	O
;	O
storing	O
logs	O
gives	O
you	O
an	O
effective	O
least	O
positive	O
normal	O
1E-	O
(	O
1.7E308	O
)	O
.	O

index	O
set	O
for	O
each	O
position	O
in	O
the	O
index	O
arrays	O
.	O

I	O
am	O
wondering	O
if	O
reassigning	O
temp	O
[	O
]	O
to	O
a	O
1-element	O
shorter	O
vector	O
each	O
time	O
is	O
slow	O
,	O
would	O
it	O
be	O
faster	O
to	O
pre-allocate	O
a	O
96-3	O
length	O
list	O
of	O
vectors	O
of	O
length	O
96	O
,	O
95	O
,	O
94	O
...	O
to	O
3	O
?	O

What	O
would	O
we	O
do	O
,	O
if	O
we	O
wanted	O
to	O
change	O
values	O
at	O
indexes	O
which	O
are	O
multiple	O
of	O
given	O
n	O
,	O
like	O
a	O
[	O
2	O
]	O
,	O
a	O
[	O
4	O
]	O
,	O
a	O
[	O
6	O
]	O
,	O
a	O
[8	O
]	O
.....	O
for	O
n=2	O
?	O

Thanks	O
for	O
all	O
the	O
python	O
guidance	O
!	O

I'm	O
not	O
really	O
pro	O
Matlab	O
,	O
but	O
surely	O
Stata	O
can't	O
be	O
so	O
bad	O
as	O
to	O
require	O
`	O
adoedit	O
`	O
just	O
to	O
know	O
what	O
algorithm	O
it	O
is	O
using	O
?	O

This	O
can	O
be	O
found	O
relatively	O
easily	O
by	O
just	O
looking	O
at	O
points	O
where	O
the	O
potential	O
exceeds	O
a	O
certain	O
threshold	O
.	O

Negative	O
indices	O
are	O
interpreted	O
as	O
counting	O
from	O
the	O
end	O
of	O
the	O
array	O

I	O
was	O
using	O
unsigned	O
int	O
indices	O
to	O
speed	O
up	O
access	O
according	O
to	O
:	O
#URL	O

I've	O
tried	O
to	O
vectorise	O
it	O
using	O
numpy	O
but	O
I'm	O
not	O
really	O
sure	O
how	O
to	O
do	O
it	O
given	O
that	O
the	O
matrix	O
/	O
2D	O
array	O
gets	O
changed	O
on	O
each	O
iteration	O
.	O

Numpy	O
slicing	O
x	O
,	O
y	O
,	O
z	O
array	O
for	O
variable	O
z	O

I	O
would	O
like	O
to	O
convert	O
(	O
a	O
more	O
complicated	O
form	O
of	O
)	O
the	O
follwing	O
Matlab	O
code	O
#CODE	O

I	O
have	O
a	O
NumPy	O
array	O
'	O
boolarr	O
'	O
of	O
boolean	O
type	O
.	O

If	O
you	O
have	O
only	O
integers	O
that	O
are	O
between	O
0	O
and	O
n	O
(	O
if	O
not	O
its	O
no	O
problem	O
to	O
generalize	O
to	O
any	O
integer	O
range	O
unless	O
its	O
very	O
sparse	O
)	O
,	O
the	O
most	O
efficient	O
way	O
is	O
the	O
use	O
of	O
take	O
/	O
fancy	O
indexing	O
:	O
#CODE	O

Instead	O
of	O
2D	O
coordinates	O
,	O
I	O
use	O
index	O
for	O
every	O
elements	O
in	O
the	O
matrix	O
.	O

I	O
already	O
tried	O
converting	O
the	O
cols	O
to	O
int	O
but	O
that	O
didn't	O
solve	O
it	O
.	O

Although	O
I'm	O
sure	O
there	O
are	O
methods	O
for	O
applying	O
RK	O
to	O
an	O
equation	O
such	O
as	O
this	O
,	O
I	O
didn't	O
find	O
any	O
evidence	O
of	O
them	O
in	O
_Numerical	O
Recipes_	O
,	O
which	O
I	O
think	O
qualifies	O
that	O
topic	O
as	O
relatively	O
obscure	O
;-)	O

When	O
facing	O
a	O
big	O
computation	O
,	O
it	O
will	O
run	O
tests	O
using	O
several	O
implementations	O
to	O
find	O
out	O
which	O
is	O
the	O
fastest	O
one	O
on	O
our	O
computer	O
at	O
this	O
moment	O
.	O

Use	O
an	O
array	O
of	O
floating	O
point	O
numbers	O
instead	O
.	O

`	O
numpy	O
`	O
slicing	O
operations	O
probably	O
involve	O
`	O
for	O
`	O
loops	O
at	O
some	O
level	O
,	O
but	O
they're	O
implemented	O
in	O
c	O
,	O
and	O
provide	O
a	O
linear	O
time	O
solution	O
for	O
this	O
.	O

I	O
have	O
one	O
question	O
:	O
Is	O
there	O
only	O
one	O
way	O
to	O
do	O
addition	O
of	O
two	O
matrix	O
?	O

@USER	O
It	O
is	O
now	O
supported	O
,	O
at	O
least	O
in	O
my	O
version	O
(	O
1.7.1	O
)	O
.	O

I	O
know	O
I	O
could	O
start	O
a	O
number	O
of	O
times	O
at	O
random	O
locations	O
but	O
I'm	O
not	O
able	O
to	O
do	O
that	O
with	O
what	O
I	O
am	O
currently	O
working	O
on	O
and	O
have	O
to	O
use	O
on	O
of	O
these	O
minimisers	O
out	O
of	O
the	O
box	O
.	O

For	O
small	O
displacements	O
of	O
around	O
4-5	O
pixels	O
,	O
the	O
direction	O
of	O
vector	O
calculated	O
seems	O
to	O
be	O
fine	O
,	O
but	O
the	O
magnitude	O
of	O
the	O
vector	O
is	O
too	O
small	O
(	O
that's	O
why	O
I	O
had	O
to	O
multiply	O
u	O
,	O
v	O
by	O
3	O
before	O
plotting	O
them	O
)	O
.	O

However	O
,	O
I	O
will	O
need	O
to	O
access	O
all	O
waveforms	O
at	O
some	O
point	O
.	O

I've	O
find	O
this	O
:	O
#URL	O
but	O
when	O
I	O
try	O
to	O
install	O
this	O
I	O
get	O
an	O
error	O
:	O
#CODE	O

yes	O
,	O
I	O
can	O
assume	O
either	O
that	O
I	O
have	O
g	O
explicitly	O
or	O
that	O
I	O
can	O
sample	O
x	O
according	O
to	O
g	O
.	O

`	O
example	O
`	O
is	O
a	O
structured	O
array	O
consisting	O
of	O
two	O
elements	O
(	O
`	O
(	O
1	O
,	O
2	O
,	O
3	O
)`	O
and	O
`	O
(	O
4	O
,	O
5	O
,	O
6	O
)`)	O
,	O
each	O
element	O
(	O
or	O
'	O
record	O
')	O
having	O
3	O
fields	O
.	O

If	O
i	O
have	O
two	O
variables	O
-	O
where	O
they	O
either	O
are	O
a	O
1d	O
array	O
of	O
values	O
length	O
n	O
,	O
or	O
are	O
a	O
single	O
value	O
,	O
how	O
do	O
i	O
loop	O
through	O
them	O
so	O
that	O
I	O
get	O
n	O
values	O
returned	O
.	O

For	O
each	O
point	O
in	O
array	O
A	O
,	O
I	O
need	O
to	O
find	O
how	O
many	O
points	O
in	O
array	O
B	O
are	O
within	O
a	O
certain	O
distance	O
of	O
it	O
.	O

It	O
does	O
,	O
but	O
somehow	O
it	O
is	O
8	O
times	O
slower	O
than	O
copying	O
to	O
numpy	O
array	O
:(	O
I	O
suppose	O
the	O
regular	O
python	O
overhead	O
slows	O
things	O
down	O
much	O
more	O
than	O
a	O
copy	O
...	O

It	O
all	O
depends	O
on	O
its	O
dependencies	O
.	O

Is	O
there	O
a	O
way	O
to	O
make	O
an	O
array	O
of	O
such	O
strings	O
?	O

`	O
grid	O
[	O
1	O
]`	O
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
the	O
index	O
`	O
j	O
`	O
.	O

After	O
doing	O
so	O
,	O
I	O
discovered	O
that	O
if	O
I	O
tried	O
to	O
open	O
the	O
IPython	O
HTML	O
Notebook	O
I	O
got	O
the	O
error	O
message	O
:	O
#CODE	O

(	O
the	O
new	O
matrix	O
would	O
have	O
n-2	O
rows	O
m-2	O
columns	O
)	O
.	O

and	O
duplicate	O
index	O
values	O
at	O
the	O
correpsonding	O
sites	O
within	O

I	O
found	O
that	O
the	O
best	O
way	O
to	O
produce	O
small	O
pdf	O
files	O
is	O
to	O
save	O
as	O
eps	O
in	O
matplotlib	O
and	O
then	O
use	O
epstopdf	O
.	O

You	O
could	O
rearrange	O
the	O
image	O
to	O
put	O
the	O
(	O
0	O
,	O
0	O
)	O
in	O
the	O
middle	O
with	O
some	O
matrix	O
manipulation	O
.	O

Please	O
,	O
see	O
the	O
next	O
example	O
:	O

A	O
function	O
that	O
broadcasts	O
a	O
scalar	O
operation	O
over	O
an	O
array	O
is	O
called	O
a	O
universal	O
function	O
,	O
or	O
ufunc	O
.	O

may	O
not	O
exist	O
until	O
the	O
datasets	O
get	O
quite	O
big	O
(	O
maybe	O
you'll	O
need	O
at	O
least	O
10,000	O
rows	O
per	O
data	O
set	O
)	O
.	O

Magic	O
answers	O
like	O
this	O
are	O
not	O
really	O
helpful	O
because	O
they	O
don't	O
solve	O
the	O
problem	O
.	O

I	O
think	O
what	O
I	O
was	O
missing	O
is	O
that	O
I	O
really	O
have	O
a	O
3	O
dimensional	O
array	O
,	O
48x365x3	O
.	O

I	O
load	O
a	O
some	O
machine	O
learning	O
data	O
from	O
a	O
csv	O
file	O
.	O

So	O
I	O
have	O
it	O
running	O
(	O
or	O
at	O
least	O
that	O
assignment	O
isn't	O
throwing	O
an	O
error	O
and	O
it's	O
compiling	O
)	O
!	O

@USER	O
The	O
first	O
function	O
is	O
taking	O
chunks	O
of	O
200	O
items	O
from	O
your	O
huge	O
array	O
,	O
and	O
copying	O
those	O
chunks	O
to	O
a	O
new	O
,	O
even	O
more	O
ginormous	O
array	O
.	O

@USER	O
:	O
With	O
`	O
where	O
`	O
it	O
looks	O
definitely	O
nice	O
,	O
but	O
have	O
you	O
consider	O
also	O
the	O
implications	O
to	O
performance	O
when	O
implementing	O
with	O
`	O
where	O
`	O
?	O

Anyone	O
any	O
idea	O
what	O
this	O
means	O
?!	O

Assuming	O
you	O
are	O
using	O
g++	O
to	O
compile	O
...	O
have	O
you	O
had	O
different	O
results	O
in	O
any	O
way	O
when	O
experimenting	O
with	O
compiler	O
optimization	O
flags	O
?	O

With	O
the	O
overhead	O
of	O
the	O
data	O
structure	O
you	O
could	O
be	O
looking	O
at	O
usage	O
much	O
higher	O
than	O
that	O
--	O
I	O
can't	O
say	O
how	O
much	O
because	O
I	O
don't	O
know	O
the	O
memory	O
model	O
behind	O
SciPy	O
/	O
numpy	O
.	O

I	O
have	O
serious	O
doubt	O
that	O
adding	O
two	O
numpy	O
arrays	O
is	O
a	O
bottleneck	O
that	O
you	O
can	O
solve	O
rewriting	O
things	O
in	O
C	O
.	O

Where	O
exactly	O
is	O
the	O
error	O
occurring	O
?	O

I	O
frequently	O
convert	O
16-bit	O
grayscale	O
image	O
data	O
to	O
8-b	O
it	O
image	O
data	O
for	O
display	O
.	O

Reduce	O
it	O
to	O
a	O
1	O
/	O
10	O
resolution	O
,	O
find	O
the	O
one	O
white	O
pixel	O
,	O
and	O
then	O
you	O
have	O
a	O
precise	O
idea	O
of	O
where	O
to	O
search	O
for	O
the	O
centroid	O
.	O

I	O
ran	O
a	O
test	O
to	O
compare	O
the	O
times	O
,	O
and	O
found	O
that	O
my	O
method	O
is	O
faster	O
by	O
quite	O
a	O
bit	O
,	O
but	O
Freddie	O
Witherdon	O
'	O
s	O
suggestion	O
is	O
even	O
faster	O
.	O

I	O
couldn't	O
find	O
it	O
in	O
the	O
OLS	O
recipe	O
(	O
#URL	O
)	O
.	O

convert	O
binary	O
string	O
to	O
numpy	O
array	O

How	O
to	O
know	O
where	O
warning	O
come	O
from	O
in	O
Python	O

How	O
do	O
I	O
add	O
a	O
title	O
to	O
my	O
MatPlotLib	O
basemap	O
?	O

It	O
might	O
help	O
issue	O
of	O
things	O
refusing	O
to	O
update	O
,	O
and	O
at	O
the	O
least	O
it	O
will	O
be	O
faster	O
.	O

For	O
future	O
use	O
you	O
should	O
really	O
find	O
out	O
why	O
it	O
is	O
installed	O
in	O
a	O
location	O
that	O
isn't	O
searched	O
by	O
python	O
by	O
default	O
.	O

I've	O
done	O
that	O
and	O
I'm	O
still	O
getting	O
the	O
same	O
error	O
-	O
it	O
can't	O
find	O
the	O
sip	O
module	O
.	O

ImportError	O
:	O
DLL	O
load	O
failed	O
:	O
The	O
specified	O
module	O
could	O
not	O
be	O
found	O
.	O

This	O
has	O
happened	O
previously	O
(	O
and	O
I	O
was	O
not	O
able	O
to	O
get	O
around	O
it	O
)	O
.	O

@USER	O
SObolev	O
:	O
Histograms	O
of	O
the	O
values	O
falling	O
within	O
a	O
range	O
of	O
keys	O
where	O
each	O
bin	O
is	O
of	O
size	O
0.1	O

Also	O
,	O
now	O
that	O
I	O
look	O
at	O
it	O
,	O
the	O
y-axis	O
limits	O
haven't	O
been	O
set	O
right	O
.	O

I've	O
looked	O
for	O
various	O
solutions	O
to	O
my	O
problem	O
but	O
can't	O
get	O
any	O
help	O
.	O

I	O
normally	O
use	O
something	O
akin	O
to	O
the	O
following	O
for	O
this	O
purpose	O
and	O
the	O
result	O
is	O
a	O
very	O
nice	O
plot	O
where	O
x1	O
,	O
x2	O
and	O
x3	O
are	O
defined	O
as	O
follows	O

And	O
I	O
have	O
all	O
of	O
them	O
installed	O
.	O

When	O
I	O
save	O
these	O
plots	O
to	O
pdf	O
,	O
it	O
takes	O
a	O
long	O
time	O
to	O
write	O
,	O
and	O
reading	O
the	O
pdf	O
is	O
even	O
worse	O
.	O

Seems	O
like	O
a	O
security	O
setting	O
on	O
OS	O
X	O
is	O
preventing	O
it	O
,	O
and	O
the	O
quick	O
fix	O
seems	O
to	O
be	O
to	O
run	O
apache	O
as	O
root	O
.	O

Any	O
ideas	O
how	O
to	O
deal	O
with	O
this	O
problem	O
?	O

If	O
I	O
remove	O
the	O
function	O
and	O
put	O
its	O
body	O
back	O
under	O
the	O
"	O
if	O
name	O
==	O
'	O
main	O
'"	O
block	O
,	O
it	O
works	O
as	O
expected	O
(	O
like	O
in	O
the	O
original	O
code	O
)	O
.	O

Seems	O
like	O
a	O
bit	O
of	O
searching	O
would	O
have	O
turned	O
up	O
[	O
how	O
to	O
set	O
custom	O
ticks	O
]	O
(	O
#URL	O
)	O
,	O
where	O
the	O
defined	O
`	O
t11	O
`	O
is	O
where	O
you	O
would	O
write	O
out	O
A	O
,	O
B	O
,	O
C	O
,	O
and	O
D	O
.	O

@USER	O
,	O
that	O
would	O
require	O
to	O
have	O
root	O
access	O
(	O
sudo	O
)	O
which	O
i	O
dont	O
have	O
!	O

Any	O
suggestions	O
how	O
this	O
can	O
work	O
?	O

Any	O
help	O
would	O
be	O
appreciated	O
in	O
understanding	O
what	O
I	O
am	O
missing	O
.	O

How	O
can	O
I	O
add	O
the	O
`	O
P_g	O
`	O
values	O
from	O
each	O
section	O
to	O
their	O
respective	O
column	O
?	O

Is	O
there	O
any	O
other	O
python	O
package	O
that	O
would	O
help	O
?	O

I'm	O
having	O
a	O
problem	O
,	O
where	O
I	O
can't	O
do	O
a	O
step	O
graph	O
of	O
2	O
lists	O
I	O
have	O
,	O
in	O
which	O
I	O
need	O
list	O
`	O
x	O
`	O
to	O
be	O
the	O
x	O
values	O
,	O
in	O
which	O
each	O
`	O
x	O
[	O
j	O
]`	O
value	O
will	O
add	O
with	O
`	O
x	O
[	O
j+1	O
]`	O
value	O
for	O
each	O
step	O
.	O

The	O
best	O
thing	O
would	O
we	O
if	O
one	O
could	O
integrate	O
it	O
into	O
the	O
label	O
of	O
the	O
colorbar	O
(	O
to	O
say	O
the	O
color	O
means	O
A	O
in	O
units	O
of	O
B	O
where	O
B	O
includes	O
the	O
order	O
of	O
magnitude	O
)	O
.	O

ive	O
tried	O
to	O
fix	O
it	O
with	O
setting	O
ax	O
limits	O
but	O
that	O
does	O
not	O
work	O
.	O

@USER	O
:	O
I	O
added	O
some	O
more	O
info	O
and	O
also	O
a	O
code	O
used	O
.	O

If	O
I	O
wanted	O
to	O
make	O
a	O
combined	O
image	O
like	O
the	O
one	O
shown	O
below	O
(	O
original	O
source	O
here	O
)	O
,	O

When	O
you	O
**	O
add	O
**	O
two	O
**	O
random	O
variables**	O
,	O
the	O
resulting	O
density	O
is	O
the	O
**	O
convolution	O
**	O
of	O
their	O
**	O
densities**	O
.	O

This	O
made	O
the	O
program	O
work	O
just	O
as	O
in	O
Windows	O
,	O
without	O
any	O
other	O
modifications	O
necessary	O
.	O

Using	O
PySide	O
in	O
place	O
of	O
PyQt4	O
results	O
in	O
the	O
same	O
behavior	O
.	O

However	O
,	O
the	O
first	O
item	O
appears	O
to	O
be	O
a	O
float	O
#CODE	O

The	O
code	O
below	O
loads	O
the	O
image	O
data	O
hosted	O
on	O
github	O
and	O
plots	O
it	O
:	O
#CODE	O

I	O
reduce	O
the	O
values	O
to	O
months	O
using	O
:	O
#CODE	O

That	O
would	O
solve	O
Question	O
1	O
.	O

I	O
modified	O
you	O
code	O
,	O
and	O
it	O
can	O
product	O
the	O
same	O
ticks	O
now	O
.	O

`'	O
$\xi$	O
(	O
r	O
)	O
M$_{200}$	O
13.4	O
'`	O
:	O
several	O
`	O
$	O
`	O
,	O
where	O
you	O
should	O
have	O
only	O
one	O
.	O

I	O
just	O
completed	O
a	O
brute	O
force	O
test	O
running	O
through	O
all	O
permutations	O
of	O
[	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
]	O
for	O
the	O
tmp_planes	O
,	O
a	O
total	O
of	O
720	O
different	O
arrangements	O
.	O

The	O
way	O
to	O
fix	O
it	O
is	O
using	O
pip	O
,	O
as	O
mentioned	O
by	O
@USER	O
.	O

Do	O
you	O
mean	O
a	O
scatterplot	O
of	O
your	O
dataset	O
instead	O
?	O

Plotting	O
subplots	O
with	O
secondary	O
continuous	O
y-axis	O
across	O
all	O
subplots	O

However	O
,	O
I	O
want	O
to	O
be	O
able	O
to	O
call	O
something	O
or	O
write	O
some	O
code	O
which	O
then	O
skips	O
any	O
value	O
in	O
the	O
array	O
which	O
would	O
cause	O
this	O
error	O
,	O
then	O
ignoring	O
that	O
row	O
altogether	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

You	O
are	O
typing	O
your	O
code	O
in	O
at	O
the	O
bash	O
(	O
terminal	O
)	O
prompt	O
,	O
not	O
the	O
Python	O
interpreter	O
prompt	O
.	O

Also	O
,	O
if	O
you	O
take	O
a	O
look	O
at	O
Rob's	O
answer	O
,	O
its	O
far	O
simpler	O
than	O
the	O
example	O
shown	O
on	O
the	O
website	O
.	O

MPL	O
is	O
a	O
big	O
project	O
with	O
lots	O
of	O
moving	O
parts	O
,	O
any	O
help	O
keep	O
them	O
up	O
is	O
appreciated	O
.	O

I	O
am	O
less	O
certain	O
if	O
I	O
am	O
solving	O
the	O
correct	O
problem	O
.	O

I	O
am	O
having	O
a	O
problem	O
getting	O
matplotlib	O
to	O
work	O
well	O
with	O
interactive	O
plotting	O
...	O
what	O
I	O
see	O
is	O
that	O
after	O
displaying	O
a	O
few	O
frames	O
of	O
my	O
simulated	O
data	O
matplotlib	O
hangs-and	O
doesn't	O
display	O
any	O
more	O
.	O

If	O
anyone	O
has	O
any	O
idea	O
,	O
I	O
appreciate	O
the	O
help	O
.	O

You	O
can	O
populate	O
a	O
general	O
2d	O
array	O
by	O
:	O
Zhu	O
shows	O
in	O
the	O
answer	O
below	O
.	O

Have	O
a	O
look	O
at	O
this	O
example	O
:	O
#CODE	O

That	O
way	O
,	O
it	O
is	O
possible	O
to	O
see	O
2	O
dots	O
on	O
the	O
same	O
place	O
.	O

This	O
error	O
shows	O
up	O
when	O
the	O
size	O
of	O
a	O
and	O
b	O
(	O
taking	O
from	O
above	O
example	O
)	O
is	O
not	O
the	O
same	O
-	O
so	O
,	O
128	O
x-values	O
here	O
should	O
be	O
plotted	O
against	O
128	O
y-values	O
.	O

[	O
These	O
should	O
all	O
be	O
installed	O
into	O
/	O
usr	O
/	O
local	O
/	O
Cellar	O
,	O
which	O
is	O
the	O
default	O
install	O
location	O
for	O
homebrew	O
.	O
]	O

I'm	O
trying	O
to	O
make	O
an	O
exponential	O
fit	O
to	O
find	O
the	O
Lyapunov	O
Exponent	O
of	O
this	O
data	O
,	O
however	O
,	O
I	O
keep	O
getting	O
this	O
error	O
:	O
#CODE	O

So	O
I	O
wrote	O
an	O
argument	O
`	O
origin=	O
'	O
lower	O
'`	O
.	O

Note	O
that	O
I	O
don't	O
reduce	O
to	O
2	O
,	O
I	O
reduce	O
to	O
10000	O
,	O
and	O
then	O
extract	O
the	O
first	O
2	O
.	O

which	O
you	O
can	O
run	O
with	O
this	O
sample	O
code	O
:	O
#CODE	O

Try	O
converting	O
your	O
the	O
pandas	O
data	O
series	O
to	O
lists	O
or	O
numpy	O
array	O
before	O
plotting	O
.	O

If	O
I	O
de-homogenize	O
only	O
the	O
last	O
column	O
I	O
can	O
find	O
the	O
center	O
of	O
where	O
the	O
ellipse	O
was	O
projected	O
,	O
but	O
I	O
would	O
like	O
to	O
see	O
some	O
shape	O
information	O
as	O
well	O
.	O

Ignoring	O
a	O
problem	O
doesn't	O
fix	O
it	O
.	O

Will	O
add	O
the	O
code	O
.	O

I	O
found	O
that	O
in	O
the	O
folder	O
C	O
:\	O
Program	O
Files\Python27\DLLs	O
I	O
cann't	O
find	O
the	O
_socket	O
file	O

The	O
index	O
is	O
zero-based	O
.	O

The	O
image	O
ggd4	O
for	O
the	O
test	O
can	O
be	O
downloaded	O
from	O
:	O

Load	O
the	O
data	O
sets	O
individually	O
,	O
and	O
then	O
plot	O
each	O
one	O
individually	O
.	O

Have	O
you	O
looked	O
at	O
mplot3d	O
on	O
matplotlib	O
?	O

The	O
columns	O
are	O
floats	O
,	O
I'm	O
not	O
using	O
the	O
index	O
of	O
integers	O
.	O

Incidentally	O
(	O
for	O
someone	O
reading	O
this	O
in	O
the	O
future	O
as	O
you	O
know	O
yours	O
)	O
if	O
you	O
need	O
to	O
find	O
your	O
current	O
Python	O
version	O
you	O
can	O
simply	O
type	O
`	O
python	O
-V	O
`	O
in	O
the	O
command	O
line	O
and	O
it'll	O
return	O
the	O
details	O
,	O
for	O
example	O
mine	O
returns	O
:	O

The	O
code	O
is	O
located	O
at	O

find	O
the	O
file	O
from	O
the	O
link	O

@USER	O
:	O
What	O
do	O
you	O
mean	O
?	O

Thank	O
you	O
in	O
advance	O
for	O
any	O
help	O
you	O
can	O
give	O
me	O

how	O
i	O
can	O
add	O
vector	O
arrows	O
starting	O
from	O
0	O
,	O
0	O
and	O
end	O
at	O
the	O
points	O
that	O
i	O
want	O
?	O

If	O
you	O
want	O
the	O
sum	O
(	O
not	O
the	O
integral	O
)	O
to	O
be	O
one	O
#CODE	O

As	O
I	O
have	O
no	O
idea	O
how	O
long	O
`	O
data_all	O
`	O
is	O
,	O
I	O
created	O
a	O
(	O
random	O
)	O
array	O
,	O
which	O
in	O
my	O
case	O
turned	O
out	O
to	O
give	O
a	O
2D	O
array	O
with	O
512	O
columns	O
(	O
lucky	O
shot	O
)	O
.	O

Do	O
you	O
mean	O
something	O
like	O
this	O
:	O

Can	O
you	O
copy	O
and	O
paste	O
the	O
code	O
lines	O
described	O
in	O
your	O
comment	O
,	O
please	O
?	O

When	O
I	O
print	O
this	O
in	O
A3	O
(	O
good	O
size	O
)	O
it	O
still	O
almost	O
unreadable	O
because	O
the	O
letters	O
are	O
stacking	O
in	O
each	O
other	O
.	O

I	O
mean	O
my	O
grid	O
is	O
20	O
units	O
in	O
x	O
direction	O
however	O
,	O
my	O
resulting	O
plot	O
is	O
10	O
units	O
in	O
x	O
direction	O
with	O
replica	O
plot	O
from	O
10	O
to	O
20	O
units	O
.	O

Now	O
how	O
to	O
plot	O
[	O
M	O
,	O
F	O
+	O
other	O
]	O

To	O
specify	O
that	O
you	O
want	O
the	O
first	O
interval	O
filled	O
with	O
"	O
pure	O
"	O
black	O
,	O
set	O
`	O
vmin=mean	O
(	O
lvls	O
[:	O
1	O
])`	O
in	O
your	O
example	O
.	O

Your	O
Data	O
looks	O
pretty	O
good	O
without	O
any	O
filtering	O
,	O
your	O
"	O
outlieres	O
"	O
all	O
have	O
a	O
very	O
big	O
error	O
and	O
won't	O
affect	O
the	O
fit	O
much	O
.	O

This	O
is	O
a	O
terribly	O
unclear	O
answer	O
,	O
at	O
least	O
some	O
comment	O
in	O
the	O
code	O
or	O
explanation	O
of	O
why	O
this	O
helps	O
floating	O
point	O
error	O
would	O
be	O
nice	O
.	O

I	O
produce	O
a	O
histogram	O
where	O
I	O
put	O
the	O
weights	O
in	O
the	O
parameter	O
#CODE	O

Besides	O
,	O
I	O
see	O
that	O
you	O
use	O
`	O
numpy	O
`	O
in	O
your	O
code	O
,	O
so	O
you	O
can	O
save	O
and	O
load	O
file	O
by	O
`	O
numpy	O
`	O
with	O
a	O
more	O
compact	O
codes	O
like	O
this	O
without	O
using	O
the	O
`	O
csv	O
`	O
module	O
.	O

Usually	O
these	O
sorts	O
of	O
simple	O
array	O
reshaping	O
operations	O
can	O
be	O
done	O
in	O
a	O
couple	O
of	O
lines	O
with	O
Numpy	O
,	O
so	O
I	O
feel	O
like	O
I'm	O
missing	O
something	O
.	O

I	O
don't	O
see	O
any	O
way	O
I	O
could	O
do	O
that	O
.	O

Look	O
at	O
the	O
docs	O
:	O
#CODE	O

To	O
fix	O
that	O
we	O
turn	O
the	O
frame	O
off	O
on	O
`	O
ax1	O
`	O
(	O
so	O
we	O
can	O
see	O
`	O
ax2	O
`)	O
.	O

At	O
one	O
point	O
I	O
was	O
able	O
to	O
trick	O
it	O
into	O
working	O
from	O
the	O
console	O
by	O
installing	O
some	O
thing	O
in	O
the	O
virtualenv	O
,	O
but	O
other	O
things	O
only	O
in	O
the	O
global	O
namespace	O
,	O
but	O
I	O
forgot	O
how	O
I	O
did	O
it	O
.	O

(	O
Technically	O
we	O
could	O
skip	O
the	O
`	O
FigureCanvas	O
`	O
,	O
but	O
it	O
will	O
be	O
needed	O
as	O
soon	O
as	O
we	O
want	O
to	O
save	O
the	O
plot	O
to	O
an	O
image	O
,	O
etc	O
.	O
)	O
#CODE	O

But	O
how	O
can	O
I	O
make	O
plt	O
do	O
all	O
this	O
stuff	O
?	O

From	O
your	O
piece	O
of	O
code	O
it	O
seems	O
to	O
me	O
that	O
`	O
clust_data	O
`	O
is	O
already	O
a	O
list	O
of	O
lists	O
with	O
the	O
correct	O
shape	O
and	O
that	O
`	O
cellText	O
`	O
after	O
being	O
filled	O
is	O
going	O
to	O
be	O
the	O
same	O
of	O
`	O
clust_data	O
`	O
.	O

Anaconda	O
doesn't	O
by	O
default	O
load	O
pylab	O
into	O
IPython	O
so	O
you	O
can	O
choose	O
the	O
backend	O
after	O
launching	O
IPython	O
.	O

Most	O
likely	O
I'll	O
have	O
graphs	O
with	O
longer	O
labels	O
and	O
if	O
I	O
put	O
them	O
near	O
lines	O
,	O
it'll	O
look	O
messy	O
.	O

The	O
lines	O
are	O
random	O
within	O
ranges	O
that	O
cause	O
clustering	O
of	O
lines	O
;	O
a	O
behavior	O
I	O
wanted	O
to	O
verify	O
.	O

have	O
those	O
lists	O
all	O
the	O
same	O
length	O
aka	O
would	O
it	O
be	O
directly	O
compatible	O
witha	O
2d	O
numpy	O
array	O
?	O

Any	O
assistance	O
will	O
be	O
appreciated	O
.	O

I	O
have	O
another	O
working	O
project	O
where	O
the	O
toolbar	O
is	O
in	O
the	O
same	O
panel	O
as	O
the	O
canvas	O
,	O
and	O
I	O
think	O
that	O
might	O
be	O
the	O
key	O
difference	O
between	O
the	O
two	O
.	O

any	O
of	O
#CODE	O

Which	O
way	O
is	O
the	O
easiest	O
to	O
load	O
all	O
txt	O
files	O
?	O

(	O
Note	O
that	O
F	O
is	O
not	O
given	O
in	O
the	O
data	O
,	O
but	O
you	O
gave	O
it	O
as	O
an	O
example	O
)	O

Now	O
I	O
need	O
to	O
put	O
this	O
plot	O
in	O
a	O
PyQt	O
form	O
.	O

Where	O
is	O
the	O
URL	O
API	O
documented	O
?	O

Are	O
there	O
any	O
other	O
option	O
?	O

You	O
need	O
some	O
criteria	O
to	O
select	O
out	O
the	O
"	O
best	O
"	O
in	O
your	O
context	O
.	O

What	O
do	O
you	O
mean	O
when	O
you	O
say	O
`	O
the	O
first	O
histogram	O
will	O
be	O
the	O
first	O
column	O
as	O
the	O
x	O
values	O
plotted	O
against	O
the	O
second	O
column	O
as	O
the	O
y-values	O
`	O
.	O

Unfortunately	O
I	O
don't	O
have	O
any	O
experience	O
doing	O
this	O
so	O
won't	O
be	O
able	O
to	O
help	O
.	O

A	O
sample	O
of	O
my	O
data	O
to	O
plot	O
:	O
#CODE	O

Also	O
,	O
I	O
do	O
not	O
want	O
user	O
intermediation	O
at	O
all	O
!	O

Rather	O
than	O
put	O
together	O
an	O
example	O
from	O
scratch	O
,	O
there's	O
an	O
excellent	O
example	O
of	O
this	O
written	O
by	O
Paul	O
Ivanov	O
in	O
the	O
matplotlib	O
examples	O
(	O
It's	O
only	O
in	O
the	O
current	O
git	O
tip	O
,	O
as	O
it	O
was	O
only	O
committed	O
a	O
few	O
months	O
ago	O
.	O
It's	O
not	O
on	O
the	O
webpage	O
yet	O
.	O
)	O
.	O

what	O
to	O
do	O
next	O
?	O

I	O
think	O
you'll	O
find	O
you	O
get	O
better	O
help	O
if	O
you	O
describe	O
*	O
what	O
*	O
you're	O
trying	O
to	O
do	O
,	O
rather	O
than	O
*	O
how	O
*	O
you	O
want	O
to	O
do	O
it	O
.	O

(	O
ie	O
save	O
etc	O
)	O
.	O
ill	O
have	O
a	O
look	O
at	O
the	O
toolkits	O
:D	O

in	O
place	O
of	O
#CODE	O

You	O
could	O
cast	O
the	O
array	O
to	O
a	O
list	O
:	O
#CODE	O

Add	O
as	O
many	O
symlinks	O
to	O
the	O
directory	O
as	O
you	O
wish	O
,	O
it	O
gives	O
you	O
fine-grained	O
control	O
over	O
which	O
packages	O
you	O
want	O
to	O
make	O
accessible	O
.	O

In	O
the	O
end	O
,	O
I	O
found	O
out	O
that	O
the	O
problem	O
was	O
simply	O
that	O
matplotlib	O
could	O
not	O
find	O
any	O
backend	O
for	O
plotting	O
.	O

Please	O
change	O
your	O
title	O
to	O
describe	O
your	O
_actual_	O
issue	O
.	O

If	O
you	O
did	O
not	O
create	O
one	O
when	O
you	O
installed	O
matplotlib	O
,	O
you	O
can	O
get	O
this	O
template	O
from	O
the	O
matplotlib	O
source	O
,	O
or	O
from	O
the	O
matplotlib	O
website	O
.	O

I	O
am	O
sorry	O
to	O
be	O
so	O
needy	O
,	O
but	O
I	O
really	O
cant	O
find	O
any	O
useful	O
information	O
anywhere	O
.	O

Or	O
is	O
there	O
another	O
way	O
to	O
change	O
the	O
size	O
?	O

I	O
did	O
find	O
the	O
way	O
to	O
make	O
my	O
images	O
almost	O
what	O
I	O
wanted	O
with	O
:	O
#CODE	O

any	O
idea	O
?	O

@USER	O
:	O
I	O
suppose	O
the	O
specific	O
way	O
I	O
have	O
it	O
set	O
up	O
would	O
work	O
better	O
if	O
you	O
had	O
,	O
say	O
,	O
a	O
1000x1000	O
matrix	O
,	O
as	O
it	O
lets	O
numpy	O
pick	O
how	O
the	O
tics	O
should	O
be	O
spaced	O
.	O

As	O
you	O
can	O
see	O
it	O
only	O
covers	O
half	O
of	O
the	O
matrix	O
.	O

You	O
iterate	O
the	O
items	O
in	O
`	O
glass	O
`	O
,	O
but	O
then	O
compare	O
to	O
the	O
whole	O
list	O
instead	O
of	O
to	O
the	O
current	O
`	O
item	O
`	O

If	O
that	O
helps	O
,	O
this	O
is	O
the	O
process	O
that	O
I	O
am	O
doing	O
to	O
save	O
and	O
plot	O
:	O
#CODE	O

Please	O
consider	O
answering	O
your	O
own	O
question	O
or	O
removing	O
it	O
all	O
together	O
if	O
you	O
feel	O
it	O
does	O
not	O
add	O
.	O

Now	O
"	O
print	O
zi	O
"	O
gives	O
me	O
an	O
array	O
like	O
before	O
but	O
with	O
nan	O
everywhere	O
.	O

Im	O
not	O
by	O
computer	O
to	O
test	O
.	O

I	O
want	O
to	O
put	O
my	O
custom	O
datetime	O
format	O
but	O
when	O
I	O
tried	O
my	O
custom	O
date	O
format	O
is	O
overlapped	O
over	O
the	O
date	O
given	O
by	O
default	O
by	O
pandas	O
plot	O
.	O

Change	O
formatting	O
on	O
datetime	O
ticks	O
when	O
plotting	O
daily	O
mean	O
with	O
Pandas	O
/	O
matplotlib	O

Any	O
other	O
way	O
to	O
get	O
a	O
similar	O
effect	O
?	O

If	O
you	O
see	O
the	O
right	O
side	O
,	O
the	O
size	O
is	O
changed	O
.	O

How	O
can	O
I	O
select	O
multiple	O
data	O
points	O
with	O
matplotlib	O
and	O
export	O
it	O
?	O

Two	O
steps	O
to	O
sketch	O
an	O
idea	O
to	O
solve	O
this	O
:	O

This	O
is	O
not	O
necessarily	O
the	O
nicest	O
possible	O
way	O
of	O
doing	O
things	O
,	O
and	O
you	O
may	O
bump	O
into	O
resolution	O
problems	O
,	O
as	O
well	O
(	O
check	O
the	O
size	O
of	O
the	O
`	O
mayavi	O
`	O
window	O
)	O
.	O

Is	O
there	O
a	O
way	O
I	O
can	O
get	O
y=x	O
to	O
show	O
up	O
even	O
if	O
I	O
don't	O
have	O
a	O
list	O
of	O
all	O
the	O
points	O
that	O
I	O
plotted	O
?	O

Knowing	O
all	O
that	O
,	O
your	O
example	O
becomes	O
:	O
#CODE	O

Any	O
Ideas	O
what	O
might	O
be	O
the	O
problem	O
?	O

It	O
appear	O
that	O
these	O
disappear	O
when	O
I	O
add	O
the	O
`	O
facecolors=UWR	O
(	O
heatmap	O
)`	O
property	O
to	O
the	O
`	O
surf	O
(	O
...	O
)`	O
.	O

Can	O
you	O
post	O
the	O
full	O
traceback	O
?	O

The	O
full	O
list	O
of	O
positions	O
is	O
given	O
here	O
:	O
#URL	O

unutbu's	O
answer	O
should	O
fix	O
it	O
everywhere	O
.	O

how	O
can	O
i	O
insert	O
the	O
plot	O
returned	O
by	O
create_predef_plot	O
into	O
f2	O
#CODE	O

And	O
the	O
formatted	O
ones	O
(	O
Used	O
as	O
_id	O
of	O
the	O
documents	O
after	O
using	O
aggregation	O
):	O
#CODE	O

Any	O
packages	O
you	O
accidentally	O
installed	O
for	O
other	O
Python	O
installations	O
,	O
just	O
reinstall	O
them	O
for	O
MacPorts	O
.	O

However	O
,	O
other	O
programs	O
seem	O
to	O
be	O
ok	O
with	O
filling	O
almost	O
all	O
the	O
memory	O
before	O
the	O
swap	O
is	O
used	O
.	O

I'm	O
not	O
sure	O
how	O
to	O
tackle	O
this	O
problem	O
and	O
any	O
help	O
or	O
direction	O
would	O
be	O
very	O
appreciated	O
.	O

i	O
looked	O
at	O
sage	O
,	O
the	O
source	O
code	O
is	O
huge	O
,	O
does	O
it	O
support	O
python	O
out	O
of	O
the	O
box	O
?	O

If	O
any	O
one	O
know	O
about	O
it	O
then	O
please	O
reply	O
me	O
.	O

I'll	O
have	O
a	O
look	O
at	O
it	O

The	O
main	O
issue	O
is	O
that	O
you	O
can't	O
make	O
UI	O
changes	O
from	O
a	O
separate	O
process	O
from	O
the	O
main	O
UI	O
thread	O
(	O
the	O
one	O
that	O
all	O
of	O
your	O
Qt	O
calls	O
are	O
in	O
)	O
.	O

I	O
say	O
"	O
quick	O
fix	O
"	O
because	O
there	O
are	O
more	O
changes	O
that	O
could	O
be	O
recommended	O
.	O

Maybe	O
look	O
at	O
the	O
scikit-learn	O
docs	O
for	O
inspiration	O
?	O

Labels	O
are	O
located	O
at	O
the	O
right	O
place	O
,	O
but	O
label	O
is	O
not	O
correct	O
.	O

I	O
ran	O
the	O
following	O
code	O
to	O
get	O
two	O
plots	O
next	O
to	O
each	O
other	O
(	O
it	O
is	O
a	O
minimal	O
working	O
example	O
that	O
you	O
can	O
copy	O
):	O
#CODE	O

here	O
is	O
the	O
code	O
where	O
I	O
create	O
the	O
plot	O
and	O
a	O
picture	O
of	O
this	O
:	O

Admittedly	O
,	O
I'm	O
new	O
to	O
pandas	O
but	O
it	O
seems	O
like	O
at	O
every	O
turn	O
I	O
am	O
stopped	O
,	O
by	O
my	O
ignorance	O
no	O
doubt	O
,	O
but	O
it's	O
getting	O
tiresome	O
.	O

Are	O
all	O
of	O
your	O
modes	O
needed	O
to	O
demonstrate	O
the	O
problem	O
?	O

Note	O
that	O
the	O
explicit	O
masking	O
is	O
no	O
longer	O
necessary	O
in	O
matplotlib	O
master	O
as	O
arrays	O
are	O
now	O
masked	O
automatically	O
internally	O
.	O

(	O
The	O
disadvantage	O
is	O
that	O
this	O
method	O
is	O
much	O
less	O
flexible	O
.	O
):	O
#CODE	O

Python	O
&	O
Matplotlib	O
:	O
creating	O
a	O
chart	O
first	O
and	O
showing	O
it	O
at	O
a	O
second	O
moment	O

Put	O
#CODE	O

If	O
you	O
require	O
a	O
completely	O
manual	O
installation	O
,	O
you	O
should	O
give	O
those	O
instructions	O
a	O
thorough	O
read	O
.	O

Take	O
a	O
look	O
at	O
this	O
example	O
.	O

I	O
tried	O
something	O
like	O
that	O
already	O
,	O
but	O
when	O
I	O
plot	O
each	O
data	O
point	O
individually	O
in	O
a	O
for-loop	O
,	O
I	O
see	O
no	O
way	O
to	O
use	O
a	O
colormap	O
and	O
colorbar	O
for	O
all	O
of	O
them	O
.	O

Here's	O
how	O
to	O
do	O
it	O
:	O
if	O
you	O
find	O
that	O
an	O
answer	O
has	O
been	O
helpful	O
to	O
you	O
,	O
then	O
move	O
your	O
mouse	O
over	O
the	O
upper	O
left-hand	O
corner	O
of	O
that	O
answer	O
(	O
where	O
the	O
answer	O
"	O
score	O
"	O
appears	O
between	O
two	O
gray	O
triangles	O
)	O
.	O

Can	O
you	O
recommend	O
any	O
tutorial	O
for	O
MATLAB	O
speakers	O
?	O

The	O
second	O
plot	O
keeps	O
rescaling	O
the	O
x-axis	O
for	O
me	O
,	O
losing	O
the	O
overview	O
of	O
the	O
full	O
column	O
1	O
plot	O
.	O

Thanks	O
for	O
the	O
answer	O
,	O
I	O
searched	O
for	O
event	O
handling	O
examples	O
but	O
could	O
not	O
find	O
it	O
(	O
I	O
am	O
newbie	O
to	O
python	O
)	O
,	O
can	O
you	O
please	O
send	O
me	O
link	O
to	O
at	O
least	O
one	O
such	O
example	O
.	O

You	O
can	O
find	O
it	O
here	O
.	O

(	O
I	O
know	O
I	O
add	O
nothing	O
new	O
,	O
but	O
the	O
straightforward	O
answer	O
should	O
be	O
visible	O
)	O
.	O

So	O
,	O
I	O
don't	O
even	O
know	O
where	O
to	O
start	O
looking	O

When	O
I	O
try	O
to	O
run	O
the	O
example	O
script	O
at	O

Thanks	O
for	O
your	O
comment	O
,	O
I	O
have	O
put	O
everything	O
,	O
I	O
wanna	O
plot	O
(	O
years	O
,	O
Frequency_accidents_years	O
,	O
regions	O
)	O

Via	O
`	O
apt-get	O
`	O
,	O
I've	O
tried	O
installing	O
ffmpeg	O
,	O
every	O
codec	O
imaginable	O
,	O
and	O
even	O
tried	O
to	O
compile	O
ffmpeg	O
from	O
source	O
.	O

@USER	O
This	O
isn't	O
a	O
bug	O
,	O
but	O
a	O
design	O
choice	O
.	O

The	O
left-hand	O
side	O
can	O
,	O
however	O
,	O
contain	O
any	O
number	O
of	O
elements	O
,	O
and	O
provided	O
it	O
is	O
a	O
tuple	O
of	O
variables	O
the	O
unpacking	O
will	O
take	O
place	O
.	O

You	O
want	O
to	O
look	O
at	O
the	O
OO	O
interface	O
,	O
not	O
the	O
state	O
machine	O
interface	O
for	O
matplotlib	O
.	O

This	O
would	O
be	O
an	O
interesting	O
thing	O
to	O
add	O
to	O
the	O
emerging	O
thoughts	O
about	O
semantic	O
objects	O
in	O
mpl	O
.	O

Maybe	O
you	O
could	O
post	O
a	O
sample	O
of	O
your	O
data	O
array	O
?	O

I	O
believe	O
this	O
is	O
because	O
I'm	O
plotting	O
the	O
image	O
wrong	O
and	O
so	O
it	O
is	O
plotting	O
all	O
the	O
data	O
as	O
0	O
.	O

Try	O
running	O
with	O
`	O
python	O
-X	O
faulthandler	O
`	O
and	O
see	O
if	O
you	O
get	O
a	O
useful	O
traceback	O
at	O
the	O
segfault	O
.	O

Notice	O
how	O
only	O
one	O
body	O
has	O
been	O
coloured	O
,	O
all	O
other	O
disjoint	O
polygons	O
remain	O
white	O
:	O

Thanks	O
@USER	O
,	O
All	O
I	O
want	O
set	O
a	O
plot	O
title	O
based	O
on	O
the	O
what	O
user	O
is	O
selected	O
on	O
the	O
plot	O
,	O
for	O
example	O
if	O
the	O
plot	O
shows	O
a	O
data	O
from	O
'	O
Jan	O
2012	O
'	O
to	O
'	O
July	O
2014	O
'	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Years	O
'	O
and	O
if	O
the	O
user	O
selects	O
from	O
'	O
May	O
2014	O
'	O
to	O
'	O
July	O
2014	O
'	O
then	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Months	O
'	O
and	O
if	O
the	O
user	O
selects	O
from	O
'	O
July	O
2014	O
'	O
to	O
'	O
July	O
2104	O
'	O
then	O
dynamically	O
want	O
to	O
set	O
the	O
title	O
to	O
'	O
Days	O
'	O
.	O

I	O
would	O
like	O
to	O
know	O
if	O
I	O
have	O
a	O
triangular	O
marker	O
,	O
is	O
it	O
possible	O
to	O
control	O
its	O
orientation	O
?	O

I	O
like	O
to	O
sort	O
these	O
points	O
row	O
by	O
row	O
.	O

Before	O
comparing	O
between	O
these	O
two	O
PDFs	O
,	O
I	O
would	O
like	O
to	O
compare	O
original	O
time	O
series	O
and	O
the	O
series	O
of	O
Gaussian	O
distributed	O
numbers	O
with	O
the	O
standard	O
deviation	O
and	O
mean	O
from	O
original	O
wind	O
velocity	O
series	O
.	O

I	O
will	O
like	O
create	O
a	O
alpha	O
channel	O
,	O
for	O
explanation	O
I	O
put	O
a	O
example	O
:	O

The	O
function	O
is	O
supposed	O
to	O
be	O
smooth	O
and	O
connect	O
at	O
0	O
and	O
2	O
pi	O
in	O
the	O
y	O
range	O
of	O
(	O
0	O
,	O
2pi	O
)	O
not	O
touching	O
0	O
and	O
2pi	O
.	O

I	O
have	O
run	O
a	O
similar	O
script	O
before	O
,	O
and	O
I	O
think	O
I've	O
imported	O
all	O
the	O
relevant	O
modules	O
.	O

The	O
sample	O
image	O
makes	O
things	O
clear	O
.	O

Take	O
Care	O
,	O
George	O

I	O
don't	O
know	O
of	O
any	O
movie	O
format	O
that	O
is	O
vector	O
based	O
(	O
but	O
I	O
don't	O
know	O
a	O
lot	O
about	O
video	O
codecs	O
)	O
.	O

I	O
can't	O
find	O
anything	O
like	O
that	O
in	O
the	O
galery	O
and	O
all	O
my	O
attempts	O
do	O
deal	O
with	O
the	O
colorbar	O
failed	O
.	O

I	O
encourage	O
you	O
to	O
try	O
all	O
of	O
the	O
great	O
ideas	O
people	O
are	O
telling	O
you	O
,	O
and	O
I'm	O
interested	O
to	O
see	O
if	O
any	O
of	O
them	O
work	O
!	O

Have	O
you	O
tried	O
setting	O
your	O
backend	O
to	O
SVG	O
so	O
that	O
all	O
of	O
your	O
images	O
are	O
in	O
vector	O
format	O
?	O

I	O
appreciate	O
any	O
help	O
:)	O

Would	O
be	O
better	O
to	O
change	O
the	O
title	O

My	O
csv	O
is	O
like	O
:	O
id	O
,	O
author	O
,	O
title	O
,	O
language	O

After	O
making	O
all	O
of	O
the	O
changes	O
listed	O
in	O
steps	O
2	O
and	O
3	O
(	O
and	O
correcting	O
the	O
spelling	O
of	O
the	O
title	O
)	O
,	O
I	O
get	O
the	O
following	O
very-reasonable-looking	O
plot	O
:	O

When	O
I	O
look	O
at	O
the	O
plot	O
,	O
the	O
x	O
range	O
ends	O
up	O
being	O
from	O
0	O
to	O
12000	O
.	O

It	O
was	O
still	O
at	O
-1	O
when	O
I	O
saw	O
it	O
.	O

when	O
the	O
bad	O
points	O
are	O
deleted	O
,	O
the	O
user	O
can	O
click	O
on	O
different	O
buttons	O
to	O
classify	O
the	O
source	O
(	O
lets	O
say	O
"	O
star	O
"	O
,	O
"	O
galaxy	O
"	O
,	O
"	O
whatever	O
")	O
.	O

Your	O
`	O
Z	O
`	O
'	O
array	O
'	O
is	O
actually	O
a	O
list	O
of	O
numpy	O
arrays	O
of	O
different	O
lengths	O
.	O

when	O
the	O
2D	O
array	O
is	O
viewed	O
as	O
a	O
color	O

I	O
looked	O
through	O
the	O
examples	O
in	O
MatPlotLib	O
or	O
R	O
and	O
they	O
all	O
seem	O
to	O
already	O
start	O
with	O
random	O
data	O
to	O
generate	O
the	O
image	O
.	O

Filtering	O
spreads	O
out	O
the	O
information	O
and	O
makes	O
the	O
inner	O
contours	O
disappear	O
.	O

This	O
is	O
probably	O
related	O
to	O
upgrading	O
the	O
library	O
,	O
and	O
having	O
old	O
versions	O
of	O
the	O
javascript	O
hanging	O
around	O
.	O

can	O
you	O
explain	O
your	O
data	O
format	O
in	O
a	O
little	O
more	O
detail	O
and	O
give	O
us	O
some	O
copy	O
/	O
paste-able	O
code	O
to	O
create	O
a	O
toy	O
version	O
of	O
it	O
?	O

'	O
To	O
open	O
"	O
Python	O
,	O
"	O
you	O
need	O
to	O
install	O
X11	O
'	O
.	O

@USER	O
:	O
You	O
can	O
certainly	O
change	O
the	O
exact	O
shape	O
by	O
modifying	O
the	O
factor	O
"	O
3	O
"	O
in	O
front	O
of	O
the	O
lower	O
branch	O
.	O

Downvotes	O
are	O
due	O
to	O
the	O
lack	O
of	O
research	O
effort	O
(	O
if	O
you	O
look	O
at	O
the	O
reasons	O
for	O
downvotes	O
,	O
it	O
says	O
"	O
This	O
question	O
does	O
not	O
show	O
any	O
research	O
effort	O
")	O
.	O

As	O
for	O
the	O
matplotlib	O
version	O
1.1.0	O
,	O
it's	O
a	O
really	O
easy	O
install	O
,	O
at	O
least	O
on	O
my	O
by-now-quite	O
old	O
Lucid	O
it's	O
no	O
problem	O
at	O
all	O
.	O

Numpy	O
index	O
out	O
of	O
range	O
error	O

Though	O
I	O
want	O
to	O
add	O
that	O
manually	O
saving	O
the	O
chart	O
from	O
Qt	O
Widget	O
is	O
OK	O
.	O

Without	O
seeing	O
any	O
of	O
the	O
existing	O
plotting	O
functions	O
,	O
how	O
am	O
I	O
supposed	O
to	O
know	O
how	O
similar	O
or	O
different	O
they	O
are	O
?	O

You	O
can	O
use	O
spline	O
to	O
fit	O
the	O
[	O
blue	O
curve	O
-	O
peak	O
/	O
2	O
]	O
,	O
and	O
then	O
find	O
it's	O
roots	O
:	O
#CODE	O

I	O
had	O
to	O
add	O
`	O
colorbar	O
(	O
im2	O
)`	O
right	O
after	O
the	O
definition	O
of	O
im2	O
...	O

Creating	O
sample	O
data	O
#CODE	O

The	O
numbers	O
choose	O
now	O
many	O
decimal	O
places	O
you	O
want	O
and	O
the	O
e	O
chooses	O
scientific	O
notation	O
.	O

To	O
clear	O
up	O
the	O
code	O
,	O
I	O
put	O
the	O
individual	O
plotting	O
functions	O
for	O
the	O
six	O
or	O
so	O
groups	O
of	O
subplots	O
into	O
separate	O
submodules	O
and	O
call	O
them	O
with	O
the	O
arrays	O
they	O
need	O
for	O
plotting	O
purposes	O
.	O

You	O
have	O
given	O
us	O
a	O
list	O
of	O
requirements	O
,	O
but	O
what	O
have	O
you	O
tried	O
to	O
solve	O
your	O
own	O
problem	O
?	O

For	O
that	O
purpose	O
,	O
I	O
have	O
put	O
the	O
following	O
command	O
in	O
a	O
for	O
loop	O
:	O
#CODE	O

I	O
work	O
at	O
a	O
large	O
company	O
and	O
do	O
not	O
have	O
admin	O
rights	O
.	O

Note	O
that	O
it	O
only	O
detect	O
truly	O
equal	O
placements	O
,	O
if	O
you	O
would	O
have	O
values	O
of	O
`	O
[	O
0	O
,	O
0.00001	O
,	O
2	O
,	O
10	O
]`	O
,	O
they	O
would	O
probably	O
still	O
overlap	O
.	O

Suppose	O
I	O
have	O
a	O
triangle	O
mesh	O
defined	O
by	O
X	O
,	O
Y	O
,	O
and	O
Z	O
,	O
the	O
3D	O
coordinates	O
of	O
a	O
point	O
cloud	O
,	O
each	O
a	O
vector	O
of	O
length	O
n	O
,	O
and	O
UVW	O
,	O
a	O
2D	O
m-x-3	O
matrix	O
in	O
which	O
each	O
row	O
is	O
a	O
triplet	O
of	O
indices	O
into	O
the	O
point	O
cloud	O
.	O

I	O
am	O
trying	O
to	O
create	O
a	O
plot	O
with	O
vertical	O
bars	O
equal	O
to	O
score	O
length	O
,	O
AND	O
with	O
a	O
radiobutton	O
which	O
enables	O
the	O
user	O
to	O
choose	O
which	O
level	O
scores	O
he	O
/	O
she	O
wants	O
to	O
see	O
,	O
as	O
such	O
:	O

The	O
2nd	O
set	O
of	O
data	O
will	O
also	O
include	O
the	O
first	O
as	O
written	O
in	O
this	O
snippet	O
(	O
looking	O
for	O
good	O
way	O
to	O
delete	O
it	O
that	O
doesn't	O
cause	O
problems	O
later	O
down	O
the	O
line	O
)	O
.	O

So	O
you	O
need	O
to	O
add	O
`	O
u	O
`	O
:	O
#CODE	O

Regardless	O
,	O
once	O
you	O
get	O
used	O
to	O
it	O
,	O
I	O
find	O
it	O
much	O
easier	O
to	O
write	O
reusable	O
plotting	O
functions	O
in	O
python	O
with	O
matplotlib	O
than	O
in	O
Matlab	O
.	O

Now	O
,	O
you	O
should	O
be	O
able	O
to	O
get	O
all	O
the	O
libraries	O
you	O
need	O
just	O
as	O
easily	O
,	O
using	O
,	O
too	O
,	O
Macports	O
.	O

I	O
believe	O
that	O
I'm	O
using	O
the	O
TK	O
backend	O
;	O
I'm	O
not	O
married	O
to	O
that	O
choice	O
,	O
but	O
it's	O
not	O
clear	O
how	O
I	O
should	O
install	O
other	O
backends	O
.	O

What	O
is	O
this	O
float	O
,	O
and	O
how	O
can	O
I	O
convert	O
it	O
to	O
a	O
datetime	O
?	O

xtics	O
and	O
ytics	O
labels	O
because	O
they	O
appear	O
for	O
all	O
values	O
of	O
xPoints	O
and	O
yPoints	O
(	O
they	O
overlap	O
)	O
.	O

matplotlib	O
(	O
equal	O
unit	O
length	O
)	O

Is	O
there	O
any	O
reason	O
why	O
I	O
can't	O
do	O
this	O
?	O

I	O
am	O
using	O
the	O
following	O
codes	O
to	O
generate	O
the	O
square	O
wave	O
format	O
[	O
Eg	O
:	O
from	O
0	O
till	O
5	O
]	O
using	O
for	O
loop	O
.	O

Can	O
you	O
reduce	O
the	O
extent	O
of	O
you	O
problem	O
by	O
calculating	O
something	O
akin	O
to	O
a	O
density	O
,	O
reducing	O
millions	O
of	O
line	O
segments	O
to	O
maybe	O
a	O
few	O
hundred	O
or	O
a	O
few	O
thousand	O
data	O
points	O
?	O

array	O
erroneously	O
filled	O
with	O
the	O
same	O
value	O

You	O
can	O
change	O
the	O
`	O
reduce_C_function	O
`	O
option	O
if	O
you	O
want	O
something	O
different	O
than	O
a	O
simple	O
average	O
.	O

The	O
whole	O
traceback	O
is	O
rather	O
large	O
but	O
seems	O
to	O
revolve	O
around	O
this	O
part	O
(	O
if	O
I	O
read	O
it	O
correctly	O
):	O
#CODE	O

Matplotlib	O
:	O
WebAgg	O
backend	O
doesn't	O
show	O
any	O
figures	O

While	O
this	O
seems	O
what	O
I	O
was	O
looking	O
for	O
,	O
I'm	O
not	O
lucky	O
finding	O
any	O
proper	O
documentation	O
for	O
webagg	O
.	O

If	O
I	O
were	O
you	O
,	O
I'd	O
start	O
from	O
this	O
example	O
to	O
see	O
where	O
do	O
these	O
lines	O
start	O
showing	O
up	O
.	O

According	O
to	O
efiring	O
,	O
matplotlib	O
does	O
not	O
support	O
NumPy	O
datetime64	O
objects	O
(	O
at	O
least	O
not	O
yet	O
)	O
.	O

At	O
least	O
on	O
my	O
system	O
(	O
OSX	O
,	O
Python	O
2.7	O
,	O
mpl	O
1.1.0	O
)	O
,	O
I	O
don't	O
have	O
any	O
issues	O
with	O
panning	O
,	O
etc	O
.	O

Yeah	O
,	O
I	O
understand	O
why	O
Pylab	O
exists	O
,	O
and	O
why	O
it	O
seems	O
so	O
attractive	O
to	O
people	O
moving	O
over	O
from	O
MATLAB	O
,	O
but	O
over	O
the	O
long	O
term	O
I	O
feel	O
like	O
it's	O
a	O
net	O
negative	O
because	O
it	O
makes	O
it	O
harder	O
to	O
share	O
code	O
with	O
and	O
get	O
help	O
from	O
everyone	O
who	O
isn't	O
using	O
it	O
.	O

How	O
can	O
I	O
get	O
around	O
this	O
?	O

Function	O
of	O
Numpy	O
Array	O
with	O
if-statement	O

By	O
default	O
it	O
sets	O
the	O
(	O
0	O
,	O
0	O
)	O
element	O
of	O
the	O
array	O
in	O
the	O
upper	O
left	O
corner	O
.	O

I	O
appreciate	O
any	O
help	O
with	O
this	O
.	O

At	O
the	O
moment	O
I	O
do	O
a	O
for	O
loop	O
and	O
plot	O
each	O
point	O
:	O
#CODE	O

What	O
I	O
currently	O
have	O
to	O
convert	O
it	O
:	O
#CODE	O

You	O
should	O
post	O
your	O
new	O
code	O
and	O
the	O
**	O
full	O
**	O
traceback	O
in	O
another	O
question	O
.	O

I	O
don't	O
want	O
to	O
make	O
any	O
translations	O
/	O
shifts	O
to	O
the	O
data	O
itself	O
(	O
it's	O
a	O
large	O
dataset	O
,	O
etc	O
)	O
,	O
so	O
I'd	O
be	O
looking	O
to	O
do	O
this	O
with	O
some	O
matplotlib-trickery	O
.	O

Raster	O
to	O
Numpy	O
Array	O
-	O
how	O
to	O
change	O
a	O
default	O
color-scheme	O
of	O
a	O
matplotlib	O
plot	O

or	O
invert	O
the	O
Y-axis	O
:	O
#CODE	O

So	O
,	O
a	O
solution	O
is	O
simply	O
to	O
create	O
a	O
list	O
of	O
all	O
objects	O
that	O
will	O
be	O
refreshed	O
,	O
and	O
to	O
call	O
this	O
`	O
remove	O
`	O
method	O
for	O
everyone	O
of	O
them	O
:	O

The	O
best	O
choice	O
is	O
to	O
make	O
your	O
widgets	O
in	O
the	O
order	O
you	O
wish	O
them	O
to	O
appear	O
.	O

Python	O
:	O
How	O
to	O
generate	O
a	O
power	O
law	O
graph	O

Unfortunately	O
the	O
broken	O
command	O
is	O
also	O
preventing	O
me	O
from	O
tryig	O
this	O
,	O
so	O
I'll	O
have	O
to	O
fix	O
that	O
first	O
.	O

(	O
`	O
1	O
`	O
is	O
the	O
the	O
start	O
index	O
,	O
`	O
2	O
`	O
is	O
the	O
increment	O
)	O
In	O
contrast	O
,	O
`	O
x	O
[:	O
:	O
-1	O
]`	O
just	O
specifies	O
an	O
increment	O
of	O
`	O
-1	O
`	O
,	O
thus	O
reversing	O
the	O
array	O
.	O

One	O
way	O
around	O
this	O
is	O
not	O
to	O
use	O
the	O
pandas	O
`	O
plot	O
`	O
method	O
,	O
but	O
to	O
directly	O
the	O
matplotlib's	O
`	O
plot	O
`	O
function	O
.	O

where	O
`	O
numctrl_ccm90	O
`	O
and	O
`	O
numctrl_ucp90	O
`	O
are	O
the	O
widgets	O
corresponding	O
to	O
the	O
collimated	O
counts	O
at	O
-90deg	O
and	O
the	O
uncollimated	O
counts	O
+90deg	O
etc	O
.	O

Any	O
tips	O
?	O

I	O
would	O
appreciate	O
any	O
comment	O
or	O
suggestion	O
.	O

I	O
looked	O
at	O
`	O
masked_array	O
`	O
which	O
is	O
different	O
.	O

Without	O
having	O
all	O
the	O
code	O
listed	O
,	O
I	O
had	O
to	O
make	O
some	O
assumptions	O
about	O
the	O
data	O
and	O
about	O
what	O
you	O
wanted	O
.	O

You	O
mean	O
`	O
Axes3D	O
`	O
is	O
different	O
from	O
`	O
axes3d	O
`	O
?	O

If	O
you	O
already	O
have	O
python	O
and	O
gfortran	O
and	O
such	O
,	O
jump	O
in	O
at	O
the	O
point	O
where	O
you	O
need	O
.	O

Maybe	O
you	O
know	O
a	O
fix	O
for	O
this	O
?	O

How	O
can	O
I	O
fix	O
this	O
?	O

The	O
background	O
is	O
that	O
the	O
array's	O
values	O
represent	O
the	O
water	O
depth	O
of	O
a	O
square	O
grid	O
.	O

And	O
try	O
debugging	O
one	O
step	O
at	O
a	O
time	O
:	O
make	O
sure	O
you	O
have	O
good	O
data	O
before	O
you	O
try	O
to	O
plot	O
it	O
.	O

What	O
do	O
you	O
mean	O
the	O
"	O
label	O
"	O
?	O

To	O
do	O
this	O
we	O
ll	O
use	O
the	O
standard	O
y=mx+b	O
line	O
equation	O
where	O
m	O
is	O
the	O
line	O
s	O
slope	O
and	O
b	O
is	O
the	O
line	O
s	O
y-intercept	O
.	O

If	O
you	O
check	O
the	O
Matplotlib	O
example	O
for	O
the	O
function	O
you	O
are	O
using	O
,	O
you	O
will	O
notice	O
they	O
use	O
a	O
similar	O
methodology	O
:	O
build	O
empty	O
matrix	O
and	O
fill	O
it	O
with	O
strings	O
built	O
with	O
the	O
interpolation	O
method	O
.	O

But	O
the	O
question	O
still	O
remains-	O
how	O
do	O
I	O
fix	O
it	O
?	O

Based	O
on	O
the	O
plot	O
selection	O
I	O
want	O
to	O
set	O
the	O
plot	O
title	O
,	O
so	O
I	O
need	O
to	O
know	O
the	O
current	O
x	O
-axis	O
value	O
after	O
the	O
plot	O
is	O
shown	O
.	O

I	O
have	O
tried	O
the	O
following	O
code	O
but	O
this	O
creates	O
2	O
separate	O
charts	O
but	O
I	O
would	O
like	O
this	O
all	O
on	O
one	O
chart	O
.	O

Any	O
advice	O
would	O
be	O
greatly	O
appreciated	O
.	O

How	O
can	O
I	O
add	O
colorbar	O
to	O
this	O
code	O
?	O

I'm	O
reading	O
in	O
a	O
file	O
with	O
several	O
columns	O
,	O
each	O
of	O
which	O
contains	O
around	O
100	O
values	O
between	O
1	O
and	O
10	O
.	O

But	O
for	O
the	O
all	O
three	O
plots	O
We	O
have	O
same	O
temperature	O
T=4K	O
.	O

I've	O
downloaded	O
a	O
number	O
of	O
nifty	O
programs	O
and	O
utilities	O
(	O
and	O
info	O
sites	O
)	O
/	O
quantum	O
gis	O
,	O
virtual	O
terrain	O
project	O
,	O
grass	O
,	O
I'm	O
looking	O
to	O
get	O
an	O
xyz	O
csv	O
file	O
into	O
a	O
format	O
that	O
these	O
tools	O
can	O
use	O
.	O

I	O
have	O
all	O
the	O
plots	O
as	O
several	O
modules	O
in	O
python	O
/	O
matplotlib	O
already	O
coded	O
,	O
so	O
that's	O
perfect	O
.	O

At	O
some	O
level	O
,	O
this	O
is	O
purely	O
a	O
matter	O
of	O
taste	O
and	O
depends	O
a	O
bit	O
on	O
what	O
you	O
are	O
doing	O
.	O

I	O
really	O
want	O
to	O
plot	O
all	O
the	O
points	O
in	O
my	O
matrix	O
instead	O
of	O
the	O
(	O
M-1	O
)	O
x	O
(	O
N-1	O
)	O
.	O

I	O
want	O
to	O
set	O
a	O
colour	O
scheme	O
for	O
my	O
python	O
plots	O
,	O
so	O
that	O
they	O
don't	O
repeat	O
the	O
same	O
colour	O
like	O
they	O
are	O
for	O
A	O
and	O
H	O
in	O
the	O
top	O
plot	O
shown	O
below	O
.	O

I've	O
got	O
a	O
2D	O
numpy	O
array	O
with	O
1.0e6	O
as	O
the	O
no	O
data	O
value	O
.	O

Is	O
this	O
what	O
you	O
mean	O
?	O

This	O
is	O
somewhat	O
based	O
on	O
guesswork	O
,	O
since	O
I	O
can't	O
test	O
it	O
with	O
`	O
matplotlib	O
`	O
,	O
but	O
something	O
along	O
these	O
lines	O
might	O
work	O
:	O
#CODE	O

Here	O
is	O
some	O
sample	O
code	O
resembling	O
my	O
case	O
:	O
#CODE	O

This	O
is	O
the	O
result	O
for	O
printing	O
every	O
item	O
in	O
the	O
xyz	O
list	O
and	O
piping	O
it	O
into	O
a	O
file	O
.	O

Or	O
is	O
your	O
plot	O
supposed	O
to	O
be	O
an	O
average	O
or	O
something	O
?	O

I	O
just	O
wasn't	O
sure	O
if	O
changing	O
some	O
plotting	O
functions	O
around	O
would	O
help	O
.	O

I	O
agree	O
with	O
you	O
,	O
but	O
I	O
just	O
tried	O
to	O
make	O
code	O
which	O
uses	O
as	O
few	O
less	O
well-known	O
features	O
of	O
`	O
matplotlib	O
`	O
.	O

You	O
could	O
add	O
a	O
URL	O
to	O
the	O
image	O
,	O
so	O
someone	O
with	O
enough	O
reputation	O
can	O
upload	O
it	O
to	O
your	O
post	O
.	O

'	O
plot	O
'	O
shows	O
me	O
a	O
line	O
interconnecting	O
all	O
data	O
points	O
,	O
but	O
not	O
the	O
data	O
points	O
themselves	O
(	O
unless	O
there's	O
a	O
way	O
I	O
don't	O
know	O
of	O
)	O
.	O

Did	O
you	O
add	O
matplotlib	O
to	O
your	O
PYTHONPATH	O
?	O

I	O
have	O
successfully	O
edited	O
and	O
put	O
it	O
in	O
my	O
code	O
and	O
works	O
fine	O
,	O
but	O
my	O
problem	O
is	O
that	O
I	O
can't	O
show	O
multiple	O
lines	O
of	O
data	O
in	O
the	O
plot	O
,	O
I	O
want	O
to	O
show	O
up	O
to	O
10	O
lines	O
in	O
my	O
graph	O
but	O
I'm	O
getting	O
a	O
lot	O
of	O
errors	O
when	O
I	O
try	O
to	O
add	O
them	O
.	O

That	O
is	O
correct	O
,	O
it	O
draws	O
a	O
line	O
from	O
the	O
lower	O
left	O
to	O
the	O
upper	O
right	O
of	O
the	O
plot	O
as	O
stated	O
in	O
my	O
answer	O
.	O

The	O
problem	O
arises	O
when	O
I	O
select	O
a	O
subset	O
of	O
the	O
data	O
based	O
on	O
the	O
time	O
,	O
ie	O
:	O
#CODE	O

Do	O
you	O
have	O
any	O
suggestion	O
to	O
fix	O
this	O
?	O

I	O
think	O
I'm	O
going	O
to	O
start	O
from	O
scratch	O
so	O
that	O
I	O
know	O
what	O
I'm	O
doing	O
but	O
I	O
need	O
help	O
on	O
where	O
to	O
start	O
.	O

It	O
was	O
a	O
simplified	O
example	O
since	O
I	O
didn't	O
have	O
your	O
code	O
at	O
the	O
time	O
.	O

The	O
question	O
does	O
not	O
define	O
matrix	O
very	O
well	O
:	O
"	O
matrix	O
of	O
values	O
"	O
,	O
"	O
matrix	O
of	O
data	O
"	O
.	O

The	O
algorithm	O
returns	O
an	O
array	O
of	O
ranges	O
,	O
then	O
I	O
create	O
several	O
histograms	O
for	O
the	O
same	O
dataset	O
using	O
different	O
ranges	O
.	O

(	O
Since	O
all	O
you're	O
saying	O
now	O
is	O
"	O
I	O
have	O
this	O
equation	O
and	O
it	O
doesn't	O
work	O
"	O
,	O
we	O
don't	O
have	O
enough	O
info	O
to	O
answer	O
this	O
and	O
it's	O
not	O
a	O
question	O
that	O
can	O
be	O
answered	O
.	O
)	O

This	O
method	O
generalizes	O
to	O
any	O
number	O
of	O
rows	O
(	O
or	O
dimensions	O
)	O
.	O

Another	O
possibility	O
would	O
be	O
to	O
put	O
a	O
tuple	O
into	O
flist	O
:	O
#CODE	O

I	O
downloaded	O
the	O
100meg	O
datafiles	O
from	O
the	O
full	O
Basemap	O
distro	O
.	O

Do	O
you	O
have	O
any	O
other	O
ideas	O
to	O
handle	O
that	O
?	O

This	O
controls	O
whether	O
the	O
data	O
limits	O
or	O
the	O
bounding	O
rectangle's	O
shape	O
are	O
changed	O
when	O
the	O
aspect	O
/	O
limits	O
are	O
changed	O
.	O

Could	O
you	O
be	O
able	O
to	O
give	O
me	O
some	O
hints	O
how	O
to	O
solve	O
this	O
?	O

like	O
you	O
could	O
in	O
MATLAB	O
,	O
as	O
well	O
as	O
having	O
all	O
the	O
features	O
of	O

I	O
tried	O
setting	O
all	O
x-values	O
to	O
'	O
1	O
'	O
,	O
but	O
it	O
turns	O
out	O
as	O
kind	O
of	O
a	O
2d-diagram	O
,	O
anyway	O
:	O

This	O
is	O
the	O
hacky	O
way	O
to	O
fix	O
your	O
aspect	O
ratio	O
on	O
the	O
image	O
.	O

Is	O
Anaconda's	O
main	O
advantage	O
then	O
that	O
one	O
has	O
essentially	O
all	O
one	O
would	O
need	O
,	O
without	O
needing	O
continuously	O
to	O
pip	O
this	O
and	O
pip	O
that	O
?	O

Just	O
for	O
the	O
record	O
,	O
there's	O
no	O
need	O
to	O
jump	O
through	O
all	O
of	O
those	O
hoops	O
with	O
PIL	O
.	O

I'll	O
play	O
with	O
Dermen's	O
link	O
as	O
well	O
,	O
but	O
at	O
the	O
moment	O
the	O
most	O
pressing	O
problem	O
is	O
not	O
having	O
the	O
data	O
points	O
be	O
all	O
squished	O
together	O
because	O
the	O
spacing	O
between	O
the	O
ticks	O
is	O
very	O
very	O
small	O
.	O

This	O
is	O
perfect	O
,	O
though	O
I'm	O
not	O
sure	O
exactly	O
where	O
I	O
went	O
wrong	O
...	O

you	O
can	O
manually	O
set	O
the	O
size	O
of	O
the	O
colorbar	O
using	O
:	O
#CODE	O

Plotting	O
values	O
From	O
a	O
Array	O
in	O
Real	O
time	O
using	O
matplotlib	O
in	O
Python	O

I	O
will	O
also	O
gladly	O
accept	O
any	O
examples	O
where	O
someone	O
has	O
built	O
up	O
any	O
other	O
distribution	O
.	O

How	O
sure	O
are	O
you	O
that	O
the	O
code	O
*	O
runs	O
at	O
all*	O
?	O

I	O
was	O
temporarily	O
disappeared	O
at	O
my	O
day	O
job	O
!	O

In	O
addition	O
,	O
you'll	O
find	O
that	O
`	O
pylab	O
`	O
leaves	O
a	O
generous	O
,	O
often	O
undesirable	O
,	O
whitespace	O
around	O
the	O
image	O
.	O

Force	O
plot	O
to	O
have	O
a	O
special	O
size	O
or	O
to	O
increase	O
the	O
distance	O
between	O
subplots	O

If	O
you	O
do	O
all	O
the	O
data	O
processing	O
beforehand	O
,	O
and	O
provide	O
the	O
(	O
nearly	O
)	O
finished	O
product	O
.	O

To	O
make	O
it	O
clearer	O
I	O
want	O
to	O
get	O
the	O
x-axis	O
and	O
the	O
y-axis	O
range	O
so	O
as	O
to	O
find	O
the	O
mid	O
point	O
of	O
the	O
grid	O
.	O

and	O
you	O
might	O
want	O
to	O
add	O
the	O
ax	O
to	O
the	O
colorbar	O
.	O

The	O
question	O
is	O
:	O
Why	O
does	O
the	O
`	O
matplotlibrc	O
`	O
file	O
have	O
an	O
invalid	O
backend	O
(	O
`	O
pyside	O
`)	O
value	O
in	O
the	O
first	O
place	O
?	O

Is	O
that	O
a	O
bug	O
(	O
could	O
not	O
find	O
any	O
reports	O
to	O
this	O
)	O
,	O
a	O
feature	O
(	O
?!	O
)	O
or	O

I'm	O
not	O
sure	O
how	O
you	O
want	O
to	O
convert	O
your	O
data	O
,	O
but	O
if	O
you	O
have	O
irregular	O
line	O
lengths	O
,	O
the	O
easiest	O
way	O
would	O
be	O
something	O
like	O
this	O
:	O
#CODE	O

No	O
worries	O
,	O
I	O
was	O
coming	O
at	O
this	O
from	O
a	O
radio	O
perspective	O
not	O
a	O
gamma-ray	O
one	O
.	O

How	O
can	O
I	O
fix	O
this	O
?	O

Is	O
not	O
there	O
any	O
wx	O
example	O
in	O
your	O
iPython	O
distribution	O
?	O

Hmm	O
,	O
sorry	O
,	O
maybe	O
I	O
wasn't	O
clear	O
-	O
colours	O
[	O
li	O
,	O
:]	O
only	O
substitutes	O
the	O
tuple	O
with	O
index	O
"	O
li	O
"	O
,	O
but	O
I	O
need	O
to	O
substitute	O
all	O
tuples	O
with	O
indices	O
in	O
this	O
list	O
called	O
"	O
indices	O
"	O
(	O
and	O
li=length	O
(	O
indices	O
))	O

I've	O
also	O
tried	O
installing	O
1.3.0	O
through	O
pip	O
as	O
well	O
as	O
source	O
.	O

Any	O
help	O
would	O
be	O
much	O
appreciated	O
.	O

Maybe	O
you	O
need	O
to	O
rescale	O
with	O
1	O
/	O
30000	O
to	O
be	O
at	O
the	O
right	O
place	O
on	O
your	O
graph	O
.	O

Changing	O
this	O
would	O
require	O
low-level	O
tweaking	O
.	O

I've	O
also	O
refined	O
your	O
approach	O
to	O
allow	O
zooming	O
in	O
over	O
a	O
section	O
of	O
the	O
data	O
and	O
to	O
produce	O
better	O
results	O
at	O
the	O
borders	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

Can	O
someone	O
help	O
me	O
sort	O
out	O
the	O
issue	O
resulting	O
to	O
such	O
an	O
error	O
.	O

Is	O
there	O
any	O
way	O
,	O
I	O
can	O
save	O
a	O
magnified	O
version	O
for	O
proper	O
visualisation	O
later	O
?	O

So	O
let's	O
say	O
I	O
only	O
need	O
`	O
4402	O
x	O
4410	O
`	O
but	O
I	O
don't	O
know	O
the	O
index	O
.	O

I	O
had	O
to	O
upgrade	O
to	O
most	O
recent	O
MPL	O
to	O
find	O
the	O
subplots	O
method	O
of	O
pylab	O
.	O

averaging	O
elements	O
in	O
a	O
matrix	O
with	O
the	O
corresponding	O
elements	O
in	O
another	O
matrix	O
(	O
in	O
python	O
)	O

We	O
consider	O
now	O
that	O
`	O
--	O
pylab	O
`	O
was	O
a	O
mistake	O
,	O
but	O
that	O
it	O
was	O
still	O
really	O
usefull	O
at	O
the	O
beginning	O
of	O
IPython	O
.	O

Is	O
there	O
a	O
way	O
to	O
increase	O
the	O
thickness	O
and	O
size	O
of	O
ticks	O
in	O
matplotlib	O
without	O
having	O
to	O
write	O
a	O
long	O
piece	O
of	O
code	O
like	O
this	O
:	O
#CODE	O

Or	O
you	O
could	O
define	O
column	O
'	O
date_obj	O
'	O
as	O
the	O
index	O
of	O
your	O
data	O
:	O
#CODE	O

Am	O
I	O
missing	O
something	O
,	O
or	O
have	O
you	O
left	O
out	O
all	O
the	O
plot	O
commands	O
?	O

This	O
is	O
my	O
first	O
attempt	O
at	O
python	O
,	O
however	O
,	O
and	O
am	O
still	O
finding	O
out	O
how	O
it	O
works	O
,	O
so	O
please	O
put	O
me	O
right	O
if	O
this	O
idea	O
is	O
fundamentally	O
flawed	O
!	O

Finally	O
,	O
here's	O
a	O
sample	O
portion	O
of	O
an	O
old	O
(	O
bad	O
)	O
data	O
file	O
:	O
#CODE	O

If	O
you	O
you	O
have	O
any	O
questions	O
,	O
please	O
leave	O
a	O
comment	O
.	O

Where	O
exactly	O
is	O
the	O
problem	O
?	O

The	O
last	O
dutch	O
sentence	O
does	O
mean	O
something	O
like	O
:	O
The	O
system	O
can't	O
find	O
the	O
specified	O
file	O
.	O

This	O
is	O
nice	O
of	O
them	O
,	O
as	O
not	O
all	O
Windows	O
users	O
have	O
C++	O
compilers	O
available	O
.	O

The	O
root	O
of	O
the	O
problem	O
is	O
that	O
matplotlib	O
tries	O
to	O
put	O
all	O
bins	O
on	O
the	O
plot	O
.	O

This	O
question	O
has	O
probably	O
a	O
totally	O
simple	O
solution	O
but	O
I	O
just	O
can't	O
find	O
it	O
.	O

Any	O
suggestions	O
would	O
be	O
awesome	O
.	O

In	O
non-interactive	O
mode	O
,	O
display	O
all	O
figures	O
and	O
block	O
until	O
the	O

This	O
is	O
all	O
very	O
new	O
to	O
me	O
.	O

1	O
)	O
Let's	O
assume	O
I	O
take	O
1	O
sample	O
`	O
x	O
=	O
[	O
1	O
,	O
2	O
]`	O
from	O
the	O
dataset	O

I	O
have	O
a	O
4d	O
or	O
higher	O
ellipsoid	O
in	O
matrix	O
form	O
(	O
which	O
form	O
is	O
important	O
)	O
.	O

I	O
didn't	O
look	O
into	O
the	O
documentation	O
,	O
simply	O
used	O
the	O
dot	O
completion	O
on	O
the	O
handle	O

Here	O
is	O
your	O
example	O
with	O
those	O
modifications	O
and	O
also	O
with	O
a	O
single	O
layout	O
computed	O
at	O
the	O
beginning	O
that	O
is	O
reused	O
in	O
the	O
loop	O
.	O

I	O
now	O
understand	O
that	O
I	O
need	O
to	O
generate	O
all	O
the	O
triplets	O
across	O
the	O
three	O
tables	O
(	O
to	O
cover	O
all	O
the	O
combinations	O
)	O
.	O

i	O
dont	O
know	O
how	O
array	O
concatenation	O
works	O
on	O
numpy	O
arrays	O

I	O
can't	O
seem	O
to	O
get	O
my	O
head	O
around	O
why	O
it	O
doesn't	O
show	O
the	O
value	O
128	O
as	O
grey	O
with	O
I	O
have	O
chosen	O
the	O
cmap	O
to	O
be	O
gray-scale	O
.	O

what	O
do	O
you	O
mean	O
with	O
%s	O
?	O

If	O
that	O
isn't	O
what	O
you	O
want	O
,	O
I	O
would	O
recommend	O
doing	O
something	O
where	O
you	O
create	O
a	O
PNG	O
image	O
of	O
what	O
you	O
do	O
want	O
,	O
create	O
a	O
child	O
QLabel	O
,	O
and	O
move	O
the	O
child	O
widget	O
around	O
in	O
your	O
mouseMoveEvent	O
.	O

Any	O
suggestion	O
is	O
welcome	O
.	O

I	O
was	O
trying	O
to	O
add	O
a	O
custom	O
widget	O
into	O
qtdesginer	O
using	O
following	O
code	O
#CODE	O

source	O
:	O
#URL	O

As	O
has	O
been	O
pointed	O
out	O
above	O
,	O
this	O
will	O
get	O
more	O
complicated	O
if	O
the	O
both	O
the	O
y	O
graphs	O
were	O
defined	O
at	O
different	O
x	O
points	O
.	O

@USER	O
Simoes	O
,	O
what	O
happens	O
is	O
you	O
have	O
Nan's	O
in	O
various	O
places	O
in	O
your	O
array	O
?	O

An	O
example	O
script	O
with	O
generated	O
data	O
rather	O
than	O
the	O
real	O
data	O
is	O
:	O
#CODE	O

Also	O
,	O
it's	O
harder	O
to	O
reproduce	O
across	O
multiple	O
machines	O
(	O
even	O
if	O
that	O
just	O
means	O
"	O
my	O
current	O
machine	O
,	O
and	O
the	O
new	O
laptop	O
I'll	O
have	O
18	O
months	O
from	O
now	O
when	O
I've	O
forgotten	O
how	O
to	O
solve	O
it	O
")	O
.	O

The	O
words	O
'	O
non-default	O
'	O
and	O
'	O
normal	O
'	O
are	O
mine	O
-	O
I'm	O
not	O
sure	O
what	O
they	O
mean	O
yet	O
.	O

(	O
think	O
of	O
it	O
as	O
a	O
unit	O
square	O
)	O

This	O
code	O
uses	O
a	O
Counter	O
to	O
count	O
the	O
number	O
of	O
instances	O
that	O
an	O
object	O
occurs	O
in	O
an	O
array	O
(	O
in	O
this	O
case	O
counting	O
the	O
number	O
of	O
times	O
an	O
integer	O
is	O
in	O
your	O
list	O
)	O
.	O

In	O
the	O
image	O
above	O
I'm	O
trying	O
to	O
extract	O
iso-value	O
25	O
from	O
the	O
scalar	O
field	O
of	O
f	O
(	O
x	O
,	O
y	O
)=	O
x^3+y^3	O
.	O

Is	O
there	O
any	O
other	O
way	O
I	O
can	O
plot	O
my	O
result	O
continuously	O
?	O

at	O
lag	O
0	O
,	O
the	O
ACF	O
is	O
0.5	O
.	O

The	O
example	O
talks	O
about	O
a	O
fit	O
to	O
power	O
law	O
,	O
but	O
clearly	O
a	O
straight	O
line	O
could	O
be	O
done	O
as	O
well	O
.	O

As	O
I	O
see	O
it	O
,	O
I	O
could	O
resolve	O
and	O
make	O
it	O
work	O
if	O
I	O
could	O
adjust	O
the	O
line	O
spacing	O
/	O
height	O
to	O
be	O
less	O
than	O
what	O
the	O
font	O
requests	O
.	O

tells	O
the	O
writer	O
to	O
temporarily	O
save	O
the	O
frames	O
to	O
disc	O
before	O
composing	O
the	O
movie	O
,	O
which	O
side-steps	O
the	O
problem	O
.	O

Select	O
starting	O
color	O
in	O
matplotlib	O
colormap	O

I	O
know	O
this	O
question	O
is	O
a	O
couple	O
years	O
old	O
,	O
but	O
since	O
there	O
is	O
no	O
accepted	O
answer	O
,	O
I'll	O
add	O
what	O
works	O
for	O
me	O
.	O

include	O
the	O
piece	O
of	O
the	O
code	O
where	O
you	O
make	O
call	O
to	O
`	O
mpl_connect	O
`	O

Is	O
there	O
any	O
ready	O
to	O
use	O
function	O
based	O
on	O
python's	O
matplolib	O
?	O

-2	O
is	O
high	O
of	O
indices	O
range	O
and	O
then	O
low	O
of	O
indices	O
range	O

Any	O
insight	O
you	O
could	O
provide	O
that	O
would	O
help	O
get	O
me	O
started	O
the	O
proper	O
way	O
would	O
be	O
fantastic	O
.	O

(	O
I	O
can	O
open	O
a	O
new	O
question	O
if	O
that's	O
perferable	O
)	O

For	O
example	O
,	O
here's	O
a	O
simple	O
bit	O
of	O
code	O
that	O
sets	O
up	O
a	O
list	O
of	O
colors	O
based	O
on	O
whether	O
the	O
data	O
is	O
positive	O
or	O
negative	O
:	O
#CODE	O

With	O
that	O
example	O
data	O
I	O
was	O
getting	O
"	O
shape	O
mismatch	O
"	O
errors	O
.	O

Then	O
,	O
I'd	O
use	O
POV-Ray	O
or	O
Blender	O
to	O
model	O
the	O
planes	O
at	O
whatever	O
angles	O
,	O
spheres	O
for	O
the	O
round	O
things	O
(	O
planets	O
?	O
)	O
.	O

So	O
,	O
thus	O
far	O
,	O
all	O
I	O
produce	O
is	O
:	O

I'm	O
hoping	O
I	O
can	O
find	O
a	O
funciton	O
(	O
or	O
build	O
one	O
)	O
that	O
does	O
new_ids_arr	O
=	O
new_ids	O
*	O
new_ids^Transpose	O
...	O
or	O
similar	O

Any	O
help	O
much	O
appreciated	O
.	O

But	O
there	O
are	O
still	O
white	O
edges	O
around	O
(	O
I	O
have	O
added	O
a	O
screenshot	O
to	O
my	O
original	O
question	O
)	O

How	O
could	O
I	O
use	O
`	O
matplotlib	O
`	O
to	O
represent	O
such	O
a	O
matrix	O
as	O
a	O
grid	O
of	O
red	O
and	O
black	O
squares	O
?	O

The	O
thick	O
grey	O
line	O
in	O
the	O
middle	O
of	O
the	O
violin	O
is	O
the	O
bootstrapped	O
99%	O
confidence	O
interval	O
of	O
the	O
mean	O
,	O
which	O
is	O
the	O
white	O
horizontal	O
line	O
,	O
both	O
from	O
pointplot	O
.	O

@USER	O
I	O
up	O
voted	O
you	O
but	O
add	O
`	O
import	O
numpy	O
as	O
np	O
`	O

So	O
,	O
when	O
you	O
reduce	O
to	O
2	O
dimensions	O
,	O
the	O
power	O
is	O
the	O
sum	O
of	O
the	O
first	O
2	O
eigen	O
values	O
.	O

Indexing	O
starts	O
with	O
`	O
0	O
`	O
in	O
Python	O
(	O
and	O
most	O
any	O
other	O
programming	O
language	O
I	O
know	O
)	O
.	O

I	O
have	O
a	O
matrix	O
file	O
of	O
Hurricane	O
data	O
and	O
it	O
plots	O
with	O
the	O
command	O
:	O
#CODE	O

If	O
they	O
enter	O
30	O
,	O
they	O
will	O
look	O
at	O
a	O
30-minute	O
time	O
window	O
,	O
if	O
they	O
type	O
5	O
,	O
matplotlib	O
picks	O
this	O
up	O
,	O
data	O
gets	O
trimmed	O
,	O
and	O
only	O
5	O
minutes	O
worth	O
of	O
data	O
gets	O
shown	O
.	O

In	O
the	O
meanwhile	O
does	O
any	O
one	O
has	O
a	O
better	O
suggestion	O
?	O

I	O
want	O
it	O
at	O
y=	O
10.5	O
,	O
specifically	O
.	O

I	O
then	O
plot	O
each	O
of	O
these	O
values	O
,	O
but	O
to	O
get	O
a	O
more	O
sensible	O
result	O
,	O
I	O
need	O
to	O
take	O
the	O
mean	O
of	O
all	O
of	O
these	O
files	O
for	O
each	O
cell	O
,	O
and	O
plot	O
that	O
with	O
the	O
standard	O
deviation	O
obtained	O
as	O
error	O
bars	O
.	O

I'm	O
using	O
data	O
of	O
the	O
form	O
:	O
`	O
[	O
num1	O
,	O
num2	O
,...,	O
numk	O
]`	O
(	O
an	O
array	O
of	O
integers	O
)	O
.	O

For	O
the	O
time	O
steps	O
where	O
convergence	O
is	O
reached	O
Id	O
like	O
to	O
have	O
the	O
x-axis	O
green	O
and	O
for	O
non-convergence	O
starting	O
from	O
light	O
red	O
for	O
not	O
so	O
bad	O
to	O
red	O
for	O
bad	O
.	O

For	O
instance	O
you	O
can	O
add	O
#CODE	O

I	O
don't	O
want	O
to	O
get	O
a	O
newer	O
one	O
just	O
incase	O
any	O
of	O
the	O
practice	O
problems	O
/	O
lecture	O
problems	O
are	O
incompatible	O
with	O
a	O
newer	O
version	O
of	O
IDLE	O
.	O

All	O
of	O
these	O
frames	O
are	O
loaded	O
at	O
the	O
start	O
of	O
the	O
application	O
.	O

if	O
you	O
suspect	O
one	O
specific	O
builtin	O
,	O
try	O
checking	O
it's	O
type	O
(	O
`	O
type	O
(	O
dict	O
)`)	O
,	O
or	O
look	O
at	O
the	O
properties	O
/	O
functions	O
it	O
has	O
(	O
`	O
dir	O
(	O
dict	O
)`)	O
.	O

In	O
some	O
work	O
I	O
have	O
done	O
we	O
used	O
a	O
simple	O
approximation	O
of	O
the	O
derivative	O
,	O
when	O
this	O
changes	O
sign	O
you	O
have	O
a	O
peak	O
(	O
in	O
1D	O
data	O
)	O
,	O
one	O
can	O
then	O
add	O
some	O
parameters	O
to	O
remove	O
peaks	O
due	O
to	O
noise	O
.	O

Here's	O
a	O
working	O
code	O
sample	O
that	O
shows	O
you	O
how	O
to	O
get	O
it	O
working	O
.	O

How	O
can	O
I	O
change	O
this	O
behavior	O
and	O
use	O
Arial	O
fonts	O
for	O
all	O
of	O
my	O
plots	O
?	O

How	O
to	O
divide	O
ytics	O
to	O
a	O
certain	O
number	O
in	O
matplotlib	O
?	O

Matplotlib	O
cannot	O
find	O
facefile	O
,	O
is	O
using	O
old	O
Python	O
interpreter	O
location	O

do	O
you	O
get	O
any	O
message	O
when	O
trying	O
plotting	O
it	O
?	O

I	O
have	O
mac	O
os	O
x	O
10.5.8	O
and	O
was	O
wondering	O
if	O
anyone	O
could	O
recommend	O
where	O
I	O
could	O
find	O
the	O
proper	O
package	O
.	O

Well	O
,	O
this	O
isn't	O
a	O
wxPython	O
answer	O
but	O
I've	O
used	O
Chaco	O
for	O
this	O
sort	O
of	O
thing	O
and	O
it's	O
pretty	O
straight	O
forward	O
.	O

The	O
point	O
IS	O
at	O
its	O
position	O
namely	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
.	O

All	O
is	O
fine	O
again	O
.	O

The	O
next	O
plot	O
to	O
show	O
has	O
the	O
same	O
clusters	O
and	O
colors	O
but	O
a	O
different	O
location	O
.	O

@USER	O
The	O
first	O
approach	O
would	O
help	O
only	O
if	O
the	O
internal	O
representation	O
was	O
the	O
problem	O
,	O
but	O
it	O
doesn't	O
solve	O
the	O
underlying	O
size	O
issue	O
.	O

After	O
hours	O
analyzing	O
the	O
source	O
code	O
of	O
basemap's	O
`	O
tissot	O
`	O
function	O
,	O
learning	O
some	O
properties	O
of	O
ellipses	O
and	O
lot's	O
of	O
debugging	O
,	O
I	O
came	O
with	O
a	O
solution	O
to	O
my	O
problem	O
.	O

You'd	O
need	O
to	O
refactor	O
the	O
code	O
to	O
break	O
your	O
worker	O
method	O
up	O
into	O
smaller	O
pieces	O
that	O
multiple	O
processes	O
can	O
work	O
on	O
at	O
the	O
same	O
time	O
,	O
and	O
then	O
pull	O
it	O
back	O
together	O
once	O
all	O
the	O
pieces	O
are	O
ready	O
.	O

are	O
you	O
just	O
trying	O
to	O
open	O
a	O
separate	O
window	O
or	O
display	O
that	O
window	O
inside	O
of	O
your	O
main	O
window	O
?	O

The	O
one	O
catch	O
:	O
if	O
you	O
embed	O
a	O
PDF	O
inside	O
a	O
textbox	O
,	O
Word	O
will	O
rasterize	O
this	O
when	O
you	O
save	O
it	O
to	O
a	O
PDF	O
.	O

How	O
can	O
I	O
address	O
a	O
specific	O
line	O
(	O
by	O
name	O
,	O
by	O
number	O
,	O
by	O
reference	O
)	O
throughout	O
the	O
program	O
and	O
delete	O
that	O
line	O
?	O

And	O
also	O
I	O
need	O
to	O
do	O
that	O
for	O
all	O
variables	O
simultaneously	O
.	O

`	O
newtitle	O
=	O
"	O
%s\n$%s$	O
"	O
%	O
(	O
'	O
text\	O
below\	O
the\	O
main\	O
title	O
'	O
,	O
newtitle	O
)`	O

Have	O
you	O
had	O
a	O
look	O
at	O
the	O
`	O
webagg	O
`	O
matplotlib	O
backend	O
?	O

find	O
the	O
file	O
from	O
the	O
link	O

If	O
you	O
want	O
separate	O
figures	O
,	O
as	O
the	O
title	O
of	O
your	O
question	O
suggests	O
the	O
`	O
subplots=True	O
`	O
argument	O
might	O
get	O
the	O
job	O
done	O
.	O

I	O
suggest	O
removing	O
all	O
the	O
domain	O
specific	O
stuff	O
.	O

Just	O
change	O
`	O
shape	O
`	O
to	O
`	O
(	O
10	O
,	O
10	O
,	O
10	O
,	O
10	O
,	O
10	O
)`	O
and	O
see	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

However	O
,	O
(	O
at	O
least	O
what	O
I	O
have	O
seen	O
is	O
that	O
)	O
sometimes	O
after	O
the	O
plot	O
has	O
been	O
displayed	O
,	O
it	O
goes	O
into	O
a	O
"	O
Not	O
Responding	O
"	O
mode	O
and	O
you	O
just	O
have	O
to	O
kill	O
the	O
process	O
anyways	O
.	O

It	O
consists	O
in	O
a	O
floating-point	O
parameter	O
P	O
,	O
with	O
dimension	O
NxM	O
,	O
and	O
each	O
pixel	O
is	O
geolocated	O
by	O
the	O
fields	O
latitude	O
and	O
longitude	O
(	O
each	O
of	O
size	O
NxM	O
)	O
.	O

In	O
order	O
to	O
change	O
the	O
order	O
of	O
the	O
bars	O
,	O
you	O
should	O
change	O
the	O
order	O
in	O
the	O
index	O
.	O

This	O
does	O
not	O
work	O
when	O
I	O
replace	O
the	O
random	O
plot	O
data	O
with	O
an	O
actual	O
color	O
PNG	O
image	O
.	O

For	O
example	O
,	O
say	O
you	O
had	O
data	O
between	O
0	O
and	O
1	O
but	O
didn't	O
like	O
the	O
colors	O
used	O
at	O
the	O
extremes	O
of	O
the	O
colormap	O
for	O
0	O
and	O
1	O
.	O

After	O
searching	O
through	O
multiple	O
posts	O
,	O
I	O
basically	O
want	O
to	O
use	O
a	O
generic	O
colormap	O
(	O
rainbow	O
)	O
and	O
multiply	O
my	O
third	O
array	O
by	O
the	O
colormap	O
in	O
order	O
to	O
display	O
different	O
colors	O
for	O
each	O
of	O
the	O
xy	O
points	O
.	O

Hi	O
,	O
Thanks	O
Tobold	O
,	O
I	O
will	O
check	O
the	O
source	O
code	O
you	O
mention	O
.	O

This	O
next	O
block	O
is	O
what	O
does	O
what	O
you	O
want	O
with	O
the	O
`	O
for	O
`	O
loop	O
.	O

Say	O
that	O
I	O
have	O
6	O
variables	O
that	O
I	O
want	O
to	O
sort	O
into	O
these	O
3	O
groups	O
and	O
plot	O
like	O
a	O
venn	O
diagram	O
.	O

ValueError	O
:	O
setting	O
an	O
array	O
element	O
with	O
a	O
sequence	O
.	O

My	O
need	O
is	O
that	O
I	O
want	O
to	O
plot	O
these	O
nominal	O
values	O
of	O
both	O
actual	O
and	O
predicted	O
ones	O
on	O
the	O
same	O
plot	O
so	O
that	O
It	O
will	O
be	O
easy	O
to	O
compare	O
how	O
good	O
was	O
the	O
prediction	O
as	O
compared	O
to	O
actual	O
values	O
.	O

So	O
,	O
open	O
up	O
your	O
favorite	O
Terminal	O
emulator	O
and	O
enter	O
#CODE	O

Also	O
,	O
the	O
lists	O
need	O
to	O
be	O
equal	O
length	O
.	O

Because	O
matplotlib	O
will	O
convert	O
things	O
to	O
numpy	O
arrays	O
regardless	O
,	O
there	O
are	O
more	O
efficient	O
ways	O
to	O
do	O
it	O
.	O

And	O
here	O
is	O
where	O
I	O
grab	O
the	O
data	O
and	O
I	O
try	O
to	O
update	O
the	O
limits	O
of	O
the	O
plot	O
#CODE	O

Any	O
help	O
is	O
appreciated	O
.	O

Which	O
is	O
the	O
why	O
,	O
conveniently	O
,	O
`	O
colors	O
`	O
keyword	O
exists	O
.	O

How	O
can	O
I	O
find	O
the	O
code	O
to	O
support	O
multiple	O
plotting	O
scales	O
and	O
X-axis	O
on	O
seperate	O
panels	O
using	O
sizer	O
routines	O
?	O

I'm	O
trying	O
to	O
build	O
Matplotlib	O
from	O
source	O
with	O
Tkinter	O
.	O

This	O
means	O
that	O
you	O
can	O
install	O
matplotlib	O
locally	O
in	O
your	O
`	O
virtualenv	O
`	O
and	O
it	O
will	O
find	O
all	O
of	O
its	O
backend	O
dependencies	O
in	O
the	O
system-wide	O
`	O
site-packages	O
`	O
.	O

I	O
am	O
trying	O
to	O
create	O
a	O
simple	O
GUI	O
using	O
tkinter	O
which	O
will	O
just	O
open	O
a	O
window	O
displaying	O
the	O
plot	O
.	O

Web	O
convinces	O
me	O
using	O
this	O
is	O
no	O
good	O
idea	O
after	O
all	O
:	O
For	O
example	O
#URL	O
"	O
Avoid	O
3-D	O
charts	O
at	O
all	O
costs	O
.	O

Using	O
pure	O
Python	O
,	O
you	O
can	O
extract	O
columns	O
`	O
1	O
`	O
,	O
`	O
5	O
`	O
and	O
`	O
7	O
`	O
by	O
using	O
the	O
following	O
nested	O
list	O
comprehension	O
:	O
#CODE	O

Is	O
there	O
any	O
way	O
of	O
getting	O
some	O
input	O
from	O
a	O
user	O
in	O
matplotlib	O
?	O

`	O
theta	O
`	O
is	O
in	O
radians	O
instead	O
of	O
degrees	O
.	O

'	O
plot	O
'	O
shows	O
me	O
a	O
line	O
interconnecting	O
all	O
data	O
points	O
,	O
but	O
not	O
the	O
data	O
points	O
themselves	O
(	O
unless	O
there's	O
a	O
way	O
I	O
don't	O
know	O
of	O
)	O
.	O

I	O
could	O
divide	O
the	O
`	O
peak_top	O
`	O
by	O
2	O
to	O
find	O
the	O
half	O
height	O
and	O
then	O
try	O
and	O
find	O
y-values	O
corresponding	O
to	O
the	O
half	O
height	O
,	O
but	O
then	O
I	O
would	O
run	O
into	O
trouble	O
if	O
there	O
are	O
no	O
x-values	O
exactly	O
matching	O
the	O
half	O
height	O
.	O

why	O
can't	O
it	O
find	O
the	O
file	O
?	O

However	O
,	O
the	O
resulting	O
image	O
always	O
has	O
the	O
same	O
resolution	O
,	O
around	O
(	O
250x250	O
)	O
.	O

Installing	O
those	O
packages	O
from	O
the	O
source	O
can	O
be	O
a	O
pain	O
,	O
especially	O
on	O
Windows	O
and	O
OS	O
X	O
.	O

Sort	O
arrays	O
by	O
two	O
criteria	O

However	O
,	O
I	O
am	O
kinda	O
confused	O
,	O
any	O
help	O
would	O
be	O
appreciated	O
.	O

Unfortunately	O
if	O
you	O
search	O
this	O
array	O
for	O
all	O
subnodes	O
given	O
some	O
index	O
you	O
only	O
get	O
some	O
of	O
the	O
subnodes	O
contained	O
by	O
the	O
original	O
triangle	O
because	O
most	O
nodes	O
belong	O
to	O
several	O
triangles	O
and	O
they	O
are	O
only	O
added	O
to	O
one	O
of	O
these	O
.	O

If	O
I	O
set	O
shrink	O
1.0	O
and	O
fraction	O
to	O
anything	O
,	O
it	O
shrinks	O
the	O
graph	O
,	O
not	O
affecting	O
the	O
colorbar	O
size	O
at	O
all	O
,	O
until	O
changing	O
fraction	O
causes	O
it	O
to	O
be	O
exactly	O
what	O
I	O
already	O
have	O
,	O
at	O
which	O
point	O
changing	O
them	O
stops	O
doing	O
anything	O
.	O

It	O
doesn't	O
add	O
anything	O
else	O
.	O

Now	O
I	O
just	O
wish	O
they'd	O
add	O
some	O
kind	O
of	O
drop	O
down	O
menu	O
to	O
matplotlib	O
...	O

Thanks	O
for	O
any	O
help	O

The	O
source	O
data's	O
list	O
is	O
called	O
xyz	O
.	O

I	O
have	O
been	O
looking	O
around	O
and	O
maybe	O
it	O
could	O
be	O
a	O
version-of-Pyhton	O
problem	O
.	O

(	O
I	O
want	O
to	O
reduce	O
the	O
size	O
of	O
the	O
outlier	O
)	O
#CODE	O

This	O
should	O
work	O
for	O
any	O
number	O
of	O
columns	O
and	O
length	O
of	O
data	O
.	O

Any	O
assistance	O
would	O
be	O
greatly	O
appreciated	O
,	O
this	O
has	O
been	O
doing	O
my	O
head	O
in	O
.	O

where	O
axes_square	O
is	O
simply	O
:	O
#CODE	O

I	O
can	O
put	O
together	O
a	O
more	O
detailed	O
answer	O
based	O
on	O
that	O
package	O
if	O
desired	O
.	O

It's	O
fairly	O
easy	O
to	O
speed	O
it	O
up	O
by	O
making	O
an	O
approximation	O
--	O
just	O
take	O
the	O
2D	O
histogram	O
and	O
blur	O
it	O
with	O
a	O
guassian	O
filter	O
of	O
the	O
right	O
radius	O
and	O
covariance	O
.	O

I've	O
tried	O
the	O
solutions	O
posted	O
by	O
Marek	O
and	O
qarma	O
attempting	O
to	O
obtain	O
the	O
coordinates	O
of	O
the	O
bins	O
rather	O
than	O
the	O
index	O
of	O
them	O
,	O
like	O
so	O
:	O
#CODE	O

What	O
do	O
you	O
mean	O
by	O
make	O
sure	O
the	O
'	O
backend	O
setting	O
for	O
pylab	O
is	O
not	O
set	O
to	O
inline	O
?	O

sets	O
the	O
initial	O
x	O
,	O
v	O
of	O
every	O
particle	O
in	O
an	O
array	O

Post	O
more	O
information	O
on	O
structure	O
of	O
your	O
array	O
.	O

Here	O
is	O
a	O
sample	O
code	O
if	O
anyone	O
wants	O
to	O
play	O
with	O
it	O
.	O

Are	O
all	O
these	O
settings	O
really	O
necessary	O
to	O
achieve	O
the	O
result	O
I'm	O
looking	O
for	O
,	O
or	O
is	O
there	O
perhaps	O
a	O
more	O
compact	O
way	O
to	O
accomplish	O
my	O
goal	O
:	O
#CODE	O

I	O
want	O
to	O
select	O
(	O
in	O
the	O
SQL	O
sense	O
)	O
just	O
columns	O
`	O
b0	O
`	O
through	O
`	O
b9	O
`	O
from	O
the	O
array	O
,	O
giving	O
the	O
structure	O
#CODE	O

I	O
can't	O
find	O
any	O
method	O
to	O
tell	O
mpl	O
to	O
plot	O
all	O
of	O
the	O
data	O
,	O
it	O
seems	O
that	O
there	O
are	O
only	O
methods	O
for	O
drawing	O
every	O
n'th	O
element	O
or	O
similar	O
(	O
and	O
passing	O
1	O
does	O
not	O
help	O
)	O
.	O

I	O
haven't	O
checked	O
any	O
of	O
the	O
details	O
and	O
you	O
should	O
therefore	O
not	O
rely	O
on	O
it	O
to	O
be	O
correct	O
.	O

You	O
can	O
access	O
members	O
and	O
slices	O
of	O
the	O
array	O
as	O
you	O
would	O
with	O
normal	O
numpy	O
arrays	O
.	O

Running	O
1000	O
simulations	O
at	O
the	O
same	O
time	O
might	O
be	O
a	O
bit	O
expensive	O
though	O
.	O

I	O
need	O
to	O
look	O
at	O
other	O
formats	O
that	O
allow	O
create	O
.	O

finding	O
element	O
of	O
numpy	O
array	O
that	O
satisfies	O
condition	O

All	O
the	O
results	O
(	O
percentage	O
)	O
are	O
the	O
comparison	O
between	O
the	O
described	O
condition	O
and	O
the	O
reference	O
which	O
here	O
is	O
the	O
packaged	O
ATLAS	O
library	O
.	O

how	O
do	O
you	O
use	O
`	O
slicing	O
`	O
to	O
extract	O
something	O
from	O
`	O
x	O
`	O
?	O

Mapping	O
a	O
numpy	O
array	O
pairwise	O

If	O
that	O
is	O
all	O
you	O
want	O
to	O
do	O
,	O
it	O
should	O
just	O
work	O
otherwise	O
look	O
at	O
#URL	O
since	O
subclassing	O
an	O
array	O
is	O
not	O
that	O
simple	O
.	O

If	O
you	O
want	O
to	O
access	O
an	O
individual	O
element	O
using	O
2D	O
notation	O
,	O
you	O
can	O
create	O
a	O
view	O
and	O
work	O
with	O
that	O
.	O

Regarding	O
3	O
,	O
I	O
don't	O
think	O
this	O
would	O
be	O
necessary	O
since	O
the	O
above	O
trick	O
can	O
also	O
be	O
applied	O
to	O
an	O
array	O
allocated	O
by	O
`	O
shmarray	O
`	O
.	O

And	O
here	O
a	O
list	O
with	O
the	O
names	O
of	O
all	O
distribution	O
functions	O
available	O
in	O
Scipy	O
0.12.0	O
(	O
VI	O
):	O
#CODE	O

I	O
get	O
this	O
error	O
whenever	O
i	O
try	O
to	O
use	O
any	O
functions	O
of	O
matplotlib	O
such	O
as	O
graph	O
etc	O
...	O

Numpy	O
is	O
designed	O
for	O
repeated	O
application	O
of	O
the	O
exact	O
same	O
operation	O
in	O
parallel	O
across	O
an	O
array	O
.	O

Each	O
B	O
i	O
is	O
a	O
`	O
k	O
`	O
-by-	O
`	O
n	O
`	O
matrix	O
.	O

I	O
created	O
a	O
copy	O
of	O
the	O
initial	O
array	O
in	O
the	O
hope	O
that	O
it	O
would	O
sort	O
it	O
out	O
but	O
it	O
still	O
doesn't	O
work	O
!	O

So	O
far	O
,	O
all	O
the	O
solutions	O
I	O
found	O
required	O
converting	O
to	O
IPLImage	O
.	O

However	O
,	O
I	O
haven't	O
tested	O
this	O
very	O
systematically	O
,	O
and	O
it's	O
likely	O
that	O
for	O
smaller	O
matrices	O
the	O
additional	O
overhead	O
would	O
outweigh	O
the	O
performance	O
benefit	O
from	O
a	O
higher	O
thread	O
count	O
.	O

but	O
why	O
dimA+dimB	O
to	O
begin	O
with	O
,	O
that	O
just	O
leaves	O
you	O
0s	O
at	O
the	O
end	O
.	O

If	O
you	O
select	O
a	O
list	O
of	O
actual	O
fitness	O
entries	O
,	O
`	O
indices	O
`	O
,	O
the	O
corresponding	O
points	O
are	O
given	O
by	O
`	O
A	O
[	O
'	O
point	O
']	O
[	O
indices	O
]`	O
,	O
which	O
is	O
a	O
simple	O
`	O
(	O
n	O
,	O
3	O
)`	O
array	O
.	O

It's	O
also	O
possible	O
to	O
generate	O
an	O
array	O
of	O
indices	O
without	O
using	O
`	O
enumerate	O
`	O
.	O

If	O
you	O
want	O
to	O
keep	O
the	O
array	O
allocated	O
,	O
and	O
with	O
the	O
same	O
size	O
,	O
you	O
don't	O
need	O
to	O
clear	O
the	O
elements	O
.	O

It'll	O
print	O
out	O
all	O
methods	O
and	O
properties	O
of	O
the	O
object	O
.	O

Search	O
numpy	O
array	O
inside	O
numpy	O
array	O

Assigning	O
a	O
view	O
to	O
a	O
structured	O
array	O
also	O
copies	O
the	O
data	O
,	O
soyour	O
suggestion	O
doesnt	O
work	O
.	O

If	O
you	O
know	O
there	O
are	O
not	O
many	O
different	O
values	O
(	O
relative	O
to	O
the	O
size	O
of	O
the	O
input	O
"	O
itemArray	O
")	O
,	O
something	O
like	O
this	O
could	O
be	O
efficient	O
:	O
#CODE	O

When	O
I	O
import	O
numpy	O
in	O
a	O
script	O
,	O
I	O
don't	O
have	O
all	O
the	O
functions	O
of	O
numpy	O
available	O
,	O
only	O
few	O
of	O
them	O
(	O
not	O
a	O
lot	O
,	O
and	O
not	O
array	O
)	O
?	O

It	O
allows	O
to	O
have	O
your	O
custom	O
system	O
inside	O
the	O
home	O
directory	O
accessible	O
via	O
proot	O
and	O
,	O
therefore	O
,	O
you	O
can	O
install	O
any	O
packages	O
without	O
root	O
privileges	O
.	O

Hey	O
,	O
is	O
there	O
an	O
optimal	O
size	O
for	O
a	O
block	O
?	O

Numpy	O
array	O
,	O
how	O
to	O
select	O
indices	O
satisfying	O
multiple	O
conditions	O
?	O

I'm	O
still	O
learning	O
git	O
and	O
this	O
whole	O
open	O
source	O
thing	O
.	O

A	O
further	O
problem	O
is	O
that	O
a	O
list	O
index	O
can't	O
contain	O
duplicates	O
-	O
I	O
couldn't	O
simultaneously	O
read	O
pixels	O
`	O
[	O
1	O
,	O
2	O
]`	O
and	O
`	O
[	O
1	O
,	O
3	O
]`	O
,	O
since	O
my	O
list	O
of	O
pixel	O
x-coordinates	O
would	O
contain	O
`	O
[	O
1	O
,	O
1	O
]`	O
.	O

Your	O
interpretation	O
is	O
,	O
of	O
course	O
,	O
quite	O
correct	O
:	O
`	O
count	O
`	O
refers	O
to	O
the	O
number	O
of	O
elements	O
in	O
the	O
`	O
float*	O
`	O
"	O
array	O
"	O
.	O

This	O
all	O
works	O
.	O

Of	O
course	O
it	O
may	O
_not_	O
be	O
acceptable	O
,	O
but	O
it	O
seemed	O
like	O
it	O
was	O
at	O
least	O
trying	O
.	O

I	O
think	O
what	O
should	O
happen	O
is	O
for	O
R	O
to	O
be	O
overwritten	O
with	O
an	O
upper	O
triangular	O
matrix	O
.	O

@USER	O
Is	O
there	O
a	O
way	O
to	O
get	O
it	O
to	O
return	O
all	O
possible	O
solutions	O
?	O

Thank	O
you	O
all	O
for	O
some	O
great	O
insight	O
!	O

as	O
numpy	O
array	O
of	O
0	O
and	O
1	O
values	O
.	O

By	O
construction	O
of	O
the	O
problem	O
,	O
there	O
can	O
not	O
be	O
any	O
non-unique	O
values	O
lying	O
within	O
one	O
another	O
.	O

Both	O
functions	O
are	O
kind	O
of	O
equal	O
,	O
but	O
are	O
different	O
.	O

For	O
a	O
sparse	O
csr	O
matrix	O
(	O
X	O
)	O
and	O
a	O
list	O
of	O
indices	O
to	O
drop	O
(	O
index_to_drop	O
):	O
#CODE	O

This	O
will	O
be	O
relatively	O
slow	O
,	O
and	O
requires	O
you	O
to	O
have	O
twice	O
the	O
free	O
memory	O
space	O
required	O
to	O
store	O
the	O
array	O
.	O

I	O
need	O
numpy	O
for	O
this	O
because	O
I	O
don't	O
want	O
to	O
loop	O
through	O
the	O
array	O
n-times	O
for	O
n	O
groups	O
,	O
since	O
my	O
array	O
sizes	O
can	O
be	O
arbitrarily	O
large	O
.	O

I	O
wonder	O
what	O
the	O
speed	O
delta	O
between	O
native	O
Numpy	O
arrays	O
and	O
an	O
`	O
mpfr	O
`	O
array	O
,	O
seeing	O
as	O
`	O
mpfr	O
`	O
is	O
a	O
relatively	O
low-level	O
C	O
wrapper	O
class	O
.	O

In	O
this	O
particular	O
case	O
,	O
each	O
1D	O
column	O
(	O
`	O
column	O
=	O
myarray	O
[	O
i	O
,	O
j	O
,	O
:]	O
`)	O
of	O
the	O
3D	O
array	O
can	O
be	O
treated	O
independently	O
.	O

Any	O
ideas	O
?	O

Then	O
take	O
element	O
3	O
and	O
1	O
from	O
second	O
row	O
,	O
etc	O
.	O

print	O
len	O
(	O
index	O
)	O

I	O
don't	O
know	O
how	O
to	O
label	O
rows	O
and	O
columns	O
in	O
numpy	O
,	O
so	O
I	O
just	O
made	O
a	O
dict	O
mapping	O
the	O
row	O
label	O
to	O
the	O
row	O
index	O
and	O
another	O
doing	O
the	O
same	O
for	O
the	O
columns	O
.	O

It's	O
the	O
same	O
as	O
a	O
normal	O
solve	O
except	O
I	O
know	O
some	O
of	O
the	O
solution	O
to	O
begin	O
with	O
.	O

yeah	O
,	O
this	O
is	O
the	O
best	O
answer	O
anywhere	O
where	O
edge	O
cases	O
are	O
important	O
.	O

With	O
this	O
,	O
I	O
solve	O
the	O
problem	O
(	O
may	O
help	O
you	O
):	O
#CODE	O

Teach	O
a	O
man	O
to	O
fish	O
and	O
all	O
that	O
.	O

Looks	O
like	O
this	O
would	O
be	O
a	O
good	O
place	O
to	O
use	O
a	O
context	O
manager	O
,	O
so	O
you	O
can	O
say	O
"	O
with	O
fullprint	O
"	O
.	O

What	O
do	O
you	O
mean	O
by	O
'	O
random	O
sort	O
'	O
?	O

However	O
,	O
it	O
indeed	O
does	O
not	O
allow	O
you	O
to	O
exponentiate	O
any	O
matrix	O
directly	O
:	O
#CODE	O

The	O
first	O
one	O
would	O
probably	O
be	O
slow	O
for	O
large	O
data	O
,	O
and	O
the	O
second	O
one	O
does	O
not	O
seem	O
to	O
offer	O
any	O
other	O
interpolation	O
method	O
except	O
splines	O
.	O

EDIT	O
:	O
For	O
the	O
record	O
,	O
here	O
is	O
example	O
code	O
that	O
demonstrates	O
the	O
issue	O
:	O
#CODE	O

In	O
terms	O
of	O
functionality	O
,	O
it's	O
not	O
a	O
metaclass	O
at	O
all	O
,	O
it's	O
a	O
function	O
which	O
takes	O
a	O
class	O
(	O
along	O
with	O
some	O
others	O
stuff	O
)	O
and	O
returns	O
a	O
new	O
class	O
.	O

or	O
from	O
source	O
#CODE	O

Didn't	O
do	O
any	O
timings	O
here	O
,	O
but	O
it's	O
possible	O
this	O
version	O
has	O
reasonable	O
performance	O
.	O

I'll	O
just	O
test	O
whether	O
or	O
not	O
there's	O
a	O
significan	O
space	O
gain	O
on	O
my	O
data	O
with	O
protocol	O
2	O
.	O

Just	O
wanted	O
to	O
add	O
that	O
the	O
moving	O
average	O
function	O
has	O
been	O
extracted	O
into	O
the	O
[	O
Bottleneck	O
]	O
(	O
#URL	O
)	O
library	O
if	O
pandas	O
seems	O
too	O
heavyweight	O
as	O
a	O
dependency	O
.	O

`	O
numpy	O
`	O
can	O
use	O
`	O
malloc	O
`	O
/	O
`	O
realloc	O
`	O
for	O
creating	O
an	O
array	O
of	O
objects	O
of	O
`	O
sizeof	O
(	O
int	O
)`	O
.	O

If	O
you	O
read	O
through	O
the	O
documentation	O
of	O
those	O
engines	O
you	O
will	O
often	O
find	O
statements	O
saying	O
that	O
they	O
are	O
optimized	O
for	O
speed	O
(	O
30fps	O
-	O
60fps	O
)	O
.	O

I	O
know	O
I	O
have	O
to	O
look	O
at	O
each	O
row	O
,	O
but	O
I	O
don't	O
want	O
to	O
do	O
it	O
with	O
loops	O
.	O

I'm	O
hoping	O
this	O
can	O
at	O
least	O
save	O
someone	O
a	O
few	O
hours	O
of	O
hopeless	O
research	O
for	O
this	O
topic	O
.	O

What	O
does	O
"	O
doesn't	O
work	O
"	O
mean	O
?	O

You	O
might	O
look	O
at	O
your	O
code	O
that	O
generates	O
the	O
`	O
y	O
`	O
values	O
,	O
and	O
see	O
if	O
that	O
would	O
benefit	O
from	O
the	O
use	O
of	O
additional	O
numpy	O
or	O
scipy	O
functions	O
.	O

For	O
example	O
,	O
give	O
the	O
list	O
[	O
(	O
.3	O
,	O
'	O
a	O
')	O
,	O
(	O
.4	O
,	O
'	O
b	O
')	O
,	O
(	O
.3	O
,	O
'	O
c	O
')]	O
I'd	O
like	O
to	O
sample	O
'	O
b	O
'	O
40%	O
of	O
the	O
time	O
.	O

The	O
square	O
bracket	O
idea	O
you	O
mentioned	O
works	O
for	O
my	O
current	O
problem	O
.	O

However	O
,	O
when	O
I	O
try	O
to	O
use	O
a	O
weighted	O
average	O
#CODE	O

If	O
I	O
scale	O
that	O
to	O
a	O
2000	O
X	O
2000	O
np	O
array	O
,	O
here	O
is	O
what	O
I	O
get	O
:	O
#CODE	O

Unfortunately	O
,	O
I'm	O
not	O
aware	O
of	O
a	O
numpy	O
implementation	O
,	O
but	O
I	O
did	O
find	O
this	O
:	O
#URL	O

We	O
can	O
do	O
this	O
quite	O
neatly	O
using	O
numpy	O
,	O
without	O
having	O
to	O
worry	O
about	O
the	O
channels	O
at	O
all	O
!	O

It's	O
not	O
exactly	O
the	O
most	O
efficient	O
way	O
,	O
but	O
it	O
will	O
work	O
,	O
and	O
not	O
require	O
keeping	O
a	O
copy	O
of	O
the	O
file	O
in	O
memory	O
(	O
or	O
two	O
)	O
.	O

I'm	O
trying	O
to	O
find	O
the	O
fastest	O
way	O
to	O
find	O
the	O
first	O
non-zero	O
value	O
for	O
each	O
row	O
of	O
a	O
two	O
dimensional	O
sorted	O
array	O
.	O

Convert	O
the	O
numpy	O
array	O
into	O
a	O
list	O
first	O
.	O

@USER	O
do	O
you	O
mean	O
that	O
grid	O
dictonary	O
holds	O
the	O
results	O
in	O
memory	O
or	O
something	O
else	O
?	O

@USER	O
:	O
This	O
solution	O
is	O
indeed	O
designed	O
to	O
give	O
the	O
set	O
of	O
all	O
the	O
numbers	O
found	O
in	O
the	O
array	O
.	O

This	O
way	O
at	O
most	O
one	O
line	O
is	O
in	O
memory	O
at	O
any	O
one	O
time	O
.	O

It's	O
low	O
efficiency	O
,	O
I	O
want	O
to	O
know	O
are	O
there	O
any	O
builtin	O
functions	O
that	O
can	O
do	O
this	O
in	O
NumPy	O
.	O

The	O
raw	O
hardware	O
data	O
is	O
32-bit	O
signed	O
integer	O
,	O
which	O
becomes	O
float	O
when	O
I	O
convert	O
it	O
to	O
normal	O
physics	O
units	O
(	O
m	O
/	O
s	O
)	O

Calculate	O
subset	O
of	O
matrix	O
multiplication	O

EDIT	O
:	O
as	O
to	O
what	O
DSM	O
pointed	O
out	O
,	O
OP	O
is	O
infact	O
using	O
a	O
numpy	O
array	O
.	O

As	O
for	O
the	O
second	O
question	O
,	O
`	O
delete	O
`	O
has	O
been	O
suggested	O
before	O
:	O
#CODE	O

The	O
remaining	O
rows	O
of	O
the	O
matrices	O
are	O
all	O
linear	O
combinations	O
(	O
in	O
fact	O
exact	O
copies	O
for	O
almost	O
all	O
submatrices	O
)	O
of	O
these	O
rows	O
.	O

I'm	O
printing	O
the	O
contents	O
of	O
an	O
array	O
with	O
a	O
header	O
.	O

Edit	O
:	O
If	O
you	O
need	O
to	O
save	O
memory	O
try	O
radix	O
sort	O
,	O
which	O
is	O
much	O
faster	O
on	O
integers	O
than	O
quicksort	O
(	O
which	O
I	O
believe	O
is	O
what	O
numpy	O
uses	O
)	O
.	O

The	O
data	O
was	O
in	O
following	O
order	O
(	O
with	O
sample	O
data	O
)	O
#CODE	O

definitely	O
a	O
good	O
solution	O
,	O
nevertheless	O
I'd	O
like	O
to	O
solve	O
this	O
without	O
a	O
range	O
,	O
but	O
the	O
nearest	O
neighbour	O
.	O

For	O
my	O
recent	O
project	O
that	O
works	O
on	O
the	O
order	O
of	O
20000x20000	O
matrix	O
entries	O
,	O
I	O
will	O
quickly	O
and	O
disastrously	O
use	O
up	O
all	O
of	O
my	O
workstation's	O
8GB	O
of	O
RAM	O
and	O
more	O
.	O

Well	O
,	O
this	O
might	O
give	O
a	O
small	O
speed-up	O
just	O
because	O
it	O
uses	O
less	O
memory	O
.	O

Try	O
using	O
`	O
all	O
`	O
(	O
edited	O
to	O
return	O
`	O
int	O
`)	O
:	O
#CODE	O

:	O
I	O
don't	O
care	O
if	O
the	O
statement	O
modifies	O
array	O
or	O
not	O
.	O

Rather	O
than	O
doing	O
nans	O
I	O
put	O
in	O
-1	O
and	O
then	O
filtered	O
on	O
match	O
=	O
b	O
>	O
=	O
0	O
.	O

Removing	O
duplicate	O
columns	O
and	O
rows	O
from	O
a	O
NumPy	O
2D	O
array	O

arrays	O
to	O
extract	O
arrays	O
of	O
the	O
same	O

as	O
i	O
have	O
to	O
test	O
if	O
it	O
works	O
or	O
not	O
so	O
minimum	O
i	O
have	O
to	O
try	O
with	O
10-15	O
different	O
types	O
of	O
images	O
.	O
its	O
not	O
specific	O
images	O
it	O
can	O
be	O
any	O
image	O
of	O
people	O
.	O

So	O
having	O
a	O
python	O
loop	O
,	O
and	O
having	O
to	O
sum	O
all	O
the	O
results	O
together	O
,	O
is	O
taking	O
390	O
ms	O
more	O
than	O
200	O
times	O
what	O
it	O
takes	O
to	O
solve	O
each	O
of	O
the	O
200	O
systems	O
that	O
have	O
to	O
be	O
solved	O
.	O

You	O
could	O
then	O
sample	O
pixels	O
at	O
the	O
locations	O
of	O
the	O
new	O
points	O
.	O

I	O
could	O
use	O
an	O
idea	O
about	O
either	O
how	O
to	O
fix	O
the	O
compilation	O
errors	O
or	O
another	O
way	O
to	O
convert	O
my	O
python	O
objects	O
.	O

I	O
tried	O
every	O
possible	O
combination	O
resulting	O
with	O
a	O
0	O
bytes	O
file	O
if	O
the	O
extension	O
was	O
mpg	O
,	O
and	O
5.5kb	O
if	O
it	O
was	O
avi	O
.	O

Fortran	O
90	O
DOES	O
support	O
arbitrary	O
lower	O
bounds	O
on	O
arrays	O
,	O
and	O
borrowing	O
from	O
that	O
paradigm	O
sounds	O
quite	O
plausible	O
.	O

I	O
want	O
to	O
perform	O
an	O
operation	O
on	O
a	O
that	O
increments	O
all	O
the	O
values	O
inside	O
it	O
that	O
are	O
less	O
than	O
0	O
and	O
leaves	O
the	O
rest	O
alone	O
.	O
for	O
example	O
,	O
if	O
I	O
had	O
:	O
#CODE	O

I	O
like	O
how	O
this	O
one	O
uses	O
straight	O
python	O
slicing	O
and	O
doesn't	O
require	O
numpy	O

I	O
select	O
the	O
first	O
value	O
using	O
`	O
ys	O
[	O
0	O
]`	O
.	O

While	O
waiting	O
for	O
the	O
next	O
buffer	O
to	O
fill	O
,	O
I'd	O
like	O
to	O
process	O
the	O
most	O
recent	O
buffer	O
with	O
numpy	O
and	O
save	O
the	O
result	O
.	O

(	O
They	O
all	O
do	O
the	O
same	O
thing	O
,	O
in	O
this	O
case	O
.	O
)	O

As	O
of	O
PIL	O
1.1.6	O
,	O
the	O
"	O
proper	O
"	O
way	O
to	O
convert	O
between	O
images	O
and	O
numpy	O
arrays	O
is	O
simply	O
#CODE	O

So	O
first	O
you	O
need	O
to	O
construct	O
an	O
array	O
that	O
represents	O
the	O
rows	O
you	O
wish	O
to	O
select	O
.	O

Any	O
ideas	O
how	O
to	O
improve	O
this	O
?	O

If	O
this	O
is	O
the	O
case	O
,	O
you	O
can	O
easily	O
plot	O
a	O
known	O
asymmetric	O
shape	O
and	O
the	O
plot	O
will	O
tell	O
you	O
everything	O
.	O

Apart	O
from	O
the	O
compression	O
part	O
,	O
this	O
shouldn't	O
be	O
any	O
slower	O
than	O
normal	O
.	O

Acquiring	O
the	O
Minimum	O
array	O
out	O
of	O
Multiple	O
Arrays	O
by	O
order	O
in	O
Python	O

first	O
copy	O
#CODE	O

Getting	O
all	O
points	O
where	O
y=2	O
.	O

moving	O
average	O
function	O
on	O
numpy	O
/	O
scipy	O
?	O

for	O
a	O
N	O
dimensional	O
array	O
:	O
#CODE	O

From	O
looking	O
at	O
#URL	O
it	O
seems	O
that	O
it	O
was	O
originally	O
required	O
because	O
indexing	O
with	O
`	O
...	O

In	O
other	O
words	O
...	O
yes	O
you	O
can	O
trust	O
it	O
to	O
be	O
faster	O
...	O
but	O
don't	O
think	O
python	O
is	O
that	O
slow	O
that	O
you	O
cannot	O
do	O
a	O
for	O
loop	O
over	O
3	O
items	O
without	O
waiting	O
for	O
ages	O
...	O

And	O
all	O
that	O
while	O
you're	O
getting	O
lunch	O
.	O

If	O
you	O
do	O
want	O
to	O
raise	O
some	O
sort	O
of	O
exception	O
for	O
invalid	O
data	O
(	O
not	O
type	O
checking	O
)	O
,	O
either	O
let	O
an	O
existing	O
exception	O
propagate	O
,	O
or	O
wrap	O
it	O
in	O
your	O
own	O
exception	O
type	O
.	O

casting	O
are	O
used	O
in	O
Numexpr	O
,	O
in	O
contrast	O
with	O
NumPy	O
,	O
where	O
array	O
types	O

I	O
check	O
mathexchange	O
and	O
while	O
making	O
the	O
tags	O
for	O
the	O
post	O
,	O
it	O
didn't	O
have	O
any	O
the	O
ones	O
that	O
would	O
seem	O
relevant	O
like	O
scipy	O
and	O
numpy	O
or	O
even	O
sparse	O
.	O

Any	O
help	O
would	O
be	O
appreciated	O
.	O

Say	O
not	O
that	O
efficiency	O
is	O
a	O
secondary	O
priority	O
;	O
say	O
instead	O
that	O
I	O
want	O
to	O
perform	O
bivariate	O
optimization	O
:	O
pythonicity	O
+	O
efficiency	O
(	O
hence	O
the	O
post	O
title	O
)	O
.	O

Does	O
numpy	O
have	O
any	O
constructs	O
to	O
make	O
this	O
easier	O
?	O

If	O
you	O
view	O
`	O
P	O
`	O
as	O
a	O
rank-2	O
tensor	O
then	O
only	O
three	O
options	O
exist	O
for	O
the	O
product	O
of	O
`	O
P	O
`	O
with	O
itself	O
,	O
1	O
)	O
either	O
all	O
the	O
indexes	O
cancel	O
leaving	O
you	O
with	O
a	O
rank-0	O
tensor	O
(	O
a	O
scalar	O
)	O
,	O
2	O
)	O
1	O
set	O
of	O
indexes	O
cancels	O
and	O
you	O
are	O
left	O
with	O
a	O
rank-2	O
tensor	O
(	O
a	O
matrix	O
)	O
,	O
or	O
3	O
)	O
none	O
of	O
them	O
cancel	O
and	O
you're	O
left	O
with	O
a	O
rank-4	O
tensor	O
.	O

At	O
the	O
moment	O
I	O
made	O
a	O
custom	O
iterator	O
class	O
that	O
builds	O
a	O
list	O
of	O
lists	O
.	O

This	O
basically	O
sees	O
whether	O
two	O
circles	O
(	O
with	O
coordinates	O
that	O
correspond	O
to	O
the	O
indices	O
n	O
and	O
m	O
)	O
connect	O
.	O

It	O
would	O
mean	O
Gaussian	O
quadrature	O
using	O
points	O
along	O
the	O
line	O
that	O
are	O
easily	O
evaluated	O
.	O

I	O
see	O
one	O
potential	O
problem	O
-	O
beta	O
is	O
defined	O
as	O
1-dimensional	O
,	O
but	O
its	O
value	O
is	O
given	O
as	O
2-dimensional	O
(	O
dimensions	O
of	O
size	O
1	O
still	O
count	O
as	O
dimensions	O
)	O

Can	O
you	O
provide	O
an	O
algorithm	O
for	O
computing	O
them	O
based	O
on	O
block	O
size	O
?	O

If	O
`	O
X	O
`	O
is	O
your	O
array	O
,	O
#CODE	O

You	O
could	O
cast	O
the	O
array	O
to	O
a	O
list	O
:	O
#CODE	O

But	O
if	O
you	O
for	O
example	O
self	O
compiled	O
from	O
the	O
development	O
version	O
an	O
update	O
may	O
fix	O
most	O
of	O
it	O
.	O

In	O
VTK	O
I	O
am	O
able	O
to	O
use	O
the	O
following	O
snippet	O
to	O
save	O
the	O
render	O
window	O
as	O
an	O
image	O
.	O

In	O
other	O
words	O
,	O
the	O
4th	O
row	O
in	O
A_sorted	O
was	O
the	O
1st	O
row	O
in	O
the	O
original	O
array	O
,	O
A	O
,	O
etc	O
.	O

Add	O
a	O
`	O
return	O
`	O
statement	O
at	O
the	O
end	O
of	O
the	O
method	O
.	O

However	O
,	O
with	O
different	O
input	O
sizes	O
,	O
using	O
fft's	O
to	O
do	O
a	O
convolution	O
can	O
be	O
considerably	O
faster	O
(	O
Though	O
I	O
can't	O
seem	O
to	O
come	O
up	O
with	O
a	O
good	O
example	O
,	O
at	O
the	O
moment	O
...	O
)	O
.	O

That	O
is	O
a	O
shallow	O
copy	O
...	O

@USER	O
The	O
solutions	O
there	O
all	O
make	O
use	O
of	O
the	O
fact	O
that	O
only	O
a	O
3x3	O
sliding	O
window	O
is	O
needed	O
,	O
but	O
I	O
need	O
something	O
that	O
works	O
for	O
all	O
sizes	O
of	O
templates	O
.	O

A	O
masked	O
array	O
is	O
useful	O
here	O
:	O
#CODE	O

Second	O
I	O
would	O
like	O
it	O
to	O
be	O
easily	O
expandable	O
,	O
that	O
I	O
can	O
add	O
new	O
functions	O
easily	O
.	O

If	O
i	O
have	O
two	O
variables	O
-	O
where	O
they	O
either	O
are	O
a	O
1d	O
array	O
of	O
values	O
length	O
n	O
,	O
or	O
are	O
a	O
single	O
value	O
,	O
how	O
do	O
i	O
loop	O
through	O
them	O
so	O
that	O
I	O
get	O
n	O
values	O
returned	O
.	O

I	O
think	O
masked	O
arrays	O
have	O
been	O
in	O
numpy	O
for	O
a	O
few	O
years	O
now	O
.	O

I	O
have	O
an	O
array	O
defined	O
in	O
this	O
way	O
(	O
extracting	O
the	O
third	O
column	O
of	O
a	O
dataset	O
):	O
#CODE	O

I	O
wasn't	O
aware	O
of	O
the	O
option	O
of	O
using	O
`	O
data	O
[	O
list	O
]`	O
to	O
select	O
multiple	O
columns	O
.	O

Beware	O
that	O
if	O
the	O
type	O
of	O
the	O
output	O
array	O
is	O

Any	O
ideas	O
how	O
I	O
would	O
do	O
this	O
calculation	O
in	O
a	O
simpler	O
way	O
?	O

Those	O
take	O
up	O
about	O
15MB	O
of	O
space	O
which	O
,	O
considering	O
that	O
I	O
get	O
about	O
1000	O
result	O
files	O
in	O
a	O
series	O
,	O
is	O
unacceptable	O
to	O
save	O
.	O

I'm	O
trying	O
to	O
implement	O
a	O
logic	O
where	O
I'm	O
trying	O
to	O
subtract	O
each	O
element	O
of	O
an	O
array	O
from	O
every	O
other	O
element	O
of	O
the	O
array	O
and	O
then	O
find	O
the	O
minimum	O
difference	O
of	O
the	O
result	O
.	O

My	O
final	O
matrix	O
should	O
have	O
5416	O
rows	O
with	O
500	O
000	O
column	O
each	O
.	O

I	O
am	O
attempting	O
to	O
process	O
data	O
saved	O
to	O
CSV	O
that	O
may	O
have	O
missing	O
values	O
in	O
an	O
unknown	O
number	O
of	O
columns	O
(	O
up	O
to	O
around	O
30	O
)	O
.	O

Actually	O
the	O
surprise	O
is	O
still	O
hidden	O
in	O
a	O
way	O
,	O
because	O
in	O
your	O
examples	O
`	O
a	O
[	O
indices	O
]`	O
is	O
the	O
same	O
as	O
`	O
a	O
[	O
indices	O
[	O
0	O
]	O
,	O
indicies	O
[	O
1	O
]]`	O
but	O
`	O
a	O
[	O
indicies	O
,	O
:]	O
`	O
is	O
`	O
a	O
[(	O
indicies	O
[	O
0	O
]	O
,	O
indicies	O
[	O
1	O
])	O
,	O
:]	O
`	O
which	O
is	O
not	O
a	O
big	O
surprise	O
that	O
it	O
is	O
different	O
.	O

Slice	O
numpy	O
array	O
wth	O
list	O
of	O
wanted	O
rows	O

Here	O
is	O
a	O
slightly	O
more	O
complex	O
version	O
that	O
always	O
returns	O
a	O
view	O
into	O
the	O
original	O
array	O
(	O
of	O
course	O
provided	O
that	O
you	O
don't	O
do	O
any	O
advanced	O
indexing	O
;	O
this	O
should	O
be	O
guaranteed	O
by	O
your	O
specification	O
of	O
valid	O
indices	O
):	O
#CODE	O

If	O
the	O
array	O
is	O
one-dimensional	O
,	O
this	O
means	O
it	O
has	O
no	O
effect	O
.	O

The	O
problem	O
with	O
using	O
`	O
view	O
`	O
,	O
however	O
,	O
is	O
that	O
a	O
32-bit	O
integer	O
becomes	O
viewed	O
as	O
4	O
8-b	O
it	O
integers	O
,	O
and	O
we	O
only	O
care	O
about	O
the	O
value	O
in	O
the	O
last	O
8-b	O
its	O
.	O

So	O
this	O
won't	O
solve	O
OP's	O
question	O
(	O
unless	O
his	O
nD	O
is	O
2	O
or	O
3	O
)	O
.	O

Note	O
:	O
this	O
was	O
meant	O
as	O
a	O
comment	O
,	O
not	O
an	O
answer	O
...	O
just	O
needed	O
more	O
room	O
to	O
put	O
in	O
the	O
example	O
above	O
.	O

If	O
I	O
wanted	O
to	O
change	O
the	O
data	O
type	O
of	O
a	O
numpy	O
array	O
permanently	O
,	O
is	O
reassignment	O
the	O
best	O
way	O
?	O

The	O
shifted	O
shm-allocated	O
array	O
is	O
indeed	O
accessible	O
from	O
other	O
processes	O
.	O

The	O
second	O
solution	O
you	O
propose	O
is	O
better	O
from	O
that	O
point	O
of	O
view	O
.	O

It	O
checks	O
for	O
nans	O
and	O
empty	O
input	O
strings	O
.	O

The	O
ability	O
to	O
extract	O
columns	O
and	O
rows	O
by	O
header	O
name	O
(	O
as	O
in	O
the	O
above	O
example	O
)	O

With	O
Python	O
2.6.5	O
or	O
Python	O
2.7	O
,	O
and	O
Numpy	O
1.5.0	O
,	O
I	O
don't	O
get	O
any	O
error	O
.	O

Returns	O
a	O
boolean	O
array	O
the	O
same	O
length	O
as	O
`	O
ar1	O
`	O
that	O
is	O
True	O

If	O
the	O
objects	O
in	O
the	O
array	O
are	O
not	O
fixed	O
size	O
(	O
such	O
as	O
your	O
MultiEvent	O
)	O
the	O
operations	O
can	O
become	O
much	O
slower	O
.	O

@USER	O
:	O
True	O
the	O
code	O
I	O
gave	O
would	O
not	O
be	O
all	O
the	O
efficient	O

You	O
print	O
`	O
Length	O
of	O
together	O
2708000000	O
`	O
-	O
where	O
is	O
that	O
`	O
print	O
`	O
statement	O
in	O
your	O
code	O
?	O

Is	O
there	O
a	O
way	O
to	O
copy	O
just	O
the	O
reference	O
to	O
b	O
,	O
so	O
that	O
when	O
I	O
change	O
b	O
,	O
the	O
change	O
is	O
reflected	O
in	O
`	O
a	O
[	O
'	O
B	O
']	O
[	O
i	O
]`	O
?	O

If	O
the	O
matrix	O
is	O
not	O
symmetric	O
be	O
careful	O
about	O
the	O
order	O
in	O
dot	O
.	O

So	O
I	O
changed	O
the	O
"	O
array	O
"	O
to	O
matrix	O
.	O

config	O
paths	O
,	O
cflags	O
.	O

gnibbler	O
:	O
That	O
completely	O
misses	O
the	O
point	O
of	O
the	O
algorithm	O
,	O
this	O
is	O
a	O
simple	O
example	O
,	O
what	O
I	O
am	O
doing	O
is	O
Gauss-Seidel	O
iteration	O
,	O
which	O
infers	O
information	O
about	O
a	O
location	O
in	O
a	O
matrix	O
by	O
using	O
data	O
that	O
has	O
already	O
been	O
inferred	O
in	O
previous	O
entries	O
.	O

how	O
do	O
i	O
find	O
the	O
smallest	O
then	O
if	O
this	O
gives	O
me	O
the	O
n	O
greatest	O
values	O
?	O

@USER	O
:	O
you	O
can	O
wrap	O
the	O
insides	O
the	O
prange	O
loop	O
in	O
`	O
with	O
nogil	O
:	O
`	O
to	O
use	O
any	O
Python	O
constructs	O
.	O

For	O
each	O
`	O
Xi	O
`	O
greater	O
than	O
`	O
lower_limit_X	O
`	O
and	O
less	O
than	O
`	O
upper_limit_X	O
`	O
,	O
I	O
would	O
like	O
to	O
get	O
the	O
number	O
of	O
`	O
Yi	O
`'	O
s	O
that	O
are	O
greater	O
than	O
`	O
lower_limit_Y	O
`	O
and	O
less	O
than	O
`	O
upper_limit_Y	O
`	O
.	O

and	O
find	O
the	O
roots	O
?	O

I'd	O
like	O
to	O
generalize	O
this	O
to	O
an	O
ellipsoid	O
,	O
that	O
could	O
ideally	O
have	O
any	O
rotation	O
.	O

Is	O
there	O
any	O
way	O
to	O
integrate	O
the	O
entire	O
array	O
at	O
once	O
,	O
or	O
do	O
I	O
need	O
to	O
integrate	O
element-by-element	O
?	O

I	O
need	O
to	O
sum	O

with	O
'	O
f	O
'	O
and	O
'	O
F	O
'	O
,	O
or	O
before	O
and	O
after	O
the	O
decimal	O
point	O
for	O
a	O
floating	O

You	O
can	O
write	O
that	O
as	O
a	O
matrix	O
:	O
#CODE	O

In	O
fact	O
,	O
if	O
you	O
use	O
tuples	O
as	O
Justin	O
suggested	O
and	O
iterate	O
directly	O
over	O
the	O
rows	O
of	O
the	O
array	O
(	O
`	O
for	O
row	O
in	O
data	O
:	O
`)	O
,	O
it's	O
actually	O
faster	O
than	O
my	O
method	O
below	O
.	O

Using	O
the	O
`	O
ctypedef	O
`	O
keyword	O
in	O
Cython	O
will	O
make	O
it	O
add	O
the	O
C	O
/	O
C++	O
`	O
typedef	O
`	O
statement	O
with	O
the	O
given	O
types	O
in	O
the	O
compiled	O
Cython-code	O
.	O

Here	O
is	O
how	O
I	O
would	O
compute	O
a	O
subset	O
of	O
the	O
elements	O
of	O
C	O
given	O
a	O
list	O
of	O
tuples	O
of	O
C	O
index	O
values	O
.	O

In	O
my	O
specific	O
problem	O
`	O
A	O
,	O
B	O
`	O
are	O
slices	O
out	O
of	O
a	O
bigger	O
3-dimensional	O
array	O
`	O
Z	O
`	O
,	O

The	O
solution	O
to	O
get	O
all	O
the	O
data	O
you	O
need	O
as	O
you	O
build	O
the	O
list	O
,	O
by	O
using	O
the	O
accessor	O
on	O
each	O
iteration	O
.	O

Edit	O
:	O
Actually	O
you	O
could	O
also	O
sort	O
both	O
arrays	O
into	O
one	O
(	O
and	O
remember	O
which	O
one	O
belongs	O
to	O
which	O
class	O
)	O
,	O
then	O
go	O
from	O
there	O
by	O
checking	O
where	O
two	O
of	O
the	O
different	O
class	O
are	O
next	O
to	O
each	O
other	O
.	O

How	O
can	O
I	O
integrate	O
it	O
from	O
a	O
given	O
value	O
`	O
a	O
`	O
to	O
another	O
value	O
`	O
b	O
`	O
so	O
that	O
the	O
output	O
is	O
a	O
corresponding	O
array	O
?	O

I	O
want	O
to	O
multiply	O
a	O
sparse	O
matrix	O
A	O
,	O
with	O
a	O
matrix	O
B	O
which	O
has	O
0	O
,	O
-1	O
,	O
or	O
1	O
as	O
elements	O
.	O

If	O
you	O
turn	O
a	O
`	O
dict	O
`	O
into	O
an	O
array	O
,	O
you'll	O
get	O
an	O
object	O
array	O
.	O

`	O
Holy	O
CPU	O
cycles	O
batman	O
!	O

Note	O
:	O
Xarray	O
and	O
Yarray	O
are	O
each	O
single-column	O
vectors	O
with	O
data	O
at	O
each	O
index	O
that	O
links	O
the	O
two	O
arrays	O
as	O
sets	O
of	O
x	O
,	O
y	O
coordinates	O
.	O

Yeah	O
,	O
my	O
rule-of-thumb	O
is	O
**	O
numpy	O
**	O
for	O
anything	O
that	O
can	O
handle	O
small	O
amounts	O
of	O
latency	O
but	O
has	O
the	O
potential	O
to	O
be	O
very	O
large	O
,	O
**	O
lists	O
**	O
for	O
smaller	O
data	O
sets	O
where	O
latency	O
critical	O
,	O
and	O
of	O
course	O
**	O
real	O
benchmarking	O
**	O
FTW	O
:)	O

You	O
need	O
to	O
know	O
what	O
kind	O
of	O
information	O
is	O
stored	O
in	O
each	O
field	O
for	O
the	O
`	O
struct	O
`	O
module	O
to	O
make	O
sense	O
of	O
each	O
field	O
.	O

Once	O
you	O
have	O
this	O
array	O
,	O
you	O
can	O
get	O
your	O
sums	O
for	O
each	O
vertex	O
as	O
#CODE	O

Where	O
are	O
`	O
plot	O
`	O
and	O
`	O
show	O
`	O
coming	O
from	O
?	O

But	O
it	O
might	O
be	O
easier	O
(	O
and	O
more	O
understandable	O
when	O
you	O
look	O
at	O
the	O
code	O
in	O
the	O
future	O
)	O
to	O
just	O
drop	O
into	O
Cython	O
to	O
get	O
this	O
done	O
.	O

I	O
can't	O
comment	O
on	O
the	O
Perl	O
code	O
,	O
I	O
simply	O
don't	O
know	O
it	O
at	O
all	O
.	O

I	O
updated	O
the	O
answer	O
with	O
a	O
full	O
example	O
.	O

There	O
you	O
will	O
find	O
C	O
code	O
which	O
provides	O
the	O
same	O
functionality	O
as	O
`	O
fmincon	O
`	O
.	O

so	O
for	O
instance	O
,	O
for	O
an	O
array	O
the	O
code	O
looked	O
like	O
#CODE	O

I've	O
been	O
trying	O
to	O
find	O
a	O
solution	O
for	O
hours	O
.	O

Note	O
that	O
as	O
implemented	O
here	O
,	O
the	O
first	O
method	O
gives	O
incorrect	O
results	O
according	O
to	O
my	O
test	O
.	O

This	O
is	O
my	O
test	O
code	O
:	O
#CODE	O

This	O
will	O
short-circuit	O
if	O
you	O
just	O
want	O
to	O
determine	O
if	O
any	O
match	O
exists	O
.	O

It	O
also	O
helps	O
to	O
know	O
that	O
a	O
resource	O
like	O
Wolfram	O
Alpha	O
is	O
available	O
to	O
you	O
at	O
all	O
times	O
.	O

What	O
I	O
meant	O
was	O
if	O
we	O
had	O
three	O
arrays	O
`	O
X	O
=[	O
0	O
1	O
2	O
]	O
,	O
Y	O
=[	O
0	O
1	O
1	O
]	O
,	O
and	O
Z	O
=[	O
0	O
2	O
2	O
]`	O
there	O
would	O
be	O
six	O
values	O
in	O
the	O
range	O
of	O
greater	O
than	O
or	O
equal	O
1	O
and	O
and	O
less	O
than	O
or	O
equal	O
to	O
2	O
.	O

You	O
should	O
really	O
split	O
this	O
into	O
two	O
separate	O
questions	O
since	O
each	O
part	O
is	O
distinct	O
.	O

This	O
doesn't	O
answer	O
your	O
problem	O
exactly	O
,	O
but	O
I	O
think	O
especially	O
with	O
the	O
sum	O
issue	O
that	O
you	O
should	O
see	O
significant	O
speedups	O
with	O
these	O
changes	O
.	O

Basically	O
you're	O
iterating	O
through	O
each	O
item	O
in	O
`	O
Xa	O
`	O
and	O
omitting	O
the	O
ones	O
that	O
don't	O
fall	O
with	O
the	O
range	O
.	O

This	O
code	O
has	O
been	O
written	O
following	O
the	O
tips	O
(	O
and	O
copy	O
/	O
pasting	O
):	O
#URL	O

I	O
would	O
look	O
at	O
this	O
question	O
:	O
#URL	O

As	O
@USER	O
-Anderson	O
asked	O
,	O
why	O
not	O
avoid	O
making	O
an	O
array	O
?	O

to	O
create	O
such	O
an	O
array	O
.	O

In	O
short	O
,	O
I	O
find	O
class	O
C	O
to	O
provide	O
an	O
implementation	O
that	O
is	O
over	O
60x	O
faster	O
than	O
the	O
method	O
in	O
the	O
original	O
post	O
.	O

I	O
suggest	O
allocating	O
an	O
array	O
of	O
the	O
correct	O
size	O
up-front	O
,	O
then	O
populating	O
it	O
with	O
data	O
in	O
each	O
iteration	O
.	O

edited	O
my	O
solution	O
to	O
include	O
what	O
I	O
would	O
do	O
.	O
let	O
me	O
know	O
if	O
this	O
work	O
,	O
as	O
i	O
cannot	O
fully	O
test	O
because	O
i	O
dont	O
have	O
test	O
data	O
.	O

The	O
problem	O
is	O
if	O
i	O
don't	O
want	O
0:10	O
,	O
but	O
an	O
arbitrary	O
set	O
of	O
indices	O
.	O

and	O
it	O
occurs	O
at	O
the	O
line	O
:	O
`	O
del	O
innerAry	O
[	O
j	O
]`	O

Do	O
you	O
have	O
any	O
references	O
to	O
back	O
up	O
that	O
`	O
NaN	O
handling	O
is	O
much	O
slower	O
than	O
"	O
normal	O
"	O
float	O
at	O
the	O
CPU	O
level	O
`	O
?	O

What	O
do	O
you	O
mean	O
by	O
the	O
line	O
?	O

For	O
example	O
if	O
we	O
stick	O
with	O
a	O
linear	O
search	O
we	O
can	O
at	O
least	O
start	O
at	O
the	O
appropriate	O
end	O
(	O
search	O
backwards	O
to	O
find	O
last	O
value	O
matching	O
a	O
condition	O
)	O
.	O

Yes	O
,	O
but	O
you	O
don't	O
get	O
a	O
numpy	O
array	O
out	O
,	O
do	O
you	O
?	O

This	O
seems	O
like	O
a	O
simple	O
question	O
,	O
but	O
I	O
haven't	O
been	O
able	O
to	O
find	O
a	O
good	O
answer	O
.	O

@USER	O
:	O
it	O
could	O
also	O
mean	O
they	O
are	O
both	O
strictly	O
periodic	O
and	O
sinusoidal	O
,	O
but	O
their	O
frequencies	O
are	O
integer-independent	O
(	O
ie	O
.	O
the	O
interference	O
wave	O
is	O
not	O
periodic	O
)	O

Python	O
:	O
How	O
to	O
rotate	O
an	O
array	O
?	O

@USER	O
Your	O
edited	O
sample	O
input	O
is	O
still	O
not	O
consistent	O
with	O
your	O
expected	O
output	O
.	O

You	O
can	O
calculate	O
the	O
variance	O
yourself	O
using	O
the	O
mean	O
,	O
with	O
the	O
following	O
formula	O
:	O
#CODE	O

Populate	O
numpy	O
matrix	O
from	O
the	O
difference	O
of	O
two	O
vectors	O

where	O
#CODE	O

Interleaving	O
two	O
numpy	O
index	O
arrays	O
,	O
one	O
item	O
from	O
each	O
array	O

I	O
was	O
worried	O
about	O
the	O
performance	O
,	O
but	O
the	O
difference	O
in	O
load	O
time	O
is	O
tiny	O
for	O
me	O
.	O

I	O
looked	O
for	O
an	O
online	O
reference	O
but	O
couldn't	O
find	O
one	O
.	O

Numpy	O
/	O
Python	O
:	O
Array	O
iteration	O
without	O
for-loop	O

If	O
you	O
want	O
it	O
printed	O
with	O
commas	O
,	O
you	O
could	O
convert	O
it	O
to	O
a	O
Python	O
list	O
:	O
#CODE	O

I	O
have	O
included	O
my	O
code	O
to	O
see	O
if	O
you	O
can	O
help	O
me	O
implement	O
some	O
kind	O
of	O
'	O
fminsearch	O
'	O
to	O
find	O
the	O
optimal	O
parameter	O
values	O
k0	O
and	O
k1	O
that	O
will	O
fit	O
my	O
data	O
.	O

Any	O
ideas	O
what	O
this	O
could	O
be	O
all	O
about	O
?	O

What	O
does	O
"	O
IIUC	O
"	O
mean	O
?	O

I	O
can't	O
seem	O
to	O
find	O
examples	O
that	O
don't	O
rely	O
on	O
the	O
former	O
syntax	O
.	O

Any	O
help	O
would	O
be	O
greatly	O
appreciated	O
.	O

Where	O
blocks	O
is	O
a	O
3	O
dimensional	O
numpy	O
array	O
.	O

The	O
science	O
/	O
engineering	O
application	O
I'm	O
working	O
on	O
has	O
lots	O
of	O
linear	O
algebra	O
matrix	O
multiplications	O
,	O
therefore	O
I	O
use	O
Numpy	O
matrices	O
.	O

when	O
I	O
print	O
Chao	O
,	O
the	O
product	O
of	O
this	O
loop	O
I	O
currently	O
have	O
this	O
:	O

pyopengl	O
buffer	O
dynamic	O
read	O
from	O
numpy	O
array	O

I	O
also	O
have	O
an	O
array	O
which	O
is	O
my	O
desired	O
subset	O
of	O
ages	O
.	O

It	O
does	O
automatically	O
expand	O
the	O
array	O
,	O
but	O
now	O
every	O
item	O
in	O
the	O
array	O
is	O
`	O
None	O
`	O
and	O
cannot	O
be	O
changed	O
.	O

I'm	O
liking	O
fortran	O
more	O
at	O
the	O
moment	O
because	O
by	O
the	O
time	O
you	O
add	O
all	O
the	O
required	O
type	O
annotations	O
in	O
cython	O
I	O
think	O
it	O
ends	O
up	O
looking	O
less	O
clear	O
than	O
the	O
fortran	O
.	O

(	O
There	O
are	O
also	O
chances	O
that	O
Python	O
stores	O
the	O
number	O
on	O
the	O
heap	O
and	O
all	O
you	O
get	O
is	O
a	O
pointer	O
to	O
it	O
,	O
approximately	O
doubling	O
the	O
footprint	O
,	O
without	O
even	O
taking	O
in	O
account	O
metadata	O
but	O
that's	O
slippery	O
grounds	O
,	O
I'm	O
always	O
wrong	O
when	O
I	O
talk	O
about	O
Python	O
internals	O
,	O
so	O
let's	O
not	O
dig	O
it	O
too	O
much	O
.	O
)	O

Do	O
you	O
have	O
any	O
clue	O
?	O

your	O
method	O
works	O
,	O
@USER	O
.	O
put	O
it	O
into	O
an	O
answer	O
to	O
get	O
some	O
acceptance	O
points	O
.	O

that	O
blas	O
is	O
reference	O
blas	O
from	O
netlib	O
-	O
the	O
slowest	O
blas	O
around	O
.	O
install	O
atlas	O
or	O
mkl	O
instead	O
.	O

The	O
`	O
not	O
`	O
operator	O
implicitly	O
tries	O
to	O
convert	O
its	O
operand	O
to	O
`	O
bool	O
`	O
,	O
and	O
then	O
returns	O
the	O
opposite	O
truth	O
value	O
.	O

The	O
recommended	O
way	O
to	O
do	O
this	O
is	O
to	O
preallocate	O
before	O
the	O
loop	O
and	O
use	O
slicing	O
and	O
indexing	O
to	O
insert	O
#CODE	O

If	O
the	O
simple	O
sort	O
solution	O
is	O
good	O
enough	O
,	O
clearly	O
go	O
for	O
that	O
.	O

Basic	O
idea	O
being	O
,	O
I	O
know	O
the	O
actual	O
value	O
of	O
that	O
should	O
be	O
predicted	O
for	O
each	O
sample	O
in	O
a	O
row	O
of	O
N	O
,	O
and	O
I'd	O
like	O
to	O
determine	O
which	O
set	O
of	O
predicted	O
values	O
in	O
a	O
column	O
of	O
M	O
is	O
most	O
accurate	O
using	O
the	O
residuals	O
.	O

Fill	O
in	O
missing	O
values	O
with	O
nearest	O
neighbour	O
in	O
Python	O
numpy	O
masked	O
arrays	O
?	O

Any	O
tips	O
on	O
what	O
I'm	O
doing	O
wrong	O
?	O

SO	O
I	O
have	O
a	O
file	O
having	O
three	O
columns	O
;	O
Frequency	O
,	O
Power	O
spec	O
.	O

In	O
numpy	O
,	O
your	O
array	O
is	O
2	O
x	O
5	O
,	O
isn't	O
it	O
?	O

For	O
small-ish	O
problems	O
,	O
I	O
would	O
certainly	O
just	O
create	O
the	O
new	O
array	O
.	O

If	O
each	O
element	O
takes	O
up	O
4	O
bytes	O
,	O
it	O
would	O
require	O
4,000,000,000,000	O
bytes	O
of	O
memory	O
.	O

TypeError	O
:	O
must	O
be	O
str	O
,	O
not	O
bytes	O

Error	O
while	O
computing	O
probabilities	O
of	O
an	O
array	O
list	O

This	O
produces	O
the	O
array	O
,	O
but	O
I	O
don't	O
know	O
which	O
row	O
corresponds	O
to	O
which	O
year-disease	O
.	O

I'm	O
not	O
sure	O
what	O
you	O
mean	O
by	O
"	O
all	O
from	O
numpy	O
"	O
,	O
but	O
you	O
should	O
never	O
need	O
to	O
use	O
more	O
than	O
one	O
form	O
of	O
`	O
import	O
`	O
at	O
a	O
time	O
.	O

The	O
problem	O
is	O
that	O
you	O
have	O
an	O
array	O
of	O
strings	O
,	O
not	O
an	O
array	O
of	O
numbers	O
.	O

how	O
can	O
i	O
effectively	O
check	O
items	O
of	O
a	O
list	O
of	O
tuples	O
against	O
all	O
the	O
items	O
of	O
another	O
using	O
numpy	O
or	O
tabular	O
?	O

Replace	O
part	O
of	O
numpy	O
1D	O
array	O
with	O
shorter	O
array	O

Python	O
2.6	O
numpy	O
interaction	O
array	O
objects	O
error	O

So	O
for	O
square	O
matrices	O
it	O
is	O
basically	O
syntactic	O
sugar	O
for	O
the	O
exact	O
same	O
operation	O
.	O

If	O
we	O
evaulate	O
the	O
ij	O
th	O
element	O
of	O
the	O
matrix	O
U*A*V	O
,	O
then	O
it	O
must	O
equal	O
both	O
#CODE	O

How	O
can	O
I	O
get	O
new	O
array	O
`	O
B	O
`	O
such	O
as	O
if	O
`	O
row_set	O
=	O
[	O
0	O
,	O
2	O
,	O
5	O
]`	O
,	O
then	O
`	O
B	O
=	O
[	O
A_row	O
[	O
0	O
]	O
,	O
A_row	O
[	O
2	O
]	O
,	O
A_row	O
[	O
5	O
]]`	O
?	O

I	O
have	O
tried	O
the	O
following	O
to	O
fix	O
it	O
:	O

The	O
weighted	O
sum	O
result	O
is	O
approximated	O
by	O
the	O
multiple	O
passes	O
and	O
actually	O
after	O
very	O
few	O
of	O
them	O
the	O
output	O
is	O
already	O
smooth	O
.	O

Basically	O
,	O
it	O
comes	O
down	O
to	O
checking	O
before	O
you	O
add	O
.	O

Any	O
ideas	O
?	O

Do	O
you	O
really	O
have	O
matrix	O
type	O
or	O
just	O
list	O
of	O
lists	O
from	O
python	O
?	O

zI	O
[	O
N-1	O
]	O
=	O
f	O
(	O
xI	O
[	O
N-1	O
]	O
,	O
yI	O
[	O
N-1	O
])	O
.	O

The	O
actual	O
size	O
of	O
the	O
numpy	O
array	O
is	O
514	O
by	O
504	O
and	O
of	O
the	O
list	O
is	O
8	O
.	O

`	O
array	O
([[	O
a	O
,	O
a	O
,	O
a	O
,	O
a	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
b	O
,	O
b	O
,	O
b	O
,...	O
]	O
,	O

What's	O
wrong	O
with	O
just	O
separating	O
it	O
out	O
into	O
real	O
and	O
imaginary	O
parts	O
?	O

I	O
have	O
an	O
N-dimensional	O
array	O
and	O
a	O
set	O
N	O
index	O
arrays	O
,	O
who's	O
values	O
I	O
want	O
to	O
increment	O
.	O

As	O
@USER	O
suggests	O
in	O
a	O
comment	O
,	O
if	O
you	O
really	O
want	O
a	O
3D	O
array	O
--	O
which	O
is	O
not	O
entirely	O
clear	O
to	O
me	O
from	O
your	O
code	O
sample	O
--	O
you	O
can	O
use	O
:	O
#CODE	O

In	O
another	O
question	O
,	O
other	O
users	O
offered	O
some	O
help	O
if	O
I	O
could	O
supply	O
the	O
array	O
I	O
was	O
having	O
trouble	O
with	O
.	O

The	O
other	O
thing	O
is	O
changing	O
the	O
size	O
of	O
the	O
ticklabels	O
in	O
the	O
colorbar	O
which	O
I	O
haven't	O
figured	O
out	O
.	O

Unfortunately	O
,	O
these	O
lines	O
are	O
fast	O
already	O
,	O
but	O
I	O
will	O
take	O
any	O
speedup	O
to	O
offset	O
the	O
IO	O
issues	O
I	O
have	O
using	O
GDAL	O
.	O

First	O
you	O
need	O
to	O
write	O
a	O
function	O
that	O
when	O
given	O
an	O
array	O
of	O
values	O
,	O
with	O
the	O
middle	O
one	O
being	O
the	O
element	O
currently	O
examined	O
,	O
will	O
return	O
some	O
computation	O
of	O
those	O
values	O
.	O

Thanks	O
in	O
advance	O
for	O
any	O
help	O
.	O

so	O
what	O
you	O
want	O
is	O
some	O
sort	O
of	O
recursive	O
assignment	O
--	O
but	O
i	O
don't	O
believe	O
there	O
is	O
any	O
guarantee	O
that	O
this	O
will	O
settle	O
down	O
into	O
a	O
constant	O
value	O
.	O
sure	O
it	O
does	O
in	O
your	O
case	O
,	O
but	O
not	O
in	O
general	O
--	O
for	O
example	O
:	O
`	O
a	O
[:	O
]	O
=	O
2*a	O
[:	O
]`	O
would	O
loop	O
forever	O
.	O

Just	O
initialize	O
the	O
array	O
of	O
`	O
float*	O
`	O
to	O
point	O
to	O
each	O
of	O
the	O
rows	O
in	O
the	O
2-D	O
array	O
.	O

Do	O
things	O
go	O
wrong	O
gradually	O
and	O
more	O
and	O
more	O
,	O
or	O
all	O
at	O
once	O
?	O

If	O
you	O
don't	O
find	O
anything	O
useful	O
then	O
try	O
"	O
R	O
"	O
.	O

Unfortunately	O
,	O
when	O
I	O
tried	O
it	O
I	O
got	O
the	O
error	O
:	O
"	O
ValueError	O
:	O
array	O
is	O
too	O
big	O
.	O

EDIT	O
2	O
:	O
This	O
raises	O
another	O
question	O
-	O
What	O
is	O
`	O
env	O
`	O
and	O
why	O
does	O
`	O
make	O
`	O
add	O
it	O
?	O

However	O
it	O
doesn't	O
give	O
you	O
negative	O
overflows	O
,	O
probably	O
because	O
`	O
uint32	O
`	O
fits	O
inside	O
the	O
positive	O
values	O
of	O
the	O
`	O
int64	O
`	O
.	O

Consider	O
for	O
example	O
the	O
array	O
:	O
#CODE	O

One	O
difference	O
could	O
be	O
the	O
result	O
of	O
python	O
having	O
to	O
take	O
extra	O
steps	O
to	O
resolve	O
the	O
float64	O
types	O
.	O

How	O
do	O
I	O
turn	O
this	O
into	O
a	O
numpy	O
matrix	O
?	O

I	O
want	O
to	O
rotate	O
an	O
array	O
but	O
not	O
as	O
a	O
whole	O
,	O
only	O
small	O
portion	O
of	O
it	O
.	O

is	O
so	O
much	O
more	O
readable	O
than	O
any	O
dot	O
(	O
a	O
,	O
b	O
)	O
equivalent	O
.	O

The	O
array	O
looks	O
like	O
:	O
#CODE	O

Constructing	O
an	O
n-by-n	O
matrix	O
in	O
Numpy	O
is	O
easy	O
and	O
fairly	O
efficient	O
.	O

Numpy	O
array	O
broadcasting	O
with	O
vector	O
parameters	O

It	O
only	O
works	O
like	O
this	O
for	O
numpy	O
`	O
array	O
`	O
s	O
.	O

@USER	O
:	O
Also	O
there's	O
a	O
mistake	O
in	O
your	O
code	O
I	O
think	O
:	O
because	O
when	O
you	O
set	O
elements	O
to	O
NaN	O
in	O
each	O
iteration	O
,	O
the	O
elements	O
are	O
not	O
restored	O
to	O
their	O
pre-NaN	O
values	O
for	O
the	O
next	O
iteration	O
!	O

Quick	O
question	O
:	O
is	O
there	O
any	O
reason	O
why	O
you	O
use	O

Now	O
the	O
question	O
is	O
,	O
which	O
equal	O
area	O
projection	O
shall	O
I	O
choose	O
in	O
order	O
to	O
have	O
comparable	O
area	O
sizes	O
for	O
the	O
polygons	O
.	O

numpy	O
array	O
multiplication	O
issue	O

(	O
2	O
)	O
When	O
I	O
change	O
the	O
connection	O
keywords	O
to	O
check_same_thread=False	O
,	O
then	O
the	O
full	O
pool	O
of	O
workers	O
is	O
used	O
,	O
but	O
then	O
only	O
some	O
queries	O
succeed	O
and	O
some	O
queries	O
fail	O
.	O

It	O
also	O
has	O
the	O
advantage	O
of	O
being	O
able	O
to	O
load	O
and	O
store	O
transparently	O
with	O
HDF5	O
.	O

Bah	O
:	O
"	O
operates	O
on	O
two	O
n-dimensional	O
arrays	O
"	O
should	O
be	O
"	O
operates	O
on	O
an	O
n-dimensional	O
array	O
"	O
above	O
.	O

Instead	O
,	O
they	O
expect	O
the	O
user	O
to	O
either	O
pass	O
an	O
array	O
of	O
shape	O
`	O
(	O
r	O
,	O
c	O
)`	O
exactly	O
,	O
or	O
for	O
the	O
user	O
to	O
pass	O
a	O
1-D	O
array	O
that	O
broadcasts	O
up	O
to	O
shape	O
`	O
(	O
r	O
,	O
c	O
)`	O
.	O

Though	O
,	O
it	O
isn't	O
so	O
straight	O
forward	O
because	O
I	O
don't	O
necessarily	O
know	O
how	O
many	O
duplicates	O
of	O
each	O
lon	O
or	O
lat	O
there	O
are	O
which	O
determines	O
the	O
shape	O
of	O
the	O
array	O
.	O

geom	O
function	O
takes	O
an	O
n+1	O
X	O
2	O
array	O
and	O
n	O
as	O
input	O
,	O
i	O
guess	O
i'm	O
doing	O
something	O
really	O
stupid	O
(	O
which	O
i	O
think	O
i	O
am	O
)	O
or	O
i	O
don't	O
understand	O
this	O
behavior	O
#CODE	O

When	O
I	O
tried	O
this	O
,	O
I	O
got	O
sort	O
of	O
similar	O
shaped	O
"	O
tiles	O
"	O
of	O
different	O
colors	O
rather	O
than	O
3	O
Gaussian	O
humps	O
.	O

There	O
was	O
a	O
comment	O
here	O
saying	O
that	O
the	O
Apple	O
version	O
of	O
python	O
2.7	O
comes	O
with	O
numpy	O
so	O
you	O
shouldn't	O
have	O
to	O
install	O
it	O
at	O
all	O
.	O

For	O
more	O
general	O
solution	O
,	O
you	O
could	O
use	O
somekind	O
of	O
edge	O
detection	O
method	O
to	O
find	O
only	O
the	O
edge	O
points	O
.	O

If	O
you	O
see	O
any	O
errors	O
;	O
provide	O
a	O
link	O
to	O
the	O
code	O
that	O
can	O
be	O
run	O
.	O

What	O
I	O
am	O
looking	O
for	O
is	O
a	O
quick	O
and	O
easy	O
way	O
to	O
find	O
the	O
closest	O
(	O
nearest	O
neighbor	O
)	O
of	O
some	O
multidimensional	O
query	O
point	O
in	O
an	O
2D	O
(	O
numpy	O
)	O
array	O
of	O
multidimensional	O
points	O
(	O
also	O
numpy	O
arrays	O
)	O
.	O

How	O
do	O
I	O
get	O
the	O
`	O
Image	O
`	O
part	O
only	O
and	O
how	O
do	O
I	O
convert	O
it	O
to	O
Numpy	O
Array	O
?	O

Maybe	O
it'll	O
save	O
you	O
some	O
frustration	O
=)	O

Is	O
there	O
any	O
way	O
to	O
rewrite	O
this	O
functions	O
with	O
Numpy	O
?	O

Is	O
there	O
any	O
easy	O
way	O
to	O
speed	O
my	O
calculation	O
up	O
?	O

A	O
variable	O
in	O
Python	O
is	O
just	O
a	O
label	O
for	O
an	O
object	O
;	O
giving	O
the	O
object	O
a	O
new	O
label	O
doesn't	O
change	O
the	O
object	O
itself	O
at	O
all	O
.	O

But	O
this	O
is	O
actually	O
where	O
the	O
doc	O
belongs	O
.	O

So	O
given	O
the	O
sorted	O
version	O
,	O
you	O
can	O
reconstruct	O
the	O
original	O
by	O
"	O
putting	O
items	O
back	O
where	O
they	O
came	O
from	O
"	O
:	O
#CODE	O

Note	O
that	O
this	O
all	O
assumes	O
that	O
your	O
values	O
are	O
normally	O
distributed	O
.	O

(	O
Bounding	O
box	O
intersections	O
are	O
actually	O
a	O
rather	O
poor	O
way	O
of	O
deciding	O
where	O
to	O
place	O
labels	O
.	O
What's	O
the	O
point	O
in	O
writing	O
a	O
ton	O
of	O
code	O
for	O
something	O
that	O
will	O
only	O
work	O
in	O
one	O
case	O
out	O
of	O
1000	O
?	O
)	O

The	O
python	O
code	O
outputs	O
eleven	O
0's	O
,	O
eleven	O
1's	O
all	O
the	O
way	O
to	O
eleven	O
39's	O
.	O

@USER	O
;	O
in	O
above	O
example	O
z	O
is	O
(	O
5	O
,	O
2	O
)	O
array	O
created	O
by	O
another	O
function	O
,	O
with	O
first	O
dimension	O
from	O
the	O
number	O
of	O
True	O
(	O
at	O
least	O
one	O
True	O
in	O
x	O
>	O
y	O
)	O
,	O
here	O
5	O
,	O
and	O
second	O
dimention	O
as	O
the	O
first	O
dimension	O
of	O
x	O
,	O
here	O
2	O
.	O

I	O
have	O
a	O
NumPy	O
array	O
of	O
values	O
.	O

I	O
just	O
need	O
the	O
total	O
of	O
all	O
the	O
values	O
instead	O
of	O
the	O
actual	O
values	O
themselves	O
.	O

Didn't	O
think	O
about	O
multiplying	O
my	O
array	O
of	O
number	O
by	O
an	O
array	O
of	O
booleans	O
to	O
extract	O
my	O
data	O
.	O

not	O
to	O
convert	O
floats	O
to	O
floats	O
would	O
be	O
the	O
first	O
step	O
.	O

make	O
a	O
list	O
of	O
all	O
the	O
days	O

This	O
way	O
,	O
you	O
could	O
access	O
all	O
the	O
`	O
A	O
`	O
through	O
`	O
result	O
[	O
'	O
label	O
']	O
[	O
'	O
A	O
']`	O
...	O

picking	O
out	O
elements	O
based	O
on	O
complement	O
of	O
indices	O
in	O
Python	O
pandas	O

Joran	O
,	O
could	O
you	O
please	O
explain	O
what	O
you	O
mean	O
more	O
?	O

Anyone	O
have	O
any	O
clues	O
for	O
what	O
I	O
can	O
do	O
,	O
or	O
approaches	O
I	O
should	O
research	O
?	O

Appending	O
data	O
to	O
an	O
existing	O
array	O
is	O
a	O
natural	O
thing	O
to	O
want	O
to	O
do	O
for	O
anyone	O
with	O
python	O
experience	O
.	O

Note	O
that	O
you	O
get	O
a	O
sorte	O
copy	O
of	O
the	O
array	O
.	O

I'm	O
not	O
sure	O
that	O
this	O
is	O
the	O
way	O
that	O
you	O
should	O
do	O
things	O
as	O
I'd	O
expect	O
numpy	O
to	O
have	O
a	O
much	O
more	O
efficient	O
method	O
of	O
going	O
about	O
it	O
,	O
but	O
do	O
you	O
just	O
mean	O
something	O
like	O
this	O
?	O

`	O
KMID	O
`	O
is	O
a	O
function	O
,	O
not	O
an	O
array	O
,	O
so	O
you	O
can't	O
index	O
it	O
with	O
`	O
:	O
`	O
.	O

Insert	O
to	O
original	O
code	O
in	O
question	O
:	O
#CODE	O

Multiple	O
conditions	O
using	O
'	O
or	O
'	O
in	O
numpy	O
array	O

Usually	O
,	O
it's	O
best	O
to	O
avoid	O
the	O
matrix	O
class	O
(	O
see	O
docs	O
)	O
.	O

I	O
have	O
tried	O
two	O
different	O
methods	O
but	O
both	O
of	O
them	O
are	O
slow	O
.	O

Not	O
really	O
,	O
you	O
can	O
construct	O
the	O
Counter	O
from	O
any	O
iterable	O
.	O

(	O
I	O
have	O
the	O
code	O
ready	O
,	O
but	O
as	O
i'm	O
new	O
to	O
stackoverflow	O
,	O
i	O
don't	O
know	O
where	O
to	O
put	O
it	O
.	O
Here	O
,	O
in	O
this	O
comment	O
field	O
?	O
Or	O
rather	O
making	O
a	O
new	O
answer	O
??	O
)	O

For	O
example	O
,	O
one	O
simple	O
method	O
to	O
generate	O
at	O
most	O
rank	O
`	O
k	O
`	O
(	O
when	O
`	O
m	O
`	O
is	O
`	O
k+1	O
`)	O
is	O
to	O
get	O
a	O
random	O
valid	O
B	O
0	O
,	O
keep	O
rotating	O
all	O
rows	O
of	O
this	O
matrix	O
up	O
to	O
get	O
B	O
1	O
to	O
B	O
m-2	O
,	O
set	O
first	O
row	O
of	O
B	O
m-1	O
to	O
all	O
1	O
,	O
and	O
the	O
remaining	O
rows	O
to	O
all	O
0	O
.	O

Glad	O
you	O
saw	O
around	O
it	O
!	O

@USER	O
:	O
basically	O
I	O
am	O
converting	O
some	O
matlab	O
code	O
into	O
python	O
,	O
I	O
can	O
not	O
write	O
actual	O
code	O
because	O
that	O
is	O
confidential	O
,	O
(	O
1+float	O
(	O
100	O
))	O
Here	O
100	O
is	O
coming	O
from	O
two	O
dimension	O
string	O
matrix	O
,	O
that	O
why	O
I	O
have	O
written	O
float	O
to	O
convert	O
string	O
variable	O
.	O

instead	O
of	O
call	O
plot	O
(	O
test	O
[	O
"	O
x	O
"]	O
[	O
5:10	O
])	O
,	O
you	O
can	O
call	O
the	O
plot	O
method	O
of	O
Series	O
object	O
:	O
#CODE	O

The	O
size	O
of	O
a	O
slice	O
with	O
`	O
0:5	O
`	O
is	O
not	O
6	O
as	O
you	O
say	O
:	O
it's	O
5	O
.	O

This	O
ensures	O
proper	O
display	O
and	O
syntax	O
highlighting	O
-	O
right	O
now	O
someone	O
who	O
would	O
usually	O
fix	O
your	O
formatting	O
is	O
likely	O
to	O
not	O
do	O
it	O
because	O
he'd	O
have	O
to	O
remove	O
all	O
the	O
HTML	O
linebreaks	O
on	O
his	O
own	O
.	O

I	O
believe	O
you've	O
reduced	O
the	O
problem	O
to	O
a	O
one	O
of	O
finding	O
roots	O
.	O

In	O
order	O
to	O
make	O
sure	O
it	O
is	O
still	O
multiprocessor	O
safe	O
,	O
I	O
believe	O
you	O
will	O
have	O
to	O
use	O
the	O
`	O
acquire	B-API
`	O
and	O
`	O
release	B-API
`	O
methods	O
that	O
exist	O
on	O
the	O
`	O
Array	O
`	O
object	O
,	O
`	O
a	O
`	O
,	O
and	O
its	O
built	O
in	O
lock	O
to	O
make	O
sure	O
its	O
all	O
safely	O
accessed	O
(	O
though	O
I'm	O
not	O
an	O
expert	O
on	O
the	O
multiprocessor	O
module	O
)	O
.	O

So	O
,	O
given	O
your	O
matrix	O
M	O
,	O
your	O
problem	O
asks	O
to	O
maximize	O
the	O
PB	O
function	O
#CODE	O

I	O
would	O
like	O
to	O
find	O
all	O
elements	O
within	O
a	O
specific	O
range	O
.	O

I	O
cannot	O
seem	O
to	O
find	O
how	O
to	O
do	O
that	O
.	O

Any	O
thoughts	O
?	O

If	O
you	O
want	O
the	O
PRNGs	O
to	O
be	O
independent	O
,	O
do	O
not	O
seed	O
them	O
with	O
anything	O
.	O

In	O
the	O
end	O
I'll	O
probably	O
take	O
n	O
randomly	O
selected	O
samples	O
.	O

This	O
is	O
a	O
mystery	O
to	O
me	O
,	O
though	O
I	O
would	O
guess	O
that	O
there	O
must	O
be	O
more	O
overhead	O
associated	O
with	O
accessing	O
an	O
array	O
element	O
than	O
with	O
appending	O
to	O
a	O
list	O
.	O

If	O
it's	O
not	O
reasonable	O
,	O
you	O
can	O
always	O
decompose	O
the	O
matrix	O
multiplication	O
yourself	O
.	O

If	O
the	O
vectors	O
do	O
not	O
have	O
equal	O
dimension	O
,	O
or	O
if	O
you	O
want	O
to	O
avoid	O
numpy	O
,	O
then	O
perhaps	O
,	O
#CODE	O

In	O
particular	O
,	O
you	O
can't	O
index	O
a	O
2D	O
matrix	O
with	O
a	O
single	O
integer	O
,	O
because	O
--	O
well	O
--	O
it's	O
two	O
dimensional	O
and	O
you	O
need	O
to	O
specify	O
two	O
integers	O
,	O
hence	O
the	O
need	O
for	O
the	O
extra	O
0	O
index	O
in	O
the	O
second	O
example	O
.	O

nearly	O
all	O
of	O
which	O
the	O
author	O
responded	O
to	O
and	O
in	O
some	O
cases	O
,	O

I	O
added	O
the	O
slow	O
Python	O
code	O
to	O
the	O
description	O
.	O

The	O
easy	O
way	O
-	O
pick	O
a	O
random	O
number	O
q	O
[	O
0	O
,	O
1	O
]	O
.	O

will	O
be	O
the	O
number	O
of	O
bytes	O
which	O
the	O
pattern	O
of	O
streams	O
will	O
repeat	O
after	O
.	O

Any	O
idea	O
how	O
I	O
can	O
later	O
make	O
1.6.2	O
in	O
/	O
usr	O
/	O
local	O
/	O
lib	O
work	O
with	O
python-dbg	O
?	O

Because	O
I'm	O
still	O
not	O
quite	O
grasping	O
the	O
method	O
and	O
there	O
seems	O
to	O
be	O
simpler	O
ways	O
to	O
solve	O
the	O
problem	O
,	O
I'm	O
just	O
going	O
to	O
put	O
this	O
here	O
:	O
#CODE	O

